{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ecog2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess ECoG data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the .nwb files, save to .wav files with sr=16000 for compatibility with wav2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(869888, 256)\n"
     ]
    }
   ],
   "source": [
    "# from nwbwidgets import nwb2widget\n",
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "path = '/NWB/EFC400/EFC400_B72.nwb' # CHANGE FOR EACH SUBJECT\n",
    "\n",
    "# Open the NWB file for reading\n",
    "# with NWBHDF5IO(path, 'r') as io:\n",
    "#     nwb_file = io.read()\n",
    "\n",
    "io = NWBHDF5IO(path, load_namespaces=True, mode='r')\n",
    "nwbfile = io.read()\n",
    "\n",
    "nwbfile_electrodes = nwbfile.acquisition['ElectricalSeries'].data[:]\n",
    "print(nwbfile.acquisition['ElectricalSeries'].data[:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 256)\n",
      "(300000, 256)\n",
      "(269888, 256)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "chunk_size = 300000\n",
    "\n",
    "num_full_chunks = len(nwbfile_electrodes) // chunk_size\n",
    "last_chunk_size = len(nwbfile_electrodes) % chunk_size\n",
    "\n",
    "full_chunks = np.split(nwbfile_electrodes[:num_full_chunks * chunk_size], num_full_chunks)\n",
    "last_chunk = nwbfile_electrodes[num_full_chunks * chunk_size:]\n",
    "\n",
    "chunks = full_chunks + [last_chunk]\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk.shape)\n",
    "\n",
    "# Loop through the chunks and save them as WAV files\n",
    "for i, chunk in enumerate(chunks):\n",
    "    file_name = f'/home/bayuan/Documents/fall23/ecog2vec/ecog/EFC400/EFC400_B72_{i}.wav' # CHANGE FOR EACH SUBJECT\n",
    "    sf.write(file_name, chunk, 16000)  # adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .container-fields {\n",
       "                font-family: \"Open Sans\", Arial, sans-serif;\n",
       "            }\n",
       "            .container-fields .field-value {\n",
       "                color: #00788E;\n",
       "            }\n",
       "            .container-fields details > summary {\n",
       "                cursor: pointer;\n",
       "                display: list-item;\n",
       "            }\n",
       "            .container-fields details > summary:hover {\n",
       "                color: #0A6EAA;\n",
       "            }\n",
       "        </style>\n",
       "        \n",
       "        <script>\n",
       "            function copyToClipboard(text) {\n",
       "                navigator.clipboard.writeText(text).then(function() {\n",
       "                    console.log('Copied to clipboard: ' + text);\n",
       "                }, function(err) {\n",
       "                    console.error('Could not copy text: ', err);\n",
       "                });\n",
       "            }\n",
       "\n",
       "            document.addEventListener('DOMContentLoaded', function() {\n",
       "                let fieldKeys = document.querySelectorAll('.container-fields .field-key');\n",
       "                fieldKeys.forEach(function(fieldKey) {\n",
       "                    fieldKey.addEventListener('click', function() {\n",
       "                        let accessCode = fieldKey.getAttribute('title').replace('Access code: ', '');\n",
       "                        copyToClipboard(accessCode);\n",
       "                    });\n",
       "                });\n",
       "            });\n",
       "        </script>\n",
       "        <div class='container-wrap'><div class='container-header'><div class='xr-obj-type'><h3>root (NWBFile)</h3></div></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['session_description']\">session_description:</span> <span class=\"field-value\">NWB File</span></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['identifier']\">identifier:</span> <span class=\"field-value\">EC61_B30</span></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['session_start_time']\">session_start_time:</span> <span class=\"field-value\">2014-06-05 03:37:41-07:00</span></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['timestamps_reference_time']\">timestamps_reference_time:</span> <span class=\"field-value\">2014-06-05 03:37:41-07:00</span></div><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['file_create_date']\"><b>file_create_date</b></summary><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-value\" title=\".fields['file_create_date'][0]\">2019-10-24 21:02:56.725378-07:00</span></div></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['acquisition']\"><b>acquisition (2)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['ElectricalSeries']\"><b>ElectricalSeries</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['rate']\">rate:</span> <span class=\"field-value\">3051.7578125</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['resolution']\">resolution:</span> <span class=\"field-value\">0.00032768</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['description']\">description:</span> <span class=\"field-value\">all Wav data</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['unit']\">unit:</span> <span class=\"field-value\">volts</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes']\"><b>electrodes</b></summary><div style=\"margin-left: 60px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['description']\">description:</span> <span class=\"field-value\">electrodes</span></div><details><summary style=\"display: list-item; margin-left: 60px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['table']\"><b>table</b></summary><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['table'].fields['description']\">description:</span> <span class=\"field-value\">metadata about extracellular electrodes</span></div><details><summary style=\"display: list-item; margin-left: 80px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['table'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['table'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('x', 'y', 'z', 'imp', 'location', 'filtering', 'group', 'group_name', 'label', 'bad', 'x_warped', 'y_warped', 'z_warped')</span></div><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['table'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3881cf2700>, <hdmf.common.table.VectorData object at 0x7f3881cf2b20>, <hdmf.common.table.VectorData object at 0x7f3881cf2d60>, <hdmf.common.table.VectorData object at 0x7f3881cf2640>, <hdmf.common.table.VectorData object at 0x7f3881cf2670>, <hdmf.common.table.VectorData object at 0x7f388235fbb0>, <hdmf.common.table.VectorData object at 0x7f388235f160>, <hdmf.common.table.VectorData object at 0x7f388235fbe0>, <hdmf.common.table.VectorData object at 0x7f3881cf29d0>, <hdmf.common.table.VectorData object at 0x7f388235f2b0>, <hdmf.common.table.VectorData object at 0x7f3881cf26d0>, <hdmf.common.table.VectorData object at 0x7f3881cf2730>, <hdmf.common.table.VectorData object at 0x7f3881ef59a0>)</span></div></details></details></details><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['anin4']\"><b>anin4</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['rate']\">rate:</span> <span class=\"field-value\">24414.0625</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['resolution']\">resolution:</span> <span class=\"field-value\">4.096e-05</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['description']\">description:</span> <span class=\"field-value\">anin4</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['unit']\">unit:</span> <span class=\"field-value\">amplitude</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['anin4'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['stimulus']\"><b>stimulus (2)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['stimulus']['speaker1']\"><b>speaker1</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['rate']\">rate:</span> <span class=\"field-value\">24414.0625</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['resolution']\">resolution:</span> <span class=\"field-value\">4.096e-05</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['description']\">description:</span> <span class=\"field-value\">speaker1</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['unit']\">unit:</span> <span class=\"field-value\">amplitude</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['stimulus']['speaker1'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div></details><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['stimulus']['speaker2']\"><b>speaker2</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['rate']\">rate:</span> <span class=\"field-value\">24414.0625</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['resolution']\">resolution:</span> <span class=\"field-value\">4.096e-05</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['description']\">description:</span> <span class=\"field-value\">speaker2</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['unit']\">unit:</span> <span class=\"field-value\">amplitude</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['stimulus']['speaker2'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['processing']\"><b>processing (2)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior']\"><b>behavior</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['description']\">description:</span> <span class=\"field-value\">human-subject behavior</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']\"><b>data_interfaces (1)</b></summary><details><summary style=\"display: list-item; margin-left: 60px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs']\"><b>BehavioralEpochs</b></summary><details><summary style=\"display: list-item; margin-left: 80px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']\"><b>interval_series (3)</b></summary><details><summary style=\"display: list-item; margin-left: 100px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription']\"><b>phoneme transcription</b></summary><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['resolution']\">resolution:</span> <span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['description']\">description:</span> <span class=\"field-value\">f y uw p iy p ax l l ih v t ax b iy ax pau hh ah n d r ax d pau dh ax b ah ng g ax l ow w aa z p l eh z ax n t l iy s ih ch uw ey t ih d pau n ih r dh ax sh ao r pau aa r t ax f ih sh ax l ih n t eh l ax jh ax n s ih z f ao r pau r iy l pau hh ih z s ah d ax n d ih p aa r ch er sh aa k t dh ax k ae s t pau t eh k n ih k ax l r ay t er z pau k ae n ax b r iy v iy ey t pau ih n b ih b l iy aa g r ax f iy z pau ax hh y uw jh pau t ae p ax s t r iy pau hh ah ng ih n hh er hh ao l w ey pau t r eh s p ae s ih ng ih z f ao r b ih d ax n pau ae n d s ah b jh ih k t pau t ax p eh n ax l t iy pau dh ax k l ah m z iy k ah s t ax m er pau s p ih l d s ah m ih k s p eh n s ih v pau p er f y uw m pau b er th d ey p aa r t iy z pau hh ae v k ah p k ey k s pau ae n d ay s k r iy m pau w iy p l ae n t ax b ih l d ax n uw b eh v er ih jh pau p l ae n t pau dh ax b eh s t w ey t ax l er n ih z t ax s aa l v eh k s t r ax p r aa b l ax m z pau w ih ch l ao ng aa r t ax k ax l w aa z ow p ey k pau ae n d n iy d ax d k l eh r ax f ax k ey sh ax n pau dh ax s aw n d ah v jh eh n ax f er z b y uw g ax l pau s k eh r d pau dh ax ae n t ax l ow p pau w eh s t ch eh s t er ih z ax k aw n t iy ih n n uw y ao r k pau t uw m ah ch k y uh r iy aa s ax t iy pau k ae n g eh t y uw ih n t uw t r ah b ax l pau dh ax eh m p er er hh ae d ax m iy n pau t eh m p er pau hh iy s t ow l ax pau d ay m pau f r ah m ax b eh g er pau k ih n d er g aa r t ax n pau ch ih l d r ax n pau d eh k er ey t dh eh r k l ae s r uw m z pau f ao r ao l hh aa l ax d ey z pau t ah g b ow t s aa r k ey p ax b ax l ah v hh ao l ih ng hh y uw jh pau l ow d z pau ae ng g ao r ax k ae t s aa r f er iy er dh ae n s ay ax m iy z pau p iy t s er iy ax z aa r k ax n v iy n y ax n t f ao r ax k w ih k pau l ah n ch pau ax m ah s k y ax l er ae b d ow m ax n ih z g uh d pau f ao r y ao r b ae k pau hh ih z s k ae l p pau aa z b l ih s t er d pau f r ah m t ax d ey z hh aa t s ah n pau aa b jh eh k t s m ey d ah v p y uw t er pau aa r b y uw t ax f ax l pau ae g r ax k ah l ch er ax l p r aa d ax k t s pau aa r ax n iy v ax n l iy d ih s t r ih b y ax t ax d pau k ao l ae n ae m b y ax l ax n s f ao r m eh d ax k ax l ax s ih s t ax n s pau dh ax m ae ng g ow ae n d dh ax p ax p ay ax pau aa r ih n ax b ow l pau dh ax eh m b l ax m d ih p ih k t s dh ax ax k r aa p ax l ax s ao l ax g l ow pau dh ax m ih s k w ow t w aa z r iy t r ae k t ax d w ih dh ae n ax p aa l ax jh iy pau k ax m b ay n pau ao l dh ax ih n g r iy d iy ax n t s pau ih n ax l aa r jh pau b ow l pau k l ae s p pau dh ax s k r uw pau ih n y ao r l eh f t hh ae n d pau p l eh jh t ax p aa r t ih s ax p ey t pau ih n n ax v aa d ax z ax k w aa t ih k k aa m p ax t ih sh ax n pau s ay k l ih k ax l p r ow g r ae m z pau w ih l n eh v er k ax m p ay l pau k er eh k t pau eh k s ax k y uw sh ax n ah v m ay ih n s t r ah k sh ax n z pau ih z k r uw sh ax l pau ih n s ay k l ax p iy d iy ax z pau s eh l d ax m p r iy z eh n t pau ae n ax k d ow t ax l eh v ax d ax n s pau dh ax w ih l ax w iy w uh m ax n pau w ao r ax m ah s k r ae t pau k ow t pau dh ax k ay ow t iy pau b aa b k ae t pau ae n d hh ay iy n ax pau aa r w ay l d pau ae n ax m ax l z pau hh iy ey t f ao r pau eh k s t r ax pau eh g z pau f ao r b r eh k f ax s t pau t r ax d ih sh ax n r iy k w ay er z p er eh n t ax l ax p r uw v ax l pau f ao r pau ah n d er ey jh pau m eh r ih jh pau p ax b l ih s ax t iy ae n d n ow t er ay ax t iy pau ow hh ae n d ih n hh ae n d pau m ow s t pau p r iy s ih ng k t s pau hh ae d ax th er d pau ah v dh ax v ow t s k aw n t ax d pau l aa t s ah v f ao r ax n m uw v iy z hh ae v s ah b t ay t ax l z pau sh iy s l ih p t ae n d s p r ey n d hh er pau ae ng k ax l pau aa n dh ax s t iy p s l ow p pau d ih s eh m b er ae n d jh ae n y uw eh r iy aa r n ay s m ah n th s pau t ax s p eh n d ih n m ay ae m iy pau s ay ax n t ih f ih k p r aa g r eh s k ah m z f r ah m dh ax d ih v eh l ax p m ax n t pau ah v n uw t eh k n iy k s pau s p eh sh ax l t ae s k f ao r s ih z pau r eh s k y uw hh aa s t ax jh ax z pau f r ah m k ih d n ae p er z pau ax s k r uw d r ay v er ih z m ey d f r ah m v aa d k ax pau ae n d ao r ax n jh jh uw s pau y uw m pau ah s t ih k s p l ih s ax t l iy pau d ih l iy t pau f ay l z pau p r aa jh eh k t d ih v eh l ax p m ax n t pau w aa z p r ax s iy d ih ng pau t uw pau s l ow l iy pau dh ax k aa r t uw n f iy ch er f iy ch er z pau ax m ah s k r ae t pau ae n d ax t ae d p ow l</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['unit']\">unit:</span> <span class=\"field-value\">n/a</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['data']\"><b>data</b></summary></details><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['timestamps']\"><b>timestamps</b></summary></details><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['timestamps_unit']\">timestamps_unit:</span> <span class=\"field-value\">seconds</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['interval']\">interval:</span> <span class=\"field-value\">1</span></div></details><details><summary style=\"display: list-item; margin-left: 100px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription']\"><b>syllable transcription</b></summary><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['resolution']\">resolution:</span> <span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['description']\">description:</span> <span class=\"field-value\">f_y_uw p_iy p_ax_l l_ih_v t_ax b_iy ax hh_ah_n d_r_ax_d dh_ax b_ah_ng g_ax l_ow w_aa_z p_l_eh z_ax_n_t l_iy s_ih ch_uw ey t_ih_d n_ih_r dh_ax sh_ao_r aa_r t_ax f_ih sh_ax_l ih_n t_eh l_ax jh_ax_n_s ih_z f_ao_r r_iy_l hh_ih_z s_ah d_ax_n d_ih p_aa_r ch_er sh_aa_k_t dh_ax k_ae_s_t t_eh_k n_ih k_ax_l r_ay t_er_z k_ae_n ax b_r_iy v_iy ey_t ih_n b_ih b_l_iy aa g_r_ax f_iy_z ax hh_y_uw_jh t_ae p_ax s_t_r_iy hh_ah_ng ih_n hh_er hh_ao_l w_ey t_r_eh s_p_ae s_ih_ng ih_z f_ao_r b_ih d_ax_n ae_n_d s_ah_b jh_ih_k_t t_ax p_eh n_ax_l t_iy dh_ax k_l_ah_m z_iy k_ah s_t_ax m_er s_p_ih_l_d s_ah_m ih_k s_p_eh_n s_ih_v p_er f_y_uw_m b_er_th d_ey p_aa_r t_iy_z hh_ae_v k_ah_p k_ey_k_s ae_n_d ay_s k_r_iy_m w_iy p_l_ae_n t_ax b_ih_l_d ax n_uw b_eh v_er ih_jh p_l_ae_n_t dh_ax b_eh_s_t w_ey t_ax l_er_n ih_z t_ax s_aa_l_v eh_k s_t_r_ax p_r_aa b_l_ax_m_z w_ih_ch l_ao_ng aa_r t_ax k_ax_l w_aa_z ow p_ey_k ae_n_d n_iy d_ax_d k_l_eh r_ax f_ax k_ey sh_ax_n dh_ax s_aw_n_d ah_v jh_eh n_ax f_er_z b_y_uw g_ax_l s_k_eh_r_d dh_ax ae_n t_ax l_ow_p w_eh_s_t ch_eh s_t_er ih_z ax k_aw_n t_iy ih_n n_uw y_ao_r_k t_uw m_ah_ch k_y_uh r_iy aa s_ax t_iy k_ae_n g_eh_t y_uw ih_n t_uw t_r_ah b_ax_l dh_ax eh_m p_er er hh_ae_d ax m_iy_n t_eh_m p_er hh_iy s_t_ow_l ax d_ay_m f_r_ah_m ax b_eh g_er k_ih_n d_er g_aa_r t_ax_n ch_ih_l d_r_ax_n d_eh k_er ey_t dh_eh_r k_l_ae s_r_uw_m_z f_ao_r ao_l hh_aa l_ax d_ey_z t_ah_g b_ow_t_s aa_r k_ey p_ax b_ax_l ah_v hh_ao l_ih_ng hh_y_uw_jh l_ow_d_z ae_ng g_ao r_ax k_ae_t_s aa_r f_er iy er dh_ae_n s_ay ax m_iy_z p_iy_t s_er iy ax_z aa_r k_ax_n v_iy n_y_ax_n_t f_ao_r ax k_w_ih_k l_ah_n_ch ax m_ah s_k_y_ax l_er ae_b d_ow m_ax_n ih_z g_uh_d f_ao_r y_ao_r b_ae_k hh_ih_z s_k_ae_l_p w_aa_z b_l_ih s_t_er_d f_r_ah_m t_ax d_ey_z hh_aa_t s_ah_n aa_b jh_eh_k_t_s m_ey_d ah_v p_y_uw t_er aa_r b_y_uw t_ax f_ax_l ae g_r_ax k_ah_l ch_er ax_l p_r_aa d_ax_k_t_s aa_r ax n_iy v_ax_n l_iy d_ih s_t_r_ih b_y_ax t_ax_d k_ao_l ae_n ae_m b_y_ax l_ax_n_s f_ao_r m_eh d_ax k_ax_l ax s_ih s_t_ax_n_s dh_ax m_ae_ng g_ow ae_n_d dh_ax p_ax p_ay ax aa_r ih_n ax b_ow_l dh_ax eh_m b_l_ax_m d_ih p_ih_k_t_s dh_ax ax k_r_aa p_ax l_ax_s ao_l ax g_l_ow dh_ax m_ih s_k_w_ow_t w_aa_z r_iy t_r_ae_k t_ax_d w_ih_dh ae_n ax p_aa l_ax jh_iy k_ax_m b_ay_n ao_l dh_ax ih_n g_r_iy d_iy ax_n_t_s ih_n ax l_aa_r_jh b_ow_l k_l_ae_s_p dh_ax s_k_r_uw ih_n y_ao_r l_eh_f_t hh_ae_n_d p_l_eh_jh t_ax p_aa_r t_ih s_ax p_ey_t ih_n n_ax v_aa d_ax_z ax k_w_aa t_ih_k k_aa_m p_ax t_ih sh_ax_n s_ay k_l_ih k_ax_l p_r_ow g_r_ae_m_z w_ih_l n_eh v_er k_ax_m p_ay_l k_er eh_k_t eh_k s_ax k_y_uw sh_ax_n ah_v m_ay ih_n s_t_r_ah_k sh_ax_n_z ih_z k_r_uw sh_ax_l ih_n s_ay k_l_ax p_iy d_iy ax_z s_eh_l d_ax_m p_r_iy z_eh_n_t ae n_ax_k d_ow t_ax_l eh v_ax d_ax_n_s dh_ax w_ih l_ax w_iy w_uh m_ax_n w_ao_r ax m_ah s_k_r_ae_t k_ow_t dh_ax k_ay ow t_iy b_aa_b k_ae_t ae_n_d hh_ay iy n_ax aa_r w_ay_l_d ae n_ax m_ax_l_z hh_iy ey_t f_ao_r eh_k s_t_r_ax eh_g_z f_ao_r b_r_eh_k f_ax_s_t t_r_ax d_ih sh_ax_n r_iy k_w_ay er_z p_er eh_n t_ax_l ax p_r_uw v_ax_l f_ao_r ah_n d_er ey_jh m_eh r_ih_jh p_ax b_l_ih s_ax t_iy ae_n_d n_ow t_er ay ax t_iy g_ow hh_ae_n_d ih_n hh_ae_n_d m_ow_s_t p_r_iy s_ih_ng_k_t_s hh_ae_d ax th_er_d ah_v dh_ax v_ow_t_s k_aw_n t_ax_d l_aa_t_s ah_v f_ao r_ax_n m_uw v_iy_z hh_ae_v s_ah_b t_ay t_ax_l_z sh_iy s_l_ih_p_t ae_n_d s_p_r_ey_n_d hh_er ae_ng k_ax_l aa_n dh_ax s_t_iy_p s_l_ow_p d_ih s_eh_m b_er ae_n_d jh_ae n_y_uw eh r_iy aa_r n_ay_s m_ah_n_th_s t_ax s_p_eh_n_d ih_n m_ay ae m_iy s_ay ax_n t_ih f_ih_k p_r_aa g_r_eh_s k_ah_m_z f_r_ah_m dh_ax d_ih v_eh l_ax_p m_ax_n_t ah_v n_uw t_eh_k n_iy_k_s s_p_eh sh_ax_l t_ae_s_k f_ao_r s_ih_z r_eh s_k_y_uw hh_aa s_t_ax jh_ax_z f_r_ah_m k_ih_d n_ae p_er_z ax s_k_r_uw d_r_ay v_er ih_z m_ey_d f_r_ah_m v_aa_d k_ax ae_n_d ao r_ax_n_jh jh_uw_s y_uw m_ah_s_t ih_k s_p_l_ih s_ax_t l_iy d_ih l_iy_t f_ay_l_z p_r_aa jh_eh_k_t d_ih v_eh l_ax_p m_ax_n_t w_aa_z p_r_ax s_iy d_ih_ng t_uw s_l_ow l_iy dh_ax k_aa_r t_uw_n f_iy ch_er f_iy ch_er_z ax m_ah s_k_r_ae_t ae_n_d ax t_ae_d p_ow_l</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['unit']\">unit:</span> <span class=\"field-value\">n/a</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['data']\"><b>data</b></summary></details><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['timestamps']\"><b>timestamps</b></summary></details><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['timestamps_unit']\">timestamps_unit:</span> <span class=\"field-value\">seconds</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['interval']\">interval:</span> <span class=\"field-value\">1</span></div></details><details><summary style=\"display: list-item; margin-left: 100px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription']\"><b>word transcription</b></summary><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['resolution']\">resolution:</span> <span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['description']\">description:</span> <span class=\"field-value\">Few people live to be a hundred The bungalow was pleasantly situated near the shore Artificial intelligence is for real His sudden departure shocked the cast Technical writers can abbreviate in bibliographies A huge tapestry hung in her hallway Trespassing is forbidden and subject to penalty The clumsy customer spilled some expensive perfume Birthday parties have cupcakes and ice cream We plan to build a new beverage plant The best way to learn is to solve extra problems Which long article was opaque and needed clarification The sound of Jennifer bugle scared the antelope Westchester is a county in New York Too much curiosity can get you into trouble The emperor had a mean temper He stole a dime from a beggar Kindergarten children decorate their classrooms for all holidays Tugboats are capable of hauling huge loads Angora cats are furrier than Siamese Pizzerias are convenient for a quick lunch A muscular abdomen is good for your back His scalp was blistered from today hot sun Objects made of pewter are beautiful Agricultural products are unevenly distributed Call an ambulance for medical assistance The mango and the papaya are in a bowl The emblem depicts the Acropolis all aglow The misquote was retracted with an apology Combine all the ingredients in a large bowl Clasp the screw in your left hand Pledge to participate in Nevada aquatic competition Cyclical programs will never compile Correct execution of my instructions is crucial Encyclopedias seldom present anecdotal evidence The willowy woman wore a muskrat coat The coyote bobcat and hyena are wild animals He ate four extra eggs for breakfast Tradition requires parental approval for under age marriage Publicity and notoriety go hand in hand Most precincts had a third of the votes counted Lots of foreign movies have subtitles She slipped and sprained her ankle on the steep slope December and January are nice months to spend in Miami Scientific progress comes from the development of new techniques Special task forces rescue hostages from kidnappers A screwdriver is made from vodka and orange juice You must explicitly delete files Project development was proceeding too slowly The cartoon feature features a muskrat and a tadpole</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['unit']\">unit:</span> <span class=\"field-value\">n/a</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['data']\"><b>data</b></summary></details><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['timestamps']\"><b>timestamps</b></summary></details><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['timestamps_unit']\">timestamps_unit:</span> <span class=\"field-value\">seconds</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['interval']\">interval:</span> <span class=\"field-value\">1</span></div></details></details></details></details></details><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys']\"><b>ecephys</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['description']\">description:</span> <span class=\"field-value\">Extracellular electrophysiology data.</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']\"><b>data_interfaces (2)</b></summary><details><summary style=\"display: list-item; margin-left: 60px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP']\"><b>LFP</b></summary><details><summary style=\"display: list-item; margin-left: 80px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']\"><b>electrical_series (2)</b></summary><details><summary style=\"display: list-item; margin-left: 100px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)']\"><b>high gamma (bipolar)</b></summary><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['rate']\">rate:</span> <span class=\"field-value\">400.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['resolution']\">resolution:</span> <span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['description']\">description:</span> <span class=\"field-value\">no description</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['unit']\">unit:</span> <span class=\"field-value\">volts</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes']\"><b>electrodes</b></summary><div style=\"margin-left: 140px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['description']\">description:</span> <span class=\"field-value\">all bipolar electrodes</span></div><details><summary style=\"display: list-item; margin-left: 140px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['table']\"><b>table</b></summary><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['table'].fields['description']\">description:</span> <span class=\"field-value\">pseudo-channels derived via John Burke style bipolar referencing</span></div><details><summary style=\"display: list-item; margin-left: 160px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['table'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['table'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('x', 'y', 'z', 'imp', 'location', 'label', 'bad')</span></div><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['table'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3881c6b970>, <hdmf.common.table.VectorData object at 0x7f3881c6bfa0>, <hdmf.common.table.VectorData object at 0x7f3881c6bb50>, <hdmf.common.table.VectorData object at 0x7f38823a2340>, <hdmf.common.table.VectorData object at 0x7f3881c6bd30>, <hdmf.common.table.VectorData object at 0x7f38823a2430>, <hdmf.common.table.VectorData object at 0x7f38823a2100>)</span></div></details></details></details><details><summary style=\"display: list-item; margin-left: 100px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)']\"><b>preprocessed (bipolar)</b></summary><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['rate']\">rate:</span> <span class=\"field-value\">400.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['resolution']\">resolution:</span> <span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['comments']\">comments:</span> <span class=\"field-value\">referencing:bipolar,Notch:None, Downsampled:Yes</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['description']\">description:</span> <span class=\"field-value\">no description</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['unit']\">unit:</span> <span class=\"field-value\">volts</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes']\"><b>electrodes</b></summary><div style=\"margin-left: 140px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['description']\">description:</span> <span class=\"field-value\">all bipolar electrodes</span></div><details><summary style=\"display: list-item; margin-left: 140px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['table']\"><b>table</b></summary><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['table'].fields['description']\">description:</span> <span class=\"field-value\">pseudo-channels derived via John Burke style bipolar referencing</span></div><details><summary style=\"display: list-item; margin-left: 160px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['table'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['table'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('x', 'y', 'z', 'imp', 'location', 'label', 'bad')</span></div><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['table'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3881c6b970>, <hdmf.common.table.VectorData object at 0x7f3881c6bfa0>, <hdmf.common.table.VectorData object at 0x7f3881c6bb50>, <hdmf.common.table.VectorData object at 0x7f38823a2340>, <hdmf.common.table.VectorData object at 0x7f3881c6bd30>, <hdmf.common.table.VectorData object at 0x7f38823a2430>, <hdmf.common.table.VectorData object at 0x7f38823a2100>)</span></div></details></details></details></details></details><details><summary style=\"display: list-item; margin-left: 60px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['bipolar-referenced metadata']\"><b>bipolar-referenced metadata</b></summary><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['bipolar-referenced metadata'].fields['description']\">description:</span> <span class=\"field-value\">pseudo-channels derived via John Burke style bipolar referencing</span></div><details><summary style=\"display: list-item; margin-left: 80px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['bipolar-referenced metadata'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['bipolar-referenced metadata'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('x', 'y', 'z', 'imp', 'location', 'label', 'bad')</span></div><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['bipolar-referenced metadata'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3881c6b970>, <hdmf.common.table.VectorData object at 0x7f3881c6bfa0>, <hdmf.common.table.VectorData object at 0x7f3881c6bb50>, <hdmf.common.table.VectorData object at 0x7f38823a2340>, <hdmf.common.table.VectorData object at 0x7f3881c6bd30>, <hdmf.common.table.VectorData object at 0x7f38823a2430>, <hdmf.common.table.VectorData object at 0x7f38823a2100>)</span></div></details></details></details></details><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['epoch_tags']\">epoch_tags:</span> <span class=\"field-value\">set()</span></div><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['electrodes']\"><b>electrodes</b></summary><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['electrodes'].fields['description']\">description:</span> <span class=\"field-value\">metadata about extracellular electrodes</span></div><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['electrodes'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['electrodes'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('x', 'y', 'z', 'imp', 'location', 'filtering', 'group', 'group_name', 'label', 'bad', 'x_warped', 'y_warped', 'z_warped')</span></div><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['electrodes'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3881cf2700>, <hdmf.common.table.VectorData object at 0x7f3881cf2b20>, <hdmf.common.table.VectorData object at 0x7f3881cf2d60>, <hdmf.common.table.VectorData object at 0x7f3881cf2640>, <hdmf.common.table.VectorData object at 0x7f3881cf2670>, <hdmf.common.table.VectorData object at 0x7f388235fbb0>, <hdmf.common.table.VectorData object at 0x7f388235f160>, <hdmf.common.table.VectorData object at 0x7f388235fbe0>, <hdmf.common.table.VectorData object at 0x7f3881cf29d0>, <hdmf.common.table.VectorData object at 0x7f388235f2b0>, <hdmf.common.table.VectorData object at 0x7f3881cf26d0>, <hdmf.common.table.VectorData object at 0x7f3881cf2730>, <hdmf.common.table.VectorData object at 0x7f3881ef59a0>)</span></div></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['electrode_groups']\"><b>electrode_groups (1)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['electrode_groups']['R256GridElectrode electrodes']\"><b>R256GridElectrode electrodes</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['electrode_groups']['R256GridElectrode electrodes'].fields['description']\">description:</span> <span class=\"field-value\">R256GridElectrode</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['electrode_groups']['R256GridElectrode electrodes'].fields['location']\">location:</span> <span class=\"field-value\">R256</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['electrode_groups']['R256GridElectrode electrodes'].fields['device']\"><b>device</b></summary></details></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['devices']\"><b>devices (1)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['devices']['R256GridElectrode']\"><b>R256GridElectrode</b></summary></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['intervals']\"><b>intervals (1)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['intervals']['trials']\"><b>trials</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['intervals']['trials'].fields['description']\">description:</span> <span class=\"field-value\">experimental trials</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['intervals']['trials'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['intervals']['trials'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('start_time', 'stop_time', 'transcription', 'production_type')</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['intervals']['trials'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3882319490>, <hdmf.common.table.VectorData object at 0x7f3882319430>, <hdmf.common.table.VectorData object at 0x7f3882688760>, <hdmf.common.table.VectorData object at 0x7f3882688670>)</span></div></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['subject']\"><b>subject</b></summary><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['subject'].fields['age__reference']\">age__reference:</span> <span class=\"field-value\">birth</span></div><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['subject'].fields['subject_id']\">subject_id:</span> <span class=\"field-value\">EC61</span></div></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['trials']\"><b>trials</b></summary><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['trials'].fields['description']\">description:</span> <span class=\"field-value\">experimental trials</span></div><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['trials'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['trials'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('start_time', 'stop_time', 'transcription', 'production_type')</span></div><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['trials'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3882319490>, <hdmf.common.table.VectorData object at 0x7f3882319430>, <hdmf.common.table.VectorData object at 0x7f3882688760>, <hdmf.common.table.VectorData object at 0x7f3882688670>)</span></div></details><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['lab']\">lab:</span> <span class=\"field-value\">Chang Lab</span></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['institution']\">institution:</span> <span class=\"field-value\">University of California, San Francisco</span></div></div>"
      ],
      "text/plain": [
       "root pynwb.file.NWBFile at 0x139880672181744\n",
       "Fields:\n",
       "  acquisition: {\n",
       "    ElectricalSeries <class 'pynwb.ecephys.ElectricalSeries'>,\n",
       "    anin4 <class 'pynwb.base.TimeSeries'>\n",
       "  }\n",
       "  devices: {\n",
       "    R256GridElectrode <class 'pynwb.device.Device'>\n",
       "  }\n",
       "  electrode_groups: {\n",
       "    R256GridElectrode electrodes <class 'pynwb.ecephys.ElectrodeGroup'>\n",
       "  }\n",
       "  electrodes: electrodes <class 'hdmf.common.table.DynamicTable'>\n",
       "  file_create_date: [datetime.datetime(2019, 10, 24, 21, 2, 56, 725378, tzinfo=tzoffset(None, -25200))]\n",
       "  identifier: EC61_B30\n",
       "  institution: University of California, San Francisco\n",
       "  intervals: {\n",
       "    trials <class 'pynwb.epoch.TimeIntervals'>\n",
       "  }\n",
       "  lab: Chang Lab\n",
       "  processing: {\n",
       "    behavior <class 'pynwb.base.ProcessingModule'>,\n",
       "    ecephys <class 'pynwb.base.ProcessingModule'>\n",
       "  }\n",
       "  session_description: NWB File\n",
       "  session_start_time: 2014-06-05 03:37:41-07:00\n",
       "  stimulus: {\n",
       "    speaker1 <class 'pynwb.base.TimeSeries'>,\n",
       "    speaker2 <class 'pynwb.base.TimeSeries'>\n",
       "  }\n",
       "  subject: subject abc.ECoGSubject at 0x139880672180064\n",
       "Fields:\n",
       "  age__reference: birth\n",
       "  subject_id: EC61\n",
       "\n",
       "  timestamps_reference_time: 2014-06-05 03:37:41-07:00\n",
       "  trials: trials <class 'pynwb.epoch.TimeIntervals'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwbfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train wav2vec on ecog data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training manifest from the dataset. Train the model with 256-channel inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /home/bayuan/Documents/fall23/fairseq/examples/wav2vec/wav2vec_manifest.py \\\n",
    "    /home/bayuan/Documents/fall23/ecog2vec/ecog/EFC400 \\\n",
    "  --dest /home/bayuan/Documents/fall23/ecog2vec/manifest \\\n",
    "  --ext wav \\\n",
    "  --valid-percent 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bayuan/anaconda3/envs/wav2vec/lib/python3.9/argparse.py\n",
      "2023-10-18 20:48:07 | INFO | fairseq_cli.train | Args: \n",
      "2023-10-18 20:48:08 | INFO | fairseq.models.wav2vec.wav2vec | Wav2VecModel(\n",
      "  (feature_extractor): ConvFeatureExtractionModel(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(8,), stride=(4,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (2-4): 3 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (5-6): 2 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_aggregator): ConvAggegator(\n",
      "    (conv_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): ReplicationPad1d((1, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(2,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ReplicationPad1d((2, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ReplicationPad1d((3, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(4,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ReplicationPad1d((4, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ReplicationPad1d((5, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(6,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): ReplicationPad1d((6, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): ReplicationPad1d((7, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(8,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): ReplicationPad1d((8, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (8): Sequential(\n",
      "        (0): ReplicationPad1d((9, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(10,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (9): Sequential(\n",
      "        (0): ReplicationPad1d((10, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (10): Sequential(\n",
      "        (0): ReplicationPad1d((11, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(12,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (11): Sequential(\n",
      "        (0): ReplicationPad1d((12, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(13,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (residual_proj): ModuleList(\n",
      "      (0-11): 12 x None\n",
      "    )\n",
      "  )\n",
      "  (wav2vec_predictions): Wav2VecPredictionsModel(\n",
      "    (project_to_steps): ConvTranspose2d(512, 512, kernel_size=(1, 12), stride=(1, 1))\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (dropout_feats): Dropout(p=0.0, inplace=False)\n",
      "  (dropout_agg): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "2023-10-18 20:48:08 | INFO | fairseq_cli.train | Wav2VecModel(\n",
      "  (feature_extractor): ConvFeatureExtractionModel(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(8,), stride=(4,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (2-4): 3 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (5-6): 2 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_aggregator): ConvAggegator(\n",
      "    (conv_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): ReplicationPad1d((1, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(2,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ReplicationPad1d((2, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ReplicationPad1d((3, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(4,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ReplicationPad1d((4, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ReplicationPad1d((5, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(6,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): ReplicationPad1d((6, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): ReplicationPad1d((7, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(8,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): ReplicationPad1d((8, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (8): Sequential(\n",
      "        (0): ReplicationPad1d((9, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(10,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (9): Sequential(\n",
      "        (0): ReplicationPad1d((10, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (10): Sequential(\n",
      "        (0): ReplicationPad1d((11, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(12,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (11): Sequential(\n",
      "        (0): ReplicationPad1d((12, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(13,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (residual_proj): ModuleList(\n",
      "      (0-11): 12 x None\n",
      "    )\n",
      "  )\n",
      "  (wav2vec_predictions): Wav2VecPredictionsModel(\n",
      "    (project_to_steps): ConvTranspose2d(512, 512, kernel_size=(1, 12), stride=(1, 1))\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (dropout_feats): Dropout(p=0.0, inplace=False)\n",
      "  (dropout_agg): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "2023-10-18 20:48:08 | INFO | fairseq_cli.train | task: AudioPretrainingTask\n",
      "2023-10-18 20:48:08 | INFO | fairseq_cli.train | model: Wav2VecModel\n",
      "2023-10-18 20:48:08 | INFO | fairseq_cli.train | criterion: Wav2vecCriterion\n",
      "2023-10-18 20:48:08 | INFO | fairseq_cli.train | num. shared model params: 33,842,688 (num. trained: 33,842,688)\n",
      "2023-10-18 20:48:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-10-18 20:48:08 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 5, skipped 0 samples\n",
      "2023-10-18 20:48:08 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.1.0.bias\n",
      "2023-10-18 20:48:08 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.2.0.bias\n",
      "2023-10-18 20:48:08 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.3.0.bias\n",
      "2023-10-18 20:48:08 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.4.0.bias\n",
      "2023-10-18 20:48:08 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.5.0.bias\n",
      "2023-10-18 20:48:08 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.6.0.bias\n",
      "2023-10-18 20:48:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-10-18 20:48:08 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.528 GB ; name = NVIDIA RTX A6000                        \n",
      "2023-10-18 20:48:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-10-18 20:48:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-10-18 20:48:08 | INFO | fairseq_cli.train | max tokens per device = 150000000 and max sentences per device = 1\n",
      "2023-10-18 20:48:08 | INFO | fairseq.trainer | Preparing to load checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-18 20:48:08 | INFO | fairseq.trainer | No existing checkpoint found /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-18 20:48:08 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-10-18 20:48:08 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 57, skipped 0 samples\n",
      "2023-10-18 20:48:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:48:08 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-10-18 20:48:08 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = True\n",
      "2023-10-18 20:48:08 | INFO | fairseq.tasks.fairseq_task | batches will be rebuilt for each epoch\n",
      "2023-10-18 20:48:08 | INFO | fairseq.data.iterators | First train data shape: torch.Size([300000, 256])\n",
      "2023-10-18 20:48:08 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-10-18 20:48:08 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-10-18 20:48:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:48:08 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-10-18 20:48:08 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = True\n",
      "2023-10-18 20:48:08 | INFO | fairseq.tasks.fairseq_task | batches will be rebuilt for each epoch\n",
      "2023-10-18 20:48:08 | INFO | fairseq.data.iterators | First train data shape: torch.Size([300000, 256])\n",
      "2023-10-18 20:48:08 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-10-18 20:48:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 001:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:48:11 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-10-18 20:48:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-10-18 20:48:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   2%|▌                                | 1/56 [00:03<03:23,  3.70s/it]2023-10-18 20:48:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   4%|█▏                               | 2/56 [00:04<01:32,  1.71s/it]2023-10-18 20:48:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:   5%|█▊                               | 3/56 [00:04<01:04,  1.22s/it]2023-10-18 20:48:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001:   7%|██▎                              | 4/56 [00:04<00:40,  1.28it/s]2023-10-18 20:48:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 001:   9%|██▉                              | 5/56 [00:05<00:30,  1.67it/s]2023-10-18 20:48:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
      "epoch 001:  11%|███▌                             | 6/56 [00:05<00:21,  2.33it/s]2023-10-18 20:48:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0\n",
      "epoch 001:  12%|████▏                            | 7/56 [00:05<00:19,  2.50it/s]2023-10-18 20:48:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5\n",
      "2023-10-18 20:48:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25\n",
      "epoch 001:  25%|████████                        | 14/56 [00:06<00:06,  6.30it/s]2023-10-18 20:48:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125\n",
      "epoch 001:  98%|███████████████████████████████▍| 55/56 [00:12<00:00,  9.24it/s]2023-10-18 20:48:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:48:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:48:23 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 2\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.35it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  50%|████    | 2/4 [00:01<00:00,  2.08it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  4.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:48:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.997 | ntokens 22374 | nsentences 1 | wps 161162 | wpb 22374 | bsz 1 | num_updates 46\n",
      "2023-10-18 20:48:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 46 updates\n",
      "2023-10-18 20:48:24 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:48:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:48:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 1 @ 46 updates, score 4.997) (writing took 0.9435654190019704 seconds)\n",
      "2023-10-18 20:48:25 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-10-18 20:48:25 | INFO | train | epoch 001 | loss 7.036 | ntokens 20461.8 | nsentences 1 | wps 108174 | ups 5.29 | wpb 20461.8 | bsz 1 | num_updates 46 | lr 1.9308e-06 | gnorm 318.419 | loss_scale 0.125 | train_wall 11 | gb_free 46.3 | wall 17\n",
      "2023-10-18 20:48:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:48:26 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 2\n",
      "2023-10-18 20:48:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 002:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:48:26 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-10-18 20:48:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  96%|▉| 54/56 [00:06<00:00,  9.87it/s, loss=5.914, ntokens=20124.8, n2023-10-18 20:48:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:48:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:48:32 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 3\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.37it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00,  2.40it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  5.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:48:33 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.899 | ntokens 22374 | nsentences 1 | wps 210388 | wpb 22374 | bsz 1 | num_updates 102 | best_loss 4.899\n",
      "2023-10-18 20:48:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 102 updates\n",
      "2023-10-18 20:48:33 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:48:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:48:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 2 @ 102 updates, score 4.899) (writing took 1.302391668985365 seconds)\n",
      "2023-10-18 20:48:35 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-10-18 20:48:35 | INFO | train | epoch 002 | loss 4.927 | ntokens 19918.1 | nsentences 1 | wps 118980 | ups 5.97 | wpb 19918.1 | bsz 1 | num_updates 102 | lr 4.1596e-06 | gnorm 30.874 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 27\n",
      "2023-10-18 20:48:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:48:35 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 3\n",
      "2023-10-18 20:48:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 003:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:48:35 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-10-18 20:48:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  98%|███████████████████████████████▍| 55/56 [00:06<00:00,  9.55it/s]2023-10-18 20:48:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:48:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:48:42 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 4\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.07it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  50%|████    | 2/4 [00:01<00:00,  2.26it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  4.94it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:48:43 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.872 | ntokens 22374 | nsentences 1 | wps 306576 | wpb 22374 | bsz 1 | num_updates 158 | best_loss 4.872\n",
      "2023-10-18 20:48:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 158 updates\n",
      "2023-10-18 20:48:43 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:48:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:48:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 3 @ 158 updates, score 4.872) (writing took 1.3062317889998667 seconds)\n",
      "2023-10-18 20:48:44 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-10-18 20:48:44 | INFO | train | epoch 003 | loss 4.882 | ntokens 19918.1 | nsentences 1 | wps 114824 | ups 5.76 | wpb 19918.1 | bsz 1 | num_updates 158 | lr 6.3884e-06 | gnorm 14.093 | loss_scale 0.125 | train_wall 5 | gb_free 46.5 | wall 37\n",
      "2023-10-18 20:48:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:48:45 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 4\n",
      "2023-10-18 20:48:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 004:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:48:45 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-10-18 20:48:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  98%|▉| 55/56 [00:06<00:00, 10.22it/s, loss=4.876, ntokens=19826.4, n2023-10-18 20:48:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:48:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:48:51 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 5\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.37it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00,  2.77it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:48:52 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.856 | ntokens 22374 | nsentences 1 | wps 185615 | wpb 22374 | bsz 1 | num_updates 214 | best_loss 4.856\n",
      "2023-10-18 20:48:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 214 updates\n",
      "2023-10-18 20:48:52 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:48:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:48:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 4 @ 214 updates, score 4.856) (writing took 1.3046277060057037 seconds)\n",
      "2023-10-18 20:48:54 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-10-18 20:48:54 | INFO | train | epoch 004 | loss 4.864 | ntokens 19918.1 | nsentences 1 | wps 119148 | ups 5.98 | wpb 19918.1 | bsz 1 | num_updates 214 | lr 8.6172e-06 | gnorm 11.101 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 46\n",
      "2023-10-18 20:48:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:48:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 5\n",
      "2023-10-18 20:48:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 005:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:48:54 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-10-18 20:48:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  96%|██████████████████████████████▊ | 54/56 [00:06<00:00, 10.62it/s]2023-10-18 20:49:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:49:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:01 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 6\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.35it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00,  2.74it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00,  4.07it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  5.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:49:02 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.849 | ntokens 22374 | nsentences 1 | wps 218587 | wpb 22374 | bsz 1 | num_updates 270 | best_loss 4.849\n",
      "2023-10-18 20:49:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 270 updates\n",
      "2023-10-18 20:49:02 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:49:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:49:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 5 @ 270 updates, score 4.849) (writing took 1.3132909970008768 seconds)\n",
      "2023-10-18 20:49:03 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2023-10-18 20:49:03 | INFO | train | epoch 005 | loss 4.852 | ntokens 19918.1 | nsentences 1 | wps 118865 | ups 5.97 | wpb 19918.1 | bsz 1 | num_updates 270 | lr 1.0846e-05 | gnorm 7.226 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 55\n",
      "2023-10-18 20:49:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:03 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 6\n",
      "2023-10-18 20:49:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 006:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:49:03 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-10-18 20:49:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:  98%|▉| 55/56 [00:06<00:00,  9.63it/s, loss=4.852, ntokens=20257, nse2023-10-18 20:49:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:49:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 7\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.08it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  50%|████    | 2/4 [00:01<00:00,  2.27it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  4.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:49:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.845 | ntokens 22374 | nsentences 1 | wps 305999 | wpb 22374 | bsz 1 | num_updates 326 | best_loss 4.845\n",
      "2023-10-18 20:49:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 326 updates\n",
      "2023-10-18 20:49:11 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:49:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:49:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 6 @ 326 updates, score 4.845) (writing took 1.2880299749958795 seconds)\n",
      "2023-10-18 20:49:13 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-10-18 20:49:13 | INFO | train | epoch 006 | loss 4.848 | ntokens 19918.1 | nsentences 1 | wps 116523 | ups 5.85 | wpb 19918.1 | bsz 1 | num_updates 326 | lr 1.30748e-05 | gnorm 8.297 | loss_scale 0.125 | train_wall 5 | gb_free 46.5 | wall 65\n",
      "2023-10-18 20:49:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:13 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 7\n",
      "2023-10-18 20:49:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 007:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:49:13 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-10-18 20:49:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007:  98%|███████████████████████████████▍| 55/56 [00:06<00:00, 10.65it/s]2023-10-18 20:49:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:49:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:20 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 8\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.07it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  50%|████    | 2/4 [00:01<00:00,  2.26it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  4.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:49:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.844 | ntokens 22374 | nsentences 1 | wps 305853 | wpb 22374 | bsz 1 | num_updates 382 | best_loss 4.844\n",
      "2023-10-18 20:49:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 382 updates\n",
      "2023-10-18 20:49:21 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:49:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:49:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 7 @ 382 updates, score 4.844) (writing took 1.254704936989583 seconds)\n",
      "2023-10-18 20:49:22 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2023-10-18 20:49:22 | INFO | train | epoch 007 | loss 4.844 | ntokens 19918.1 | nsentences 1 | wps 117361 | ups 5.89 | wpb 19918.1 | bsz 1 | num_updates 382 | lr 1.53036e-05 | gnorm 7.087 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 74\n",
      "2023-10-18 20:49:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:22 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 8\n",
      "2023-10-18 20:49:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 008:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:49:22 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2023-10-18 20:49:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:  96%|▉| 54/56 [00:06<00:00, 10.49it/s, loss=4.844, ntokens=20066.4, n2023-10-18 20:49:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:49:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:29 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 9\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.21it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  50%|████    | 2/4 [00:01<00:00,  2.18it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  4.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:49:31 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.839 | ntokens 22374 | nsentences 1 | wps 209902 | wpb 22374 | bsz 1 | num_updates 438 | best_loss 4.839\n",
      "2023-10-18 20:49:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 438 updates\n",
      "2023-10-18 20:49:31 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:49:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:49:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 8 @ 438 updates, score 4.839) (writing took 1.3004544349969365 seconds)\n",
      "2023-10-18 20:49:32 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2023-10-18 20:49:32 | INFO | train | epoch 008 | loss 4.841 | ntokens 19918.1 | nsentences 1 | wps 114906 | ups 5.77 | wpb 19918.1 | bsz 1 | num_updates 438 | lr 1.75324e-05 | gnorm 5.78 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 84\n",
      "2023-10-18 20:49:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:32 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 9\n",
      "2023-10-18 20:49:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 009:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:49:32 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2023-10-18 20:49:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009:  98%|███████████████████████████████▍| 55/56 [00:06<00:00, 10.08it/s]2023-10-18 20:49:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:49:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:39 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 10\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.21it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  50%|████    | 2/4 [00:01<00:00,  2.18it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  4.80it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:49:40 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.841 | ntokens 22374 | nsentences 1 | wps 210637 | wpb 22374 | bsz 1 | num_updates 494 | best_loss 4.839\n",
      "2023-10-18 20:49:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 494 updates\n",
      "2023-10-18 20:49:40 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-18 20:49:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-18 20:49:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 9 @ 494 updates, score 4.841) (writing took 0.7390052980044857 seconds)\n",
      "2023-10-18 20:49:41 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2023-10-18 20:49:41 | INFO | train | epoch 009 | loss 4.839 | ntokens 19918.1 | nsentences 1 | wps 122296 | ups 6.14 | wpb 19918.1 | bsz 1 | num_updates 494 | lr 1.97612e-05 | gnorm 4.299 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 93\n",
      "2023-10-18 20:49:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:41 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 10\n",
      "2023-10-18 20:49:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 010:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:49:41 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2023-10-18 20:49:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010:  98%|▉| 55/56 [00:06<00:00,  9.21it/s, loss=4.84, ntokens=19553.9, ns2023-10-18 20:49:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:49:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:48 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 11\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.07it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:49:49 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.839 | ntokens 22374 | nsentences 1 | wps 376013 | wpb 22374 | bsz 1 | num_updates 550 | best_loss 4.839\n",
      "2023-10-18 20:49:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 550 updates\n",
      "2023-10-18 20:49:49 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:49:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:49:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 10 @ 550 updates, score 4.839) (writing took 1.292569701996399 seconds)\n",
      "2023-10-18 20:49:51 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2023-10-18 20:49:51 | INFO | train | epoch 010 | loss 4.839 | ntokens 19918.1 | nsentences 1 | wps 115502 | ups 5.8 | wpb 19918.1 | bsz 1 | num_updates 550 | lr 2e-05 | gnorm 4.237 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 103\n",
      "2023-10-18 20:49:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:51 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 11\n",
      "2023-10-18 20:49:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 011:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:49:51 | INFO | fairseq.trainer | begin training epoch 11\n",
      "2023-10-18 20:49:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 011:  96%|▉| 54/56 [00:06<00:00, 10.05it/s, loss=4.838, ntokens=19901.2, n2023-10-18 20:49:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:49:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:49:58 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 12\n",
      "\n",
      "epoch 011 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.21it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00,  3.86it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:49:59 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.838 | ntokens 22374 | nsentences 1 | wps 331952 | wpb 22374 | bsz 1 | num_updates 606 | best_loss 4.838\n",
      "2023-10-18 20:49:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 606 updates\n",
      "2023-10-18 20:49:59 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 11 @ 606 updates, score 4.838) (writing took 1.298239294992527 seconds)\n",
      "2023-10-18 20:50:00 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2023-10-18 20:50:00 | INFO | train | epoch 011 | loss 4.838 | ntokens 19918.1 | nsentences 1 | wps 118339 | ups 5.94 | wpb 19918.1 | bsz 1 | num_updates 606 | lr 2e-05 | gnorm 4.176 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 112\n",
      "2023-10-18 20:50:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:00 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 12\n",
      "2023-10-18 20:50:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 012:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:50:00 | INFO | fairseq.trainer | begin training epoch 12\n",
      "2023-10-18 20:50:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 012:  96%|██████████████████████████████▊ | 54/56 [00:06<00:00,  9.58it/s]2023-10-18 20:50:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:50:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:07 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 13\n",
      "\n",
      "epoch 012 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.37it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  50%|████    | 2/4 [00:01<00:00,  2.09it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  4.64it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:50:08 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.838 | ntokens 22374 | nsentences 1 | wps 160078 | wpb 22374 | bsz 1 | num_updates 662 | best_loss 4.838\n",
      "2023-10-18 20:50:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 662 updates\n",
      "2023-10-18 20:50:08 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 12 @ 662 updates, score 4.838) (writing took 1.3246942989935633 seconds)\n",
      "2023-10-18 20:50:10 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2023-10-18 20:50:10 | INFO | train | epoch 012 | loss 4.838 | ntokens 19918.1 | nsentences 1 | wps 116110 | ups 5.83 | wpb 19918.1 | bsz 1 | num_updates 662 | lr 2e-05 | gnorm 4.683 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 122\n",
      "2023-10-18 20:50:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 13\n",
      "2023-10-18 20:50:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 013:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:50:10 | INFO | fairseq.trainer | begin training epoch 13\n",
      "2023-10-18 20:50:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 013:  98%|▉| 55/56 [00:06<00:00,  9.29it/s, loss=4.838, ntokens=19588.6, n2023-10-18 20:50:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:50:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:17 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 14\n",
      "\n",
      "epoch 013 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.36it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00,  2.76it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:50:18 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.836 | ntokens 22374 | nsentences 1 | wps 185104 | wpb 22374 | bsz 1 | num_updates 718 | best_loss 4.836\n",
      "2023-10-18 20:50:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 718 updates\n",
      "2023-10-18 20:50:18 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 13 @ 718 updates, score 4.836) (writing took 1.3070338279940188 seconds)\n",
      "2023-10-18 20:50:19 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2023-10-18 20:50:19 | INFO | train | epoch 013 | loss 4.837 | ntokens 19918.1 | nsentences 1 | wps 116754 | ups 5.86 | wpb 19918.1 | bsz 1 | num_updates 718 | lr 2e-05 | gnorm 3.395 | loss_scale 0.125 | train_wall 5 | gb_free 46.4 | wall 131\n",
      "2023-10-18 20:50:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:20 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 14\n",
      "2023-10-18 20:50:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 014:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:50:20 | INFO | fairseq.trainer | begin training epoch 14\n",
      "2023-10-18 20:50:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 014:  96%|██████████████████████████████▊ | 54/56 [00:06<00:00,  9.48it/s]2023-10-18 20:50:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:50:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:26 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 15\n",
      "\n",
      "epoch 014 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.08it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.51it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:50:27 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.836 | ntokens 22374 | nsentences 1 | wps 376428 | wpb 22374 | bsz 1 | num_updates 774 | best_loss 4.836\n",
      "2023-10-18 20:50:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 774 updates\n",
      "2023-10-18 20:50:27 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 14 @ 774 updates, score 4.836) (writing took 1.2904506469785701 seconds)\n",
      "2023-10-18 20:50:29 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2023-10-18 20:50:29 | INFO | train | epoch 014 | loss 4.836 | ntokens 19918.1 | nsentences 1 | wps 117348 | ups 5.89 | wpb 19918.1 | bsz 1 | num_updates 774 | lr 2e-05 | gnorm 2.714 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 141\n",
      "2023-10-18 20:50:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:29 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 15\n",
      "2023-10-18 20:50:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 015:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:50:29 | INFO | fairseq.trainer | begin training epoch 15\n",
      "2023-10-18 20:50:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 015:  98%|▉| 55/56 [00:06<00:00,  9.38it/s, loss=4.836, ntokens=20580.1, n2023-10-18 20:50:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:50:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:36 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 16\n",
      "\n",
      "epoch 015 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.08it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.51it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:50:37 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.836 | ntokens 22374 | nsentences 1 | wps 375972 | wpb 22374 | bsz 1 | num_updates 830 | best_loss 4.836\n",
      "2023-10-18 20:50:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 830 updates\n",
      "2023-10-18 20:50:37 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 15 @ 830 updates, score 4.836) (writing took 1.3078458919771947 seconds)\n",
      "2023-10-18 20:50:39 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2023-10-18 20:50:39 | INFO | train | epoch 015 | loss 4.836 | ntokens 19918.1 | nsentences 1 | wps 113994 | ups 5.72 | wpb 19918.1 | bsz 1 | num_updates 830 | lr 2e-05 | gnorm 2.529 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 151\n",
      "2023-10-18 20:50:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:39 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 16\n",
      "2023-10-18 20:50:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 016:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:50:39 | INFO | fairseq.trainer | begin training epoch 16\n",
      "2023-10-18 20:50:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 016:  96%|██████████████████████████████▊ | 54/56 [00:06<00:00, 10.14it/s]2023-10-18 20:50:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:50:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:46 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 17\n",
      "\n",
      "epoch 016 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.19it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  50%|████    | 2/4 [00:01<00:00,  2.15it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  4.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:50:47 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.836 | ntokens 22374 | nsentences 1 | wps 209584 | wpb 22374 | bsz 1 | num_updates 886 | best_loss 4.836\n",
      "2023-10-18 20:50:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 886 updates\n",
      "2023-10-18 20:50:47 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 16 @ 886 updates, score 4.836) (writing took 1.2743821550102439 seconds)\n",
      "2023-10-18 20:50:48 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2023-10-18 20:50:48 | INFO | train | epoch 016 | loss 4.836 | ntokens 19918.1 | nsentences 1 | wps 115366 | ups 5.79 | wpb 19918.1 | bsz 1 | num_updates 886 | lr 2e-05 | gnorm 2.369 | loss_scale 0.125 | train_wall 5 | gb_free 46.5 | wall 160\n",
      "2023-10-18 20:50:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:48 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 17\n",
      "2023-10-18 20:50:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 017:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:50:48 | INFO | fairseq.trainer | begin training epoch 17\n",
      "2023-10-18 20:50:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 017:  96%|▉| 54/56 [00:06<00:00, 12.15it/s, loss=4.836, ntokens=19582.2, n2023-10-18 20:50:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:50:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:55 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 18\n",
      "\n",
      "epoch 017 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.07it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:50:56 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.835 | ntokens 22374 | nsentences 1 | wps 376113 | wpb 22374 | bsz 1 | num_updates 942 | best_loss 4.835\n",
      "2023-10-18 20:50:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 942 updates\n",
      "2023-10-18 20:50:56 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:50:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 17 @ 942 updates, score 4.835) (writing took 1.2675804659957066 seconds)\n",
      "2023-10-18 20:50:58 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2023-10-18 20:50:58 | INFO | train | epoch 017 | loss 4.836 | ntokens 19918.1 | nsentences 1 | wps 117018 | ups 5.87 | wpb 19918.1 | bsz 1 | num_updates 942 | lr 1.99999e-05 | gnorm 2.28 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 170\n",
      "2023-10-18 20:50:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:50:58 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 18\n",
      "2023-10-18 20:50:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 018:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:50:58 | INFO | fairseq.trainer | begin training epoch 18\n",
      "2023-10-18 20:50:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 018:  98%|███████████████████████████████▍| 55/56 [00:06<00:00, 11.47it/s]2023-10-18 20:51:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:51:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:05 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 19\n",
      "\n",
      "epoch 018 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.20it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00,  2.48it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.77it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:51:06 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.836 | ntokens 22374 | nsentences 1 | wps 254630 | wpb 22374 | bsz 1 | num_updates 998 | best_loss 4.835\n",
      "2023-10-18 20:51:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 998 updates\n",
      "2023-10-18 20:51:06 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-18 20:51:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-18 20:51:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 18 @ 998 updates, score 4.836) (writing took 0.7945785459887702 seconds)\n",
      "2023-10-18 20:51:07 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2023-10-18 20:51:07 | INFO | train | epoch 018 | loss 4.835 | ntokens 19918.1 | nsentences 1 | wps 123910 | ups 6.22 | wpb 19918.1 | bsz 1 | num_updates 998 | lr 1.99999e-05 | gnorm 1.726 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 179\n",
      "2023-10-18 20:51:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:07 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 19\n",
      "2023-10-18 20:51:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 019:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:51:07 | INFO | fairseq.trainer | begin training epoch 19\n",
      "2023-10-18 20:51:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 019:  96%|▉| 54/56 [00:06<00:00, 10.38it/s, loss=4.836, ntokens=19999.4, n2023-10-18 20:51:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:51:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:14 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 20\n",
      "\n",
      "epoch 019 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.20it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.44it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:51:15 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.836 | ntokens 22374 | nsentences 1 | wps 257706 | wpb 22374 | bsz 1 | num_updates 1054 | best_loss 4.835\n",
      "2023-10-18 20:51:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1054 updates\n",
      "2023-10-18 20:51:15 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-18 20:51:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-18 20:51:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 19 @ 1054 updates, score 4.836) (writing took 0.7914177290222142 seconds)\n",
      "2023-10-18 20:51:16 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2023-10-18 20:51:16 | INFO | train | epoch 019 | loss 4.836 | ntokens 19918.1 | nsentences 1 | wps 126255 | ups 6.34 | wpb 19918.1 | bsz 1 | num_updates 1054 | lr 1.99999e-05 | gnorm 2.184 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 188\n",
      "2023-10-18 20:51:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:16 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 20\n",
      "2023-10-18 20:51:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 020:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:51:16 | INFO | fairseq.trainer | begin training epoch 20\n",
      "2023-10-18 20:51:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 020:  98%|▉| 55/56 [00:06<00:00, 10.95it/s, loss=4.836, ntokens=19910.5, n2023-10-18 20:51:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:51:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:23 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 21\n",
      "\n",
      "epoch 020 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  25%|██      | 1/4 [00:01<00:03,  1.03s/it]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.20it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:51:24 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.837 | ntokens 22374 | nsentences 1 | wps 375703 | wpb 22374 | bsz 1 | num_updates 1110 | best_loss 4.835\n",
      "2023-10-18 20:51:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1110 updates\n",
      "2023-10-18 20:51:24 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-18 20:51:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-18 20:51:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 20 @ 1110 updates, score 4.837) (writing took 0.8018745790177491 seconds)\n",
      "2023-10-18 20:51:25 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2023-10-18 20:51:25 | INFO | train | epoch 020 | loss 4.836 | ntokens 19918.1 | nsentences 1 | wps 121826 | ups 6.12 | wpb 19918.1 | bsz 1 | num_updates 1110 | lr 1.99999e-05 | gnorm 2.602 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 197\n",
      "2023-10-18 20:51:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:25 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 21\n",
      "2023-10-18 20:51:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 021:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:51:25 | INFO | fairseq.trainer | begin training epoch 21\n",
      "2023-10-18 20:51:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 021:  98%|███████████████████████████████▍| 55/56 [00:06<00:00,  9.67it/s]2023-10-18 20:51:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:51:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:32 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 22\n",
      "\n",
      "epoch 021 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  25%|██      | 1/4 [00:01<00:03,  1.04s/it]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:51:33 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.835 | ntokens 22374 | nsentences 1 | wps 375013 | wpb 22374 | bsz 1 | num_updates 1166 | best_loss 4.835\n",
      "2023-10-18 20:51:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1166 updates\n",
      "2023-10-18 20:51:33 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:51:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:51:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 21 @ 1166 updates, score 4.835) (writing took 1.3027956260193605 seconds)\n",
      "2023-10-18 20:51:34 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
      "2023-10-18 20:51:34 | INFO | train | epoch 021 | loss 4.836 | ntokens 19918.1 | nsentences 1 | wps 116516 | ups 5.85 | wpb 19918.1 | bsz 1 | num_updates 1166 | lr 1.99999e-05 | gnorm 2.796 | loss_scale 0.125 | train_wall 5 | gb_free 46.7 | wall 207\n",
      "2023-10-18 20:51:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:35 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 22\n",
      "2023-10-18 20:51:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 022:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:51:35 | INFO | fairseq.trainer | begin training epoch 22\n",
      "2023-10-18 20:51:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 022:  98%|▉| 55/56 [00:06<00:00, 10.65it/s, loss=4.836, ntokens=19686.2, n2023-10-18 20:51:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:51:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:42 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 23\n",
      "\n",
      "epoch 022 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  25%|██      | 1/4 [00:01<00:03,  1.03s/it]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:51:43 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.835 | ntokens 22374 | nsentences 1 | wps 374350 | wpb 22374 | bsz 1 | num_updates 1222 | best_loss 4.835\n",
      "2023-10-18 20:51:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1222 updates\n",
      "2023-10-18 20:51:43 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:51:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:51:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 22 @ 1222 updates, score 4.835) (writing took 1.2730052789847832 seconds)\n",
      "2023-10-18 20:51:44 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
      "2023-10-18 20:51:44 | INFO | train | epoch 022 | loss 4.836 | ntokens 19918.1 | nsentences 1 | wps 115123 | ups 5.78 | wpb 19918.1 | bsz 1 | num_updates 1222 | lr 1.99998e-05 | gnorm 2.383 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 216\n",
      "2023-10-18 20:51:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:44 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 23\n",
      "2023-10-18 20:51:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 023:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:51:44 | INFO | fairseq.trainer | begin training epoch 23\n",
      "2023-10-18 20:51:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 023:  96%|██████████████████████████████▊ | 54/56 [00:06<00:00, 10.57it/s]2023-10-18 20:51:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:51:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:51 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 24\n",
      "\n",
      "epoch 023 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  25%|██      | 1/4 [00:00<00:02,  1.19it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  50%|████    | 2/4 [00:01<00:00,  2.16it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset: 100%|████████| 4/4 [00:01<00:00,  4.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:51:52 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.835 | ntokens 22374 | nsentences 1 | wps 209530 | wpb 22374 | bsz 1 | num_updates 1278 | best_loss 4.835\n",
      "2023-10-18 20:51:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1278 updates\n",
      "2023-10-18 20:51:52 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:51:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:51:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 23 @ 1278 updates, score 4.835) (writing took 1.3131758559902664 seconds)\n",
      "2023-10-18 20:51:54 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
      "2023-10-18 20:51:54 | INFO | train | epoch 023 | loss 4.835 | ntokens 19918.1 | nsentences 1 | wps 116352 | ups 5.84 | wpb 19918.1 | bsz 1 | num_updates 1278 | lr 1.99998e-05 | gnorm 1.438 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 226\n",
      "2023-10-18 20:51:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:51:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 24\n",
      "2023-10-18 20:51:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 024:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:51:54 | INFO | fairseq.trainer | begin training epoch 24\n",
      "2023-10-18 20:51:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 024:  98%|▉| 55/56 [00:06<00:00, 10.76it/s, loss=4.835, ntokens=19912.1, n2023-10-18 20:52:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:52:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:52:01 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 25\n",
      "\n",
      "epoch 024 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  25%|██      | 1/4 [00:01<00:03,  1.03s/it]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:52:02 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.835 | ntokens 22374 | nsentences 1 | wps 375587 | wpb 22374 | bsz 1 | num_updates 1334 | best_loss 4.835\n",
      "2023-10-18 20:52:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1334 updates\n",
      "2023-10-18 20:52:02 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:52:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:52:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 24 @ 1334 updates, score 4.835) (writing took 1.2976578880043235 seconds)\n",
      "2023-10-18 20:52:03 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
      "2023-10-18 20:52:03 | INFO | train | epoch 024 | loss 4.835 | ntokens 19918.1 | nsentences 1 | wps 115647 | ups 5.81 | wpb 19918.1 | bsz 1 | num_updates 1334 | lr 1.99998e-05 | gnorm 1.686 | loss_scale 0.125 | train_wall 5 | gb_free 46.3 | wall 235\n",
      "2023-10-18 20:52:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:52:04 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 25\n",
      "2023-10-18 20:52:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 56\n",
      "epoch 025:   0%|                                         | 0/56 [00:00<?, ?it/s]2023-10-18 20:52:04 | INFO | fairseq.trainer | begin training epoch 25\n",
      "2023-10-18 20:52:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 025:  98%|███████████████████████████████▍| 55/56 [00:06<00:00,  9.05it/s]2023-10-18 20:52:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-18 20:52:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:52:11 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 26\n",
      "\n",
      "epoch 025 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  25%|██      | 1/4 [00:01<00:03,  1.03s/it]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  75%|██████  | 3/4 [00:01<00:00,  3.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-10-18 20:52:12 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.835 | ntokens 22374 | nsentences 1 | wps 375169 | wpb 22374 | bsz 1 | num_updates 1390 | best_loss 4.835\n",
      "2023-10-18 20:52:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1390 updates\n",
      "2023-10-18 20:52:12 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:52:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-18 20:52:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 25 @ 1390 updates, score 4.835) (writing took 1.296240626979852 seconds)\n",
      "2023-10-18 20:52:13 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
      "2023-10-18 20:52:13 | INFO | train | epoch 025 | loss 4.835 | ntokens 19918.1 | nsentences 1 | wps 113802 | ups 5.71 | wpb 19918.1 | bsz 1 | num_updates 1390 | lr 1.99998e-05 | gnorm 1.458 | loss_scale 0.125 | train_wall 5 | gb_free 46.7 | wall 245\n",
      "2023-10-18 20:52:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-18 20:52:13 | INFO | fairseq_cli.train | done training in 242.7 seconds\n"
     ]
    }
   ],
   "source": [
    "!python3 -c 'import argparse; print(argparse.__file__)'\n",
    "!python3 /home/bayuan/Documents/fall23/fairseq/train.py \\\n",
    "    /home/bayuan/Documents/fall23/ecog2vec/manifest \\\n",
    "  --save-dir /home/bayuan/Documents/fall23/ecog2vec/model \\\n",
    "  --num-workers 6 --fp16 --max-update 400000 --save-interval 1 --no-epoch-checkpoints \\\n",
    "  --arch wav2vec --task audio_pretraining --min-lr 1e-06 --stop-min-lr 1e-09 --optimizer adam --lr 0.00002 --lr-scheduler cosine \\\n",
    "  --conv-feature-layers \"[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]\" \\\n",
    "  --conv-aggregator-layers \"[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]\" \\\n",
    "  --skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion wav2vec --num-negatives 10 \\\n",
    "  --max-sample-size 1500000 --skip-invalid-size-inputs-valid-test --max-epoch 25  --batch-size 1 --max-tokens 150000000 --tensorboard-logdir /home/bayuan/Documents/fall23/ecog2vec/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model, and extract the $c$ vectors from each .nwb file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 20:54:16 | INFO | fairseq.models.wav2vec.wav2vec | Wav2VecModel(\n",
      "  (feature_extractor): ConvFeatureExtractionModel(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(8,), stride=(4,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (2-4): 3 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (5-6): 2 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_aggregator): ConvAggegator(\n",
      "    (conv_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): ReplicationPad1d((1, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(2,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ReplicationPad1d((2, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ReplicationPad1d((3, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(4,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ReplicationPad1d((4, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ReplicationPad1d((5, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(6,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): ReplicationPad1d((6, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): ReplicationPad1d((7, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(8,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): ReplicationPad1d((8, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (8): Sequential(\n",
      "        (0): ReplicationPad1d((9, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(10,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (9): Sequential(\n",
      "        (0): ReplicationPad1d((10, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (10): Sequential(\n",
      "        (0): ReplicationPad1d((11, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(12,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (11): Sequential(\n",
      "        (0): ReplicationPad1d((12, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(13,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (residual_proj): ModuleList(\n",
      "      (0-11): 12 x None\n",
      "    )\n",
      "  )\n",
      "  (wav2vec_predictions): Wav2VecPredictionsModel(\n",
      "    (project_to_steps): ConvTranspose2d(512, 512, kernel_size=(1, 12), stride=(1, 1))\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (dropout_feats): Dropout(p=0.0, inplace=False)\n",
      "  (dropout_agg): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import fairseq\n",
    "# from scipy.io import wavfile\n",
    "import soundfile as sf\n",
    "\n",
    "cp_path = '/home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt'#'/path/to/wav2vec.pt'\n",
    "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
    "model = model[0]\n",
    "model.eval()\n",
    "\n",
    "wav_path = '/home/bayuan/Documents/fall23/ecog2vec/ecog/EFC400/EFC400_B4_1.wav'\n",
    "\n",
    "wav_input_16khz, sr = sf.read(wav_path)\n",
    "wav_input_16khz = wav_input_16khz.T\n",
    "wav_input_16khz = wav_input_16khz.reshape(1, 256, -1)\n",
    "\n",
    "wav_input_16khz = torch.from_numpy(wav_input_16khz).to(torch.float)\n",
    "# print(wav_input_16khz)\n",
    "\n",
    "# print(sr, wav_input_16khz.shape)\n",
    "z = model.feature_extractor(wav_input_16khz)\n",
    "c = model.feature_aggregator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8734, 0.8734, 0.8734,  ..., 0.8673, 0.8721, 0.8783],\n",
      "        [1.1981, 1.1981, 1.1981,  ..., 1.1979, 1.1879, 1.1902],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.6073, 0.6073, 0.6073,  ..., 0.6121, 0.6120, 0.6096],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1701, 0.1701, 0.1701,  ..., 0.1761, 0.1710, 0.1702]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 512, 1873])\n"
     ]
    }
   ],
   "source": [
    "print(c[0][:])\n",
    "print(c.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
