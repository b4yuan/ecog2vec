{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ecog2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess ECoG data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the .nwb files, save to .wav files with sr=16000 for compatibility with wav2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EFC400_B57.nwb', 'EFC400_B72.nwb', 'EFC400_B28.nwb', 'EFC400_B30.nwb', 'EFC400_B14.nwb', 'EFC400_B23.nwb', 'EFC400_B38.nwb', 'EFC400_B6.nwb', 'EFC400_B40.nwb', 'EFC400_B12.nwb', 'EFC400_B42.nwb', 'EFC400_B4.nwb', 'EFC400_B3.nwb', 'EFC400_B19.nwb', 'EFC400_B8.nwb', 'EFC400_B15.nwb', 'EFC400_B46.nwb', 'EFC400_B10.nwb', 'EFC400_B61.nwb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bayuan/anaconda3/envs/wav2vec/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/wav2vec/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(915968, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(869888, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(942592, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(994048, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(1116160, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(826880, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(1042176, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(978432, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(892416, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(898560, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(1039616, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(839936, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(796416, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(1161472, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(995328, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(1055232, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(963584, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(977920, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(1126400, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n",
      "(100000, 256)\n"
     ]
    }
   ],
   "source": [
    "# from nwbwidgets import nwb2widget\n",
    "from pynwb import NWBHDF5IO\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "# path = '/NWB/EFC400/EFC400_B72.nwb' # CHANGE FOR EACH SUBJECT\n",
    "\n",
    "# Open the NWB file for reading\n",
    "# with NWBHDF5IO(path, 'r') as io:\n",
    "#     nwb_file = io.read()\n",
    "\n",
    "folder_path = \"/NWB/EFC400/\"\n",
    "file_list = os.listdir(folder_path)\n",
    "nwb_files = [file for file in file_list if file.startswith(\"EFC400\")]\n",
    "\n",
    "print(nwb_files)\n",
    "\n",
    "for file in nwb_files:\n",
    "    path = os.path.join(folder_path, file)\n",
    "    \n",
    "    io = NWBHDF5IO(path, load_namespaces=True, mode='r')\n",
    "    nwbfile = io.read()\n",
    "\n",
    "    nwbfile_electrodes = nwbfile.acquisition['ElectricalSeries'].data[:]\n",
    "    # print(nwbfile.acquisition['ElectricalSeries'].data[:].shape)\n",
    "\n",
    "\n",
    "    chunk_size = 100000\n",
    "\n",
    "    num_full_chunks = len(nwbfile_electrodes) // chunk_size\n",
    "    # last_chunk_size = len(nwbfile_electrodes) % chunk_size\n",
    "\n",
    "    full_chunks = np.split(nwbfile_electrodes[:num_full_chunks * chunk_size], num_full_chunks)\n",
    "    last_chunk = nwbfile_electrodes[num_full_chunks * chunk_size:]\n",
    "\n",
    "    chunks = full_chunks # + [last_chunk] omit the last non-100000 chunk\n",
    "\n",
    "    # for chunk in chunks:\n",
    "    #     print(chunk.shape)\n",
    "\n",
    "    # Loop through the chunks and save them as WAV files\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        file_name = f'/home/bayuan/Documents/fall23/ecog2vec/ecog/EFC400/{file}_{i}.wav' # CHANGE FOR EACH SUBJECT\n",
    "        sf.write(file_name, chunk, 16000)  # adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .container-fields {\n",
       "                font-family: \"Open Sans\", Arial, sans-serif;\n",
       "            }\n",
       "            .container-fields .field-value {\n",
       "                color: #00788E;\n",
       "            }\n",
       "            .container-fields details > summary {\n",
       "                cursor: pointer;\n",
       "                display: list-item;\n",
       "            }\n",
       "            .container-fields details > summary:hover {\n",
       "                color: #0A6EAA;\n",
       "            }\n",
       "        </style>\n",
       "        \n",
       "        <script>\n",
       "            function copyToClipboard(text) {\n",
       "                navigator.clipboard.writeText(text).then(function() {\n",
       "                    console.log('Copied to clipboard: ' + text);\n",
       "                }, function(err) {\n",
       "                    console.error('Could not copy text: ', err);\n",
       "                });\n",
       "            }\n",
       "\n",
       "            document.addEventListener('DOMContentLoaded', function() {\n",
       "                let fieldKeys = document.querySelectorAll('.container-fields .field-key');\n",
       "                fieldKeys.forEach(function(fieldKey) {\n",
       "                    fieldKey.addEventListener('click', function() {\n",
       "                        let accessCode = fieldKey.getAttribute('title').replace('Access code: ', '');\n",
       "                        copyToClipboard(accessCode);\n",
       "                    });\n",
       "                });\n",
       "            });\n",
       "        </script>\n",
       "        <div class='container-wrap'><div class='container-header'><div class='xr-obj-type'><h3>root (NWBFile)</h3></div></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['session_description']\">session_description:</span> <span class=\"field-value\">NWB File</span></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['identifier']\">identifier:</span> <span class=\"field-value\">EC61_B30</span></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['session_start_time']\">session_start_time:</span> <span class=\"field-value\">2014-06-05 03:37:41-07:00</span></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['timestamps_reference_time']\">timestamps_reference_time:</span> <span class=\"field-value\">2014-06-05 03:37:41-07:00</span></div><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['file_create_date']\"><b>file_create_date</b></summary><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-value\" title=\".fields['file_create_date'][0]\">2019-10-24 21:02:56.725378-07:00</span></div></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['acquisition']\"><b>acquisition (2)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['ElectricalSeries']\"><b>ElectricalSeries</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['rate']\">rate:</span> <span class=\"field-value\">3051.7578125</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['resolution']\">resolution:</span> <span class=\"field-value\">0.00032768</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['description']\">description:</span> <span class=\"field-value\">all Wav data</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['unit']\">unit:</span> <span class=\"field-value\">volts</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes']\"><b>electrodes</b></summary><div style=\"margin-left: 60px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['description']\">description:</span> <span class=\"field-value\">electrodes</span></div><details><summary style=\"display: list-item; margin-left: 60px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['table']\"><b>table</b></summary><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['table'].fields['description']\">description:</span> <span class=\"field-value\">metadata about extracellular electrodes</span></div><details><summary style=\"display: list-item; margin-left: 80px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['table'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['table'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('x', 'y', 'z', 'imp', 'location', 'filtering', 'group', 'group_name', 'label', 'bad', 'x_warped', 'y_warped', 'z_warped')</span></div><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['ElectricalSeries'].fields['electrodes'].fields['table'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3881cf2700>, <hdmf.common.table.VectorData object at 0x7f3881cf2b20>, <hdmf.common.table.VectorData object at 0x7f3881cf2d60>, <hdmf.common.table.VectorData object at 0x7f3881cf2640>, <hdmf.common.table.VectorData object at 0x7f3881cf2670>, <hdmf.common.table.VectorData object at 0x7f388235fbb0>, <hdmf.common.table.VectorData object at 0x7f388235f160>, <hdmf.common.table.VectorData object at 0x7f388235fbe0>, <hdmf.common.table.VectorData object at 0x7f3881cf29d0>, <hdmf.common.table.VectorData object at 0x7f388235f2b0>, <hdmf.common.table.VectorData object at 0x7f3881cf26d0>, <hdmf.common.table.VectorData object at 0x7f3881cf2730>, <hdmf.common.table.VectorData object at 0x7f3881ef59a0>)</span></div></details></details></details><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['anin4']\"><b>anin4</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['rate']\">rate:</span> <span class=\"field-value\">24414.0625</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['resolution']\">resolution:</span> <span class=\"field-value\">4.096e-05</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['description']\">description:</span> <span class=\"field-value\">anin4</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['unit']\">unit:</span> <span class=\"field-value\">amplitude</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['acquisition']['anin4'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['acquisition']['anin4'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['stimulus']\"><b>stimulus (2)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['stimulus']['speaker1']\"><b>speaker1</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['rate']\">rate:</span> <span class=\"field-value\">24414.0625</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['resolution']\">resolution:</span> <span class=\"field-value\">4.096e-05</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['description']\">description:</span> <span class=\"field-value\">speaker1</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['unit']\">unit:</span> <span class=\"field-value\">amplitude</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['stimulus']['speaker1'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker1'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div></details><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['stimulus']['speaker2']\"><b>speaker2</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['rate']\">rate:</span> <span class=\"field-value\">24414.0625</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['resolution']\">resolution:</span> <span class=\"field-value\">4.096e-05</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['description']\">description:</span> <span class=\"field-value\">speaker2</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['unit']\">unit:</span> <span class=\"field-value\">amplitude</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['stimulus']['speaker2'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['stimulus']['speaker2'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['processing']\"><b>processing (2)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior']\"><b>behavior</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['description']\">description:</span> <span class=\"field-value\">human-subject behavior</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']\"><b>data_interfaces (1)</b></summary><details><summary style=\"display: list-item; margin-left: 60px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs']\"><b>BehavioralEpochs</b></summary><details><summary style=\"display: list-item; margin-left: 80px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']\"><b>interval_series (3)</b></summary><details><summary style=\"display: list-item; margin-left: 100px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription']\"><b>phoneme transcription</b></summary><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['resolution']\">resolution:</span> <span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['description']\">description:</span> <span class=\"field-value\">f y uw p iy p ax l l ih v t ax b iy ax pau hh ah n d r ax d pau dh ax b ah ng g ax l ow w aa z p l eh z ax n t l iy s ih ch uw ey t ih d pau n ih r dh ax sh ao r pau aa r t ax f ih sh ax l ih n t eh l ax jh ax n s ih z f ao r pau r iy l pau hh ih z s ah d ax n d ih p aa r ch er sh aa k t dh ax k ae s t pau t eh k n ih k ax l r ay t er z pau k ae n ax b r iy v iy ey t pau ih n b ih b l iy aa g r ax f iy z pau ax hh y uw jh pau t ae p ax s t r iy pau hh ah ng ih n hh er hh ao l w ey pau t r eh s p ae s ih ng ih z f ao r b ih d ax n pau ae n d s ah b jh ih k t pau t ax p eh n ax l t iy pau dh ax k l ah m z iy k ah s t ax m er pau s p ih l d s ah m ih k s p eh n s ih v pau p er f y uw m pau b er th d ey p aa r t iy z pau hh ae v k ah p k ey k s pau ae n d ay s k r iy m pau w iy p l ae n t ax b ih l d ax n uw b eh v er ih jh pau p l ae n t pau dh ax b eh s t w ey t ax l er n ih z t ax s aa l v eh k s t r ax p r aa b l ax m z pau w ih ch l ao ng aa r t ax k ax l w aa z ow p ey k pau ae n d n iy d ax d k l eh r ax f ax k ey sh ax n pau dh ax s aw n d ah v jh eh n ax f er z b y uw g ax l pau s k eh r d pau dh ax ae n t ax l ow p pau w eh s t ch eh s t er ih z ax k aw n t iy ih n n uw y ao r k pau t uw m ah ch k y uh r iy aa s ax t iy pau k ae n g eh t y uw ih n t uw t r ah b ax l pau dh ax eh m p er er hh ae d ax m iy n pau t eh m p er pau hh iy s t ow l ax pau d ay m pau f r ah m ax b eh g er pau k ih n d er g aa r t ax n pau ch ih l d r ax n pau d eh k er ey t dh eh r k l ae s r uw m z pau f ao r ao l hh aa l ax d ey z pau t ah g b ow t s aa r k ey p ax b ax l ah v hh ao l ih ng hh y uw jh pau l ow d z pau ae ng g ao r ax k ae t s aa r f er iy er dh ae n s ay ax m iy z pau p iy t s er iy ax z aa r k ax n v iy n y ax n t f ao r ax k w ih k pau l ah n ch pau ax m ah s k y ax l er ae b d ow m ax n ih z g uh d pau f ao r y ao r b ae k pau hh ih z s k ae l p pau aa z b l ih s t er d pau f r ah m t ax d ey z hh aa t s ah n pau aa b jh eh k t s m ey d ah v p y uw t er pau aa r b y uw t ax f ax l pau ae g r ax k ah l ch er ax l p r aa d ax k t s pau aa r ax n iy v ax n l iy d ih s t r ih b y ax t ax d pau k ao l ae n ae m b y ax l ax n s f ao r m eh d ax k ax l ax s ih s t ax n s pau dh ax m ae ng g ow ae n d dh ax p ax p ay ax pau aa r ih n ax b ow l pau dh ax eh m b l ax m d ih p ih k t s dh ax ax k r aa p ax l ax s ao l ax g l ow pau dh ax m ih s k w ow t w aa z r iy t r ae k t ax d w ih dh ae n ax p aa l ax jh iy pau k ax m b ay n pau ao l dh ax ih n g r iy d iy ax n t s pau ih n ax l aa r jh pau b ow l pau k l ae s p pau dh ax s k r uw pau ih n y ao r l eh f t hh ae n d pau p l eh jh t ax p aa r t ih s ax p ey t pau ih n n ax v aa d ax z ax k w aa t ih k k aa m p ax t ih sh ax n pau s ay k l ih k ax l p r ow g r ae m z pau w ih l n eh v er k ax m p ay l pau k er eh k t pau eh k s ax k y uw sh ax n ah v m ay ih n s t r ah k sh ax n z pau ih z k r uw sh ax l pau ih n s ay k l ax p iy d iy ax z pau s eh l d ax m p r iy z eh n t pau ae n ax k d ow t ax l eh v ax d ax n s pau dh ax w ih l ax w iy w uh m ax n pau w ao r ax m ah s k r ae t pau k ow t pau dh ax k ay ow t iy pau b aa b k ae t pau ae n d hh ay iy n ax pau aa r w ay l d pau ae n ax m ax l z pau hh iy ey t f ao r pau eh k s t r ax pau eh g z pau f ao r b r eh k f ax s t pau t r ax d ih sh ax n r iy k w ay er z p er eh n t ax l ax p r uw v ax l pau f ao r pau ah n d er ey jh pau m eh r ih jh pau p ax b l ih s ax t iy ae n d n ow t er ay ax t iy pau ow hh ae n d ih n hh ae n d pau m ow s t pau p r iy s ih ng k t s pau hh ae d ax th er d pau ah v dh ax v ow t s k aw n t ax d pau l aa t s ah v f ao r ax n m uw v iy z hh ae v s ah b t ay t ax l z pau sh iy s l ih p t ae n d s p r ey n d hh er pau ae ng k ax l pau aa n dh ax s t iy p s l ow p pau d ih s eh m b er ae n d jh ae n y uw eh r iy aa r n ay s m ah n th s pau t ax s p eh n d ih n m ay ae m iy pau s ay ax n t ih f ih k p r aa g r eh s k ah m z f r ah m dh ax d ih v eh l ax p m ax n t pau ah v n uw t eh k n iy k s pau s p eh sh ax l t ae s k f ao r s ih z pau r eh s k y uw hh aa s t ax jh ax z pau f r ah m k ih d n ae p er z pau ax s k r uw d r ay v er ih z m ey d f r ah m v aa d k ax pau ae n d ao r ax n jh jh uw s pau y uw m pau ah s t ih k s p l ih s ax t l iy pau d ih l iy t pau f ay l z pau p r aa jh eh k t d ih v eh l ax p m ax n t pau w aa z p r ax s iy d ih ng pau t uw pau s l ow l iy pau dh ax k aa r t uw n f iy ch er f iy ch er z pau ax m ah s k r ae t pau ae n d ax t ae d p ow l</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['unit']\">unit:</span> <span class=\"field-value\">n/a</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['data']\"><b>data</b></summary></details><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['timestamps']\"><b>timestamps</b></summary></details><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['timestamps_unit']\">timestamps_unit:</span> <span class=\"field-value\">seconds</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['phoneme transcription'].fields['interval']\">interval:</span> <span class=\"field-value\">1</span></div></details><details><summary style=\"display: list-item; margin-left: 100px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription']\"><b>syllable transcription</b></summary><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['resolution']\">resolution:</span> <span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['description']\">description:</span> <span class=\"field-value\">f_y_uw p_iy p_ax_l l_ih_v t_ax b_iy ax hh_ah_n d_r_ax_d dh_ax b_ah_ng g_ax l_ow w_aa_z p_l_eh z_ax_n_t l_iy s_ih ch_uw ey t_ih_d n_ih_r dh_ax sh_ao_r aa_r t_ax f_ih sh_ax_l ih_n t_eh l_ax jh_ax_n_s ih_z f_ao_r r_iy_l hh_ih_z s_ah d_ax_n d_ih p_aa_r ch_er sh_aa_k_t dh_ax k_ae_s_t t_eh_k n_ih k_ax_l r_ay t_er_z k_ae_n ax b_r_iy v_iy ey_t ih_n b_ih b_l_iy aa g_r_ax f_iy_z ax hh_y_uw_jh t_ae p_ax s_t_r_iy hh_ah_ng ih_n hh_er hh_ao_l w_ey t_r_eh s_p_ae s_ih_ng ih_z f_ao_r b_ih d_ax_n ae_n_d s_ah_b jh_ih_k_t t_ax p_eh n_ax_l t_iy dh_ax k_l_ah_m z_iy k_ah s_t_ax m_er s_p_ih_l_d s_ah_m ih_k s_p_eh_n s_ih_v p_er f_y_uw_m b_er_th d_ey p_aa_r t_iy_z hh_ae_v k_ah_p k_ey_k_s ae_n_d ay_s k_r_iy_m w_iy p_l_ae_n t_ax b_ih_l_d ax n_uw b_eh v_er ih_jh p_l_ae_n_t dh_ax b_eh_s_t w_ey t_ax l_er_n ih_z t_ax s_aa_l_v eh_k s_t_r_ax p_r_aa b_l_ax_m_z w_ih_ch l_ao_ng aa_r t_ax k_ax_l w_aa_z ow p_ey_k ae_n_d n_iy d_ax_d k_l_eh r_ax f_ax k_ey sh_ax_n dh_ax s_aw_n_d ah_v jh_eh n_ax f_er_z b_y_uw g_ax_l s_k_eh_r_d dh_ax ae_n t_ax l_ow_p w_eh_s_t ch_eh s_t_er ih_z ax k_aw_n t_iy ih_n n_uw y_ao_r_k t_uw m_ah_ch k_y_uh r_iy aa s_ax t_iy k_ae_n g_eh_t y_uw ih_n t_uw t_r_ah b_ax_l dh_ax eh_m p_er er hh_ae_d ax m_iy_n t_eh_m p_er hh_iy s_t_ow_l ax d_ay_m f_r_ah_m ax b_eh g_er k_ih_n d_er g_aa_r t_ax_n ch_ih_l d_r_ax_n d_eh k_er ey_t dh_eh_r k_l_ae s_r_uw_m_z f_ao_r ao_l hh_aa l_ax d_ey_z t_ah_g b_ow_t_s aa_r k_ey p_ax b_ax_l ah_v hh_ao l_ih_ng hh_y_uw_jh l_ow_d_z ae_ng g_ao r_ax k_ae_t_s aa_r f_er iy er dh_ae_n s_ay ax m_iy_z p_iy_t s_er iy ax_z aa_r k_ax_n v_iy n_y_ax_n_t f_ao_r ax k_w_ih_k l_ah_n_ch ax m_ah s_k_y_ax l_er ae_b d_ow m_ax_n ih_z g_uh_d f_ao_r y_ao_r b_ae_k hh_ih_z s_k_ae_l_p w_aa_z b_l_ih s_t_er_d f_r_ah_m t_ax d_ey_z hh_aa_t s_ah_n aa_b jh_eh_k_t_s m_ey_d ah_v p_y_uw t_er aa_r b_y_uw t_ax f_ax_l ae g_r_ax k_ah_l ch_er ax_l p_r_aa d_ax_k_t_s aa_r ax n_iy v_ax_n l_iy d_ih s_t_r_ih b_y_ax t_ax_d k_ao_l ae_n ae_m b_y_ax l_ax_n_s f_ao_r m_eh d_ax k_ax_l ax s_ih s_t_ax_n_s dh_ax m_ae_ng g_ow ae_n_d dh_ax p_ax p_ay ax aa_r ih_n ax b_ow_l dh_ax eh_m b_l_ax_m d_ih p_ih_k_t_s dh_ax ax k_r_aa p_ax l_ax_s ao_l ax g_l_ow dh_ax m_ih s_k_w_ow_t w_aa_z r_iy t_r_ae_k t_ax_d w_ih_dh ae_n ax p_aa l_ax jh_iy k_ax_m b_ay_n ao_l dh_ax ih_n g_r_iy d_iy ax_n_t_s ih_n ax l_aa_r_jh b_ow_l k_l_ae_s_p dh_ax s_k_r_uw ih_n y_ao_r l_eh_f_t hh_ae_n_d p_l_eh_jh t_ax p_aa_r t_ih s_ax p_ey_t ih_n n_ax v_aa d_ax_z ax k_w_aa t_ih_k k_aa_m p_ax t_ih sh_ax_n s_ay k_l_ih k_ax_l p_r_ow g_r_ae_m_z w_ih_l n_eh v_er k_ax_m p_ay_l k_er eh_k_t eh_k s_ax k_y_uw sh_ax_n ah_v m_ay ih_n s_t_r_ah_k sh_ax_n_z ih_z k_r_uw sh_ax_l ih_n s_ay k_l_ax p_iy d_iy ax_z s_eh_l d_ax_m p_r_iy z_eh_n_t ae n_ax_k d_ow t_ax_l eh v_ax d_ax_n_s dh_ax w_ih l_ax w_iy w_uh m_ax_n w_ao_r ax m_ah s_k_r_ae_t k_ow_t dh_ax k_ay ow t_iy b_aa_b k_ae_t ae_n_d hh_ay iy n_ax aa_r w_ay_l_d ae n_ax m_ax_l_z hh_iy ey_t f_ao_r eh_k s_t_r_ax eh_g_z f_ao_r b_r_eh_k f_ax_s_t t_r_ax d_ih sh_ax_n r_iy k_w_ay er_z p_er eh_n t_ax_l ax p_r_uw v_ax_l f_ao_r ah_n d_er ey_jh m_eh r_ih_jh p_ax b_l_ih s_ax t_iy ae_n_d n_ow t_er ay ax t_iy g_ow hh_ae_n_d ih_n hh_ae_n_d m_ow_s_t p_r_iy s_ih_ng_k_t_s hh_ae_d ax th_er_d ah_v dh_ax v_ow_t_s k_aw_n t_ax_d l_aa_t_s ah_v f_ao r_ax_n m_uw v_iy_z hh_ae_v s_ah_b t_ay t_ax_l_z sh_iy s_l_ih_p_t ae_n_d s_p_r_ey_n_d hh_er ae_ng k_ax_l aa_n dh_ax s_t_iy_p s_l_ow_p d_ih s_eh_m b_er ae_n_d jh_ae n_y_uw eh r_iy aa_r n_ay_s m_ah_n_th_s t_ax s_p_eh_n_d ih_n m_ay ae m_iy s_ay ax_n t_ih f_ih_k p_r_aa g_r_eh_s k_ah_m_z f_r_ah_m dh_ax d_ih v_eh l_ax_p m_ax_n_t ah_v n_uw t_eh_k n_iy_k_s s_p_eh sh_ax_l t_ae_s_k f_ao_r s_ih_z r_eh s_k_y_uw hh_aa s_t_ax jh_ax_z f_r_ah_m k_ih_d n_ae p_er_z ax s_k_r_uw d_r_ay v_er ih_z m_ey_d f_r_ah_m v_aa_d k_ax ae_n_d ao r_ax_n_jh jh_uw_s y_uw m_ah_s_t ih_k s_p_l_ih s_ax_t l_iy d_ih l_iy_t f_ay_l_z p_r_aa jh_eh_k_t d_ih v_eh l_ax_p m_ax_n_t w_aa_z p_r_ax s_iy d_ih_ng t_uw s_l_ow l_iy dh_ax k_aa_r t_uw_n f_iy ch_er f_iy ch_er_z ax m_ah s_k_r_ae_t ae_n_d ax t_ae_d p_ow_l</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['unit']\">unit:</span> <span class=\"field-value\">n/a</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['data']\"><b>data</b></summary></details><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['timestamps']\"><b>timestamps</b></summary></details><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['timestamps_unit']\">timestamps_unit:</span> <span class=\"field-value\">seconds</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['syllable transcription'].fields['interval']\">interval:</span> <span class=\"field-value\">1</span></div></details><details><summary style=\"display: list-item; margin-left: 100px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription']\"><b>word transcription</b></summary><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['resolution']\">resolution:</span> <span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['description']\">description:</span> <span class=\"field-value\">Few people live to be a hundred The bungalow was pleasantly situated near the shore Artificial intelligence is for real His sudden departure shocked the cast Technical writers can abbreviate in bibliographies A huge tapestry hung in her hallway Trespassing is forbidden and subject to penalty The clumsy customer spilled some expensive perfume Birthday parties have cupcakes and ice cream We plan to build a new beverage plant The best way to learn is to solve extra problems Which long article was opaque and needed clarification The sound of Jennifer bugle scared the antelope Westchester is a county in New York Too much curiosity can get you into trouble The emperor had a mean temper He stole a dime from a beggar Kindergarten children decorate their classrooms for all holidays Tugboats are capable of hauling huge loads Angora cats are furrier than Siamese Pizzerias are convenient for a quick lunch A muscular abdomen is good for your back His scalp was blistered from today hot sun Objects made of pewter are beautiful Agricultural products are unevenly distributed Call an ambulance for medical assistance The mango and the papaya are in a bowl The emblem depicts the Acropolis all aglow The misquote was retracted with an apology Combine all the ingredients in a large bowl Clasp the screw in your left hand Pledge to participate in Nevada aquatic competition Cyclical programs will never compile Correct execution of my instructions is crucial Encyclopedias seldom present anecdotal evidence The willowy woman wore a muskrat coat The coyote bobcat and hyena are wild animals He ate four extra eggs for breakfast Tradition requires parental approval for under age marriage Publicity and notoriety go hand in hand Most precincts had a third of the votes counted Lots of foreign movies have subtitles She slipped and sprained her ankle on the steep slope December and January are nice months to spend in Miami Scientific progress comes from the development of new techniques Special task forces rescue hostages from kidnappers A screwdriver is made from vodka and orange juice You must explicitly delete files Project development was proceeding too slowly The cartoon feature features a muskrat and a tadpole</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['unit']\">unit:</span> <span class=\"field-value\">n/a</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['data']\"><b>data</b></summary></details><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['timestamps']\"><b>timestamps</b></summary></details><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['timestamps_unit']\">timestamps_unit:</span> <span class=\"field-value\">seconds</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['behavior'].fields['data_interfaces']['BehavioralEpochs'].fields['interval_series']['word transcription'].fields['interval']\">interval:</span> <span class=\"field-value\">1</span></div></details></details></details></details></details><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys']\"><b>ecephys</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['description']\">description:</span> <span class=\"field-value\">Extracellular electrophysiology data.</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']\"><b>data_interfaces (2)</b></summary><details><summary style=\"display: list-item; margin-left: 60px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP']\"><b>LFP</b></summary><details><summary style=\"display: list-item; margin-left: 80px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']\"><b>electrical_series (2)</b></summary><details><summary style=\"display: list-item; margin-left: 100px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)']\"><b>high gamma (bipolar)</b></summary><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['rate']\">rate:</span> <span class=\"field-value\">400.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['resolution']\">resolution:</span> <span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['comments']\">comments:</span> <span class=\"field-value\">no comments</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['description']\">description:</span> <span class=\"field-value\">no description</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['unit']\">unit:</span> <span class=\"field-value\">volts</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes']\"><b>electrodes</b></summary><div style=\"margin-left: 140px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['description']\">description:</span> <span class=\"field-value\">all bipolar electrodes</span></div><details><summary style=\"display: list-item; margin-left: 140px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['table']\"><b>table</b></summary><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['table'].fields['description']\">description:</span> <span class=\"field-value\">pseudo-channels derived via John Burke style bipolar referencing</span></div><details><summary style=\"display: list-item; margin-left: 160px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['table'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['table'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('x', 'y', 'z', 'imp', 'location', 'label', 'bad')</span></div><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['high gamma (bipolar)'].fields['electrodes'].fields['table'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3881c6b970>, <hdmf.common.table.VectorData object at 0x7f3881c6bfa0>, <hdmf.common.table.VectorData object at 0x7f3881c6bb50>, <hdmf.common.table.VectorData object at 0x7f38823a2340>, <hdmf.common.table.VectorData object at 0x7f3881c6bd30>, <hdmf.common.table.VectorData object at 0x7f38823a2430>, <hdmf.common.table.VectorData object at 0x7f38823a2100>)</span></div></details></details></details><details><summary style=\"display: list-item; margin-left: 100px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)']\"><b>preprocessed (bipolar)</b></summary><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['starting_time']\">starting_time:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['rate']\">rate:</span> <span class=\"field-value\">400.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['resolution']\">resolution:</span> <span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['comments']\">comments:</span> <span class=\"field-value\">referencing:bipolar,Notch:None, Downsampled:Yes</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['description']\">description:</span> <span class=\"field-value\">no description</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['conversion']\">conversion:</span> <span class=\"field-value\">1.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['offset']\">offset:</span> <span class=\"field-value\">0.0</span></div><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['unit']\">unit:</span> <span class=\"field-value\">volts</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['data']\"><b>data</b></summary></details><div style=\"margin-left: 120px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['starting_time_unit']\">starting_time_unit:</span> <span class=\"field-value\">seconds</span></div><details><summary style=\"display: list-item; margin-left: 120px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes']\"><b>electrodes</b></summary><div style=\"margin-left: 140px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['description']\">description:</span> <span class=\"field-value\">all bipolar electrodes</span></div><details><summary style=\"display: list-item; margin-left: 140px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['table']\"><b>table</b></summary><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['table'].fields['description']\">description:</span> <span class=\"field-value\">pseudo-channels derived via John Burke style bipolar referencing</span></div><details><summary style=\"display: list-item; margin-left: 160px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['table'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['table'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('x', 'y', 'z', 'imp', 'location', 'label', 'bad')</span></div><div style=\"margin-left: 160px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['LFP'].fields['electrical_series']['preprocessed (bipolar)'].fields['electrodes'].fields['table'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3881c6b970>, <hdmf.common.table.VectorData object at 0x7f3881c6bfa0>, <hdmf.common.table.VectorData object at 0x7f3881c6bb50>, <hdmf.common.table.VectorData object at 0x7f38823a2340>, <hdmf.common.table.VectorData object at 0x7f3881c6bd30>, <hdmf.common.table.VectorData object at 0x7f38823a2430>, <hdmf.common.table.VectorData object at 0x7f38823a2100>)</span></div></details></details></details></details></details><details><summary style=\"display: list-item; margin-left: 60px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['bipolar-referenced metadata']\"><b>bipolar-referenced metadata</b></summary><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['bipolar-referenced metadata'].fields['description']\">description:</span> <span class=\"field-value\">pseudo-channels derived via John Burke style bipolar referencing</span></div><details><summary style=\"display: list-item; margin-left: 80px;\" class=\"container-fields field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['bipolar-referenced metadata'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['bipolar-referenced metadata'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('x', 'y', 'z', 'imp', 'location', 'label', 'bad')</span></div><div style=\"margin-left: 80px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['processing']['ecephys'].fields['data_interfaces']['bipolar-referenced metadata'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3881c6b970>, <hdmf.common.table.VectorData object at 0x7f3881c6bfa0>, <hdmf.common.table.VectorData object at 0x7f3881c6bb50>, <hdmf.common.table.VectorData object at 0x7f38823a2340>, <hdmf.common.table.VectorData object at 0x7f3881c6bd30>, <hdmf.common.table.VectorData object at 0x7f38823a2430>, <hdmf.common.table.VectorData object at 0x7f38823a2100>)</span></div></details></details></details></details><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['epoch_tags']\">epoch_tags:</span> <span class=\"field-value\">set()</span></div><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['electrodes']\"><b>electrodes</b></summary><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['electrodes'].fields['description']\">description:</span> <span class=\"field-value\">metadata about extracellular electrodes</span></div><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['electrodes'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['electrodes'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('x', 'y', 'z', 'imp', 'location', 'filtering', 'group', 'group_name', 'label', 'bad', 'x_warped', 'y_warped', 'z_warped')</span></div><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['electrodes'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3881cf2700>, <hdmf.common.table.VectorData object at 0x7f3881cf2b20>, <hdmf.common.table.VectorData object at 0x7f3881cf2d60>, <hdmf.common.table.VectorData object at 0x7f3881cf2640>, <hdmf.common.table.VectorData object at 0x7f3881cf2670>, <hdmf.common.table.VectorData object at 0x7f388235fbb0>, <hdmf.common.table.VectorData object at 0x7f388235f160>, <hdmf.common.table.VectorData object at 0x7f388235fbe0>, <hdmf.common.table.VectorData object at 0x7f3881cf29d0>, <hdmf.common.table.VectorData object at 0x7f388235f2b0>, <hdmf.common.table.VectorData object at 0x7f3881cf26d0>, <hdmf.common.table.VectorData object at 0x7f3881cf2730>, <hdmf.common.table.VectorData object at 0x7f3881ef59a0>)</span></div></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['electrode_groups']\"><b>electrode_groups (1)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['electrode_groups']['R256GridElectrode electrodes']\"><b>R256GridElectrode electrodes</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['electrode_groups']['R256GridElectrode electrodes'].fields['description']\">description:</span> <span class=\"field-value\">R256GridElectrode</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['electrode_groups']['R256GridElectrode electrodes'].fields['location']\">location:</span> <span class=\"field-value\">R256</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['electrode_groups']['R256GridElectrode electrodes'].fields['device']\"><b>device</b></summary></details></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['devices']\"><b>devices (1)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['devices']['R256GridElectrode']\"><b>R256GridElectrode</b></summary></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['intervals']\"><b>intervals (1)</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['intervals']['trials']\"><b>trials</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['intervals']['trials'].fields['description']\">description:</span> <span class=\"field-value\">experimental trials</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".fields['intervals']['trials'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['intervals']['trials'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('start_time', 'stop_time', 'transcription', 'production_type')</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['intervals']['trials'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3882319490>, <hdmf.common.table.VectorData object at 0x7f3882319430>, <hdmf.common.table.VectorData object at 0x7f3882688760>, <hdmf.common.table.VectorData object at 0x7f3882688670>)</span></div></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['subject']\"><b>subject</b></summary><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['subject'].fields['age__reference']\">age__reference:</span> <span class=\"field-value\">birth</span></div><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['subject'].fields['subject_id']\">subject_id:</span> <span class=\"field-value\">EC61</span></div></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".fields['trials']\"><b>trials</b></summary><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['trials'].fields['description']\">description:</span> <span class=\"field-value\">experimental trials</span></div><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".fields['trials'].fields['id']\"><b>id</b></summary></details><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['trials'].fields['colnames']\">colnames:</span> <span class=\"field-value\">('start_time', 'stop_time', 'transcription', 'production_type')</span></div><div style=\"margin-left: 20px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['trials'].fields['columns']\">columns:</span> <span class=\"field-value\">(<hdmf.common.table.VectorData object at 0x7f3882319490>, <hdmf.common.table.VectorData object at 0x7f3882319430>, <hdmf.common.table.VectorData object at 0x7f3882688760>, <hdmf.common.table.VectorData object at 0x7f3882688670>)</span></div></details><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['lab']\">lab:</span> <span class=\"field-value\">Chang Lab</span></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".fields['institution']\">institution:</span> <span class=\"field-value\">University of California, San Francisco</span></div></div>"
      ],
      "text/plain": [
       "root pynwb.file.NWBFile at 0x139880672181744\n",
       "Fields:\n",
       "  acquisition: {\n",
       "    ElectricalSeries <class 'pynwb.ecephys.ElectricalSeries'>,\n",
       "    anin4 <class 'pynwb.base.TimeSeries'>\n",
       "  }\n",
       "  devices: {\n",
       "    R256GridElectrode <class 'pynwb.device.Device'>\n",
       "  }\n",
       "  electrode_groups: {\n",
       "    R256GridElectrode electrodes <class 'pynwb.ecephys.ElectrodeGroup'>\n",
       "  }\n",
       "  electrodes: electrodes <class 'hdmf.common.table.DynamicTable'>\n",
       "  file_create_date: [datetime.datetime(2019, 10, 24, 21, 2, 56, 725378, tzinfo=tzoffset(None, -25200))]\n",
       "  identifier: EC61_B30\n",
       "  institution: University of California, San Francisco\n",
       "  intervals: {\n",
       "    trials <class 'pynwb.epoch.TimeIntervals'>\n",
       "  }\n",
       "  lab: Chang Lab\n",
       "  processing: {\n",
       "    behavior <class 'pynwb.base.ProcessingModule'>,\n",
       "    ecephys <class 'pynwb.base.ProcessingModule'>\n",
       "  }\n",
       "  session_description: NWB File\n",
       "  session_start_time: 2014-06-05 03:37:41-07:00\n",
       "  stimulus: {\n",
       "    speaker1 <class 'pynwb.base.TimeSeries'>,\n",
       "    speaker2 <class 'pynwb.base.TimeSeries'>\n",
       "  }\n",
       "  subject: subject abc.ECoGSubject at 0x139880672180064\n",
       "Fields:\n",
       "  age__reference: birth\n",
       "  subject_id: EC61\n",
       "\n",
       "  timestamps_reference_time: 2014-06-05 03:37:41-07:00\n",
       "  trials: trials <class 'pynwb.epoch.TimeIntervals'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwbfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train wav2vec on ecog data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training manifest from the dataset. Train the model with 256-channel inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /home/bayuan/Documents/fall23/fairseq/examples/wav2vec/wav2vec_manifest.py \\\n",
    "    /home/bayuan/Documents/fall23/ecog2vec/ecog/EFC400 \\\n",
    "  --dest /home/bayuan/Documents/fall23/ecog2vec/manifest \\\n",
    "  --ext wav \\\n",
    "  --valid-percent 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bayuan/anaconda3/envs/wav2vec/lib/python3.9/argparse.py\n",
      "2023-10-19 00:41:02 | INFO | fairseq_cli.train | Args: \n",
      "2023-10-19 00:41:02 | INFO | fairseq.models.wav2vec.wav2vec | Wav2VecModel(\n",
      "  (feature_extractor): ConvFeatureExtractionModel(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(8,), stride=(4,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (2-4): 3 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (5-6): 2 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_aggregator): ConvAggegator(\n",
      "    (conv_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): ReplicationPad1d((1, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(2,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ReplicationPad1d((2, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ReplicationPad1d((3, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(4,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ReplicationPad1d((4, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ReplicationPad1d((5, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(6,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): ReplicationPad1d((6, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): ReplicationPad1d((7, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(8,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): ReplicationPad1d((8, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (8): Sequential(\n",
      "        (0): ReplicationPad1d((9, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(10,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (9): Sequential(\n",
      "        (0): ReplicationPad1d((10, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (10): Sequential(\n",
      "        (0): ReplicationPad1d((11, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(12,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (11): Sequential(\n",
      "        (0): ReplicationPad1d((12, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(13,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (residual_proj): ModuleList(\n",
      "      (0-11): 12 x None\n",
      "    )\n",
      "  )\n",
      "  (wav2vec_predictions): Wav2VecPredictionsModel(\n",
      "    (project_to_steps): ConvTranspose2d(512, 512, kernel_size=(1, 12), stride=(1, 1))\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (dropout_feats): Dropout(p=0.0, inplace=False)\n",
      "  (dropout_agg): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "2023-10-19 00:41:02 | INFO | fairseq_cli.train | Wav2VecModel(\n",
      "  (feature_extractor): ConvFeatureExtractionModel(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(8,), stride=(4,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (2-4): 3 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (5-6): 2 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_aggregator): ConvAggegator(\n",
      "    (conv_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): ReplicationPad1d((1, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(2,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ReplicationPad1d((2, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ReplicationPad1d((3, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(4,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ReplicationPad1d((4, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ReplicationPad1d((5, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(6,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): ReplicationPad1d((6, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): ReplicationPad1d((7, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(8,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): ReplicationPad1d((8, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (8): Sequential(\n",
      "        (0): ReplicationPad1d((9, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(10,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (9): Sequential(\n",
      "        (0): ReplicationPad1d((10, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (10): Sequential(\n",
      "        (0): ReplicationPad1d((11, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(12,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (11): Sequential(\n",
      "        (0): ReplicationPad1d((12, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(13,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (residual_proj): ModuleList(\n",
      "      (0-11): 12 x None\n",
      "    )\n",
      "  )\n",
      "  (wav2vec_predictions): Wav2VecPredictionsModel(\n",
      "    (project_to_steps): ConvTranspose2d(512, 512, kernel_size=(1, 12), stride=(1, 1))\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (dropout_feats): Dropout(p=0.0, inplace=False)\n",
      "  (dropout_agg): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "2023-10-19 00:41:02 | INFO | fairseq_cli.train | task: AudioPretrainingTask\n",
      "2023-10-19 00:41:02 | INFO | fairseq_cli.train | model: Wav2VecModel\n",
      "2023-10-19 00:41:02 | INFO | fairseq_cli.train | criterion: Wav2vecCriterion\n",
      "2023-10-19 00:41:02 | INFO | fairseq_cli.train | num. shared model params: 33,842,688 (num. trained: 33,842,688)\n",
      "2023-10-19 00:41:02 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-10-19 00:41:02 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 10, skipped 0 samples\n",
      "2023-10-19 00:41:02 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.1.0.bias\n",
      "2023-10-19 00:41:02 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.2.0.bias\n",
      "2023-10-19 00:41:02 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.3.0.bias\n",
      "2023-10-19 00:41:02 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.4.0.bias\n",
      "2023-10-19 00:41:02 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.5.0.bias\n",
      "2023-10-19 00:41:02 | INFO | fairseq.trainer | detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.6.0.bias\n",
      "2023-10-19 00:41:02 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-10-19 00:41:02 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.528 GB ; name = NVIDIA RTX A6000                        \n",
      "2023-10-19 00:41:02 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-10-19 00:41:02 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-10-19 00:41:02 | INFO | fairseq_cli.train | max tokens per device = 150000000 and max sentences per device = 10\n",
      "2023-10-19 00:41:02 | INFO | fairseq.trainer | Preparing to load checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:41:02 | INFO | fairseq.trainer | No existing checkpoint found /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:41:02 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-10-19 00:41:02 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 163, skipped 0 samples\n",
      "2023-10-19 00:41:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:41:02 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-10-19 00:41:02 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = True\n",
      "2023-10-19 00:41:02 | INFO | fairseq.tasks.fairseq_task | batches will be rebuilt for each epoch\n",
      "2023-10-19 00:41:02 | INFO | fairseq.data.iterators | First train data shape: torch.Size([100000, 256])\n",
      "2023-10-19 00:41:02 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-10-19 00:41:03 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-10-19 00:41:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:41:03 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-10-19 00:41:03 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = True\n",
      "2023-10-19 00:41:03 | INFO | fairseq.tasks.fairseq_task | batches will be rebuilt for each epoch\n",
      "2023-10-19 00:41:03 | INFO | fairseq.data.iterators | First train data shape: torch.Size([100000, 256])\n",
      "2023-10-19 00:41:03 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-10-19 00:41:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 001:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:41:06 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-10-19 00:41:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-10-19 00:41:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   5%|█▋                               | 1/20 [00:09<02:53,  9.13s/it]2023-10-19 00:41:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:  10%|███▎                             | 2/20 [00:09<01:09,  3.86s/it]2023-10-19 00:41:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:  15%|████▉                            | 3/20 [00:09<00:37,  2.19s/it]2023-10-19 00:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001:  20%|██████▌                          | 4/20 [00:09<00:22,  1.41s/it]2023-10-19 00:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 001:  25%|████████▎                        | 5/20 [00:09<00:14,  1.03it/s]2023-10-19 00:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
      "epoch 001:  30%|█████████▉                       | 6/20 [00:10<00:09,  1.41it/s]2023-10-19 00:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0\n",
      "epoch 001:  35%|███████████▌                     | 7/20 [00:10<00:07,  1.84it/s]2023-10-19 00:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5\n",
      "epoch 001:  40%|█████████████▏                   | 8/20 [00:10<00:05,  2.30it/s]2023-10-19 00:41:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25\n",
      "epoch 001:  45%|██████████████▊                  | 9/20 [00:10<00:03,  2.77it/s]2023-10-19 00:41:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125\n",
      "epoch 001:  50%|████████████████                | 10/20 [00:10<00:03,  3.17it/s]2023-10-19 00:41:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625\n",
      "epoch 001:  95%|██████████████████████████████▍ | 19/20 [00:12<00:00,  5.43it/s]2023-10-19 00:41:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:41:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:41:20 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 2\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:41:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 5.329 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 9\n",
      "2023-10-19 00:41:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 9 updates\n",
      "2023-10-19 00:41:21 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:41:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:41:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 1 @ 9 updates, score 5.329) (writing took 0.9359059490088839 seconds)\n",
      "2023-10-19 00:41:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-10-19 00:41:22 | INFO | train | epoch 001 | loss 7.873 | ntokens 58992 | nsentences 8 | wps 94405.4 | ups 1.6 | wpb 58992 | bsz 8 | num_updates 9 | lr 1.80982e-05 | gnorm 335.564 | loss_scale 0.0625 | train_wall 6 | gb_free 44.8 | wall 20\n",
      "2023-10-19 00:41:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:41:23 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 2\n",
      "2023-10-19 00:41:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 002:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:41:23 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-10-19 00:41:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.83it/s]2023-10-19 00:41:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:41:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:41:31 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 3\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:41:33 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.862 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 29 | best_loss 4.862\n",
      "2023-10-19 00:41:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 29 updates\n",
      "2023-10-19 00:41:33 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:41:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:41:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 2 @ 29 updates, score 4.862) (writing took 1.2937463660200592 seconds)\n",
      "2023-10-19 00:41:34 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-10-19 00:41:34 | INFO | train | epoch 002 | loss 4.974 | ntokens 58992 | nsentences 8 | wps 97329.1 | ups 1.65 | wpb 58992 | bsz 8 | num_updates 29 | lr 5.80942e-05 | gnorm 38.737 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 33\n",
      "2023-10-19 00:41:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:41:35 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 3\n",
      "2023-10-19 00:41:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 003:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:41:35 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-10-19 00:41:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  3.75it/s]2023-10-19 00:41:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:41:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:41:44 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 4\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:41:45 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.862 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 49 | best_loss 4.862\n",
      "2023-10-19 00:41:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 49 updates\n",
      "2023-10-19 00:41:45 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:41:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:41:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 3 @ 49 updates, score 4.862) (writing took 1.2739522199844941 seconds)\n",
      "2023-10-19 00:41:47 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-10-19 00:41:47 | INFO | train | epoch 003 | loss 4.862 | ntokens 58992 | nsentences 8 | wps 96837.8 | ups 1.64 | wpb 58992 | bsz 8 | num_updates 49 | lr 9.80902e-05 | gnorm 14.765 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 45\n",
      "2023-10-19 00:41:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:41:47 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 4\n",
      "2023-10-19 00:41:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 004:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:41:47 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-10-19 00:41:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.24it/s]2023-10-19 00:41:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:41:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:41:56 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 5\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:41:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.889 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 69 | best_loss 4.862\n",
      "2023-10-19 00:41:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 69 updates\n",
      "2023-10-19 00:41:57 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:41:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:41:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 4 @ 69 updates, score 4.889) (writing took 0.7400383340136614 seconds)\n",
      "2023-10-19 00:41:58 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-10-19 00:41:58 | INFO | train | epoch 004 | loss 4.878 | ntokens 58992 | nsentences 8 | wps 101444 | ups 1.72 | wpb 58992 | bsz 8 | num_updates 69 | lr 0.000138086 | gnorm 17.735 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 56\n",
      "2023-10-19 00:41:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:41:59 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 5\n",
      "2023-10-19 00:41:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 005:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:41:59 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-10-19 00:41:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.51it/s]2023-10-19 00:42:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:42:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:42:07 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 6\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:42:09 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.044 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 89 | best_loss 4.862\n",
      "2023-10-19 00:42:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 89 updates\n",
      "2023-10-19 00:42:09 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:42:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:42:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 5 @ 89 updates, score 5.044) (writing took 0.7752471310086548 seconds)\n",
      "2023-10-19 00:42:10 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2023-10-19 00:42:10 | INFO | train | epoch 005 | loss 5.144 | ntokens 58992 | nsentences 8 | wps 102068 | ups 1.73 | wpb 58992 | bsz 8 | num_updates 89 | lr 0.000178082 | gnorm 33.483 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 68\n",
      "2023-10-19 00:42:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:42:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 6\n",
      "2023-10-19 00:42:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 006:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:42:10 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-10-19 00:42:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:  95%|▉| 19/20 [00:07<00:00,  5.38it/s, loss=5.221, ntokens=58992, nse2023-10-19 00:42:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:42:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:42:19 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 7\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:42:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.845 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 109 | best_loss 4.845\n",
      "2023-10-19 00:42:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 109 updates\n",
      "2023-10-19 00:42:21 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:42:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:42:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 6 @ 109 updates, score 4.845) (writing took 1.2790222209878266 seconds)\n",
      "2023-10-19 00:42:22 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-10-19 00:42:22 | INFO | train | epoch 006 | loss 4.89 | ntokens 58992 | nsentences 8 | wps 97670.8 | ups 1.66 | wpb 58992 | bsz 8 | num_updates 109 | lr 0.000218078 | gnorm 10.253 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 80\n",
      "2023-10-19 00:42:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:42:23 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 7\n",
      "2023-10-19 00:42:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 007:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:42:23 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-10-19 00:42:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.26it/s]2023-10-19 00:42:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:42:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:42:31 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 8\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:42:33 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 129 | best_loss 4.835\n",
      "2023-10-19 00:42:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 129 updates\n",
      "2023-10-19 00:42:33 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:42:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:42:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 7 @ 129 updates, score 4.835) (writing took 1.2595659809885547 seconds)\n",
      "2023-10-19 00:42:34 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2023-10-19 00:42:34 | INFO | train | epoch 007 | loss 4.839 | ntokens 58992 | nsentences 8 | wps 95583.2 | ups 1.62 | wpb 58992 | bsz 8 | num_updates 129 | lr 0.000258074 | gnorm 2.821 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 92\n",
      "2023-10-19 00:42:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:42:35 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 8\n",
      "2023-10-19 00:42:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 008:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:42:35 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2023-10-19 00:42:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.81it/s]2023-10-19 00:42:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:42:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:42:44 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 9\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:42:45 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.836 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 149 | best_loss 4.835\n",
      "2023-10-19 00:42:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 149 updates\n",
      "2023-10-19 00:42:45 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:42:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:42:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 8 @ 149 updates, score 4.836) (writing took 0.7769376549986191 seconds)\n",
      "2023-10-19 00:42:46 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2023-10-19 00:42:46 | INFO | train | epoch 008 | loss 4.835 | ntokens 58992 | nsentences 8 | wps 98716.1 | ups 1.67 | wpb 58992 | bsz 8 | num_updates 149 | lr 0.00029807 | gnorm 1.176 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 104\n",
      "2023-10-19 00:42:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:42:47 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 9\n",
      "2023-10-19 00:42:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 009:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:42:47 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2023-10-19 00:42:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.36it/s]2023-10-19 00:42:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:42:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:42:55 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 10\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:42:57 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.84 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 169 | best_loss 4.835\n",
      "2023-10-19 00:42:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 169 updates\n",
      "2023-10-19 00:42:57 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:42:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:42:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 9 @ 169 updates, score 4.84) (writing took 0.787830663000932 seconds)\n",
      "2023-10-19 00:42:58 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2023-10-19 00:42:58 | INFO | train | epoch 009 | loss 4.84 | ntokens 58992 | nsentences 8 | wps 101482 | ups 1.72 | wpb 58992 | bsz 8 | num_updates 169 | lr 0.000338066 | gnorm 2.966 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 116\n",
      "2023-10-19 00:42:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:42:58 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 10\n",
      "2023-10-19 00:42:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 010:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:42:58 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2023-10-19 00:42:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.90it/s]2023-10-19 00:43:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:43:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:43:07 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 11\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:43:09 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.847 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 189 | best_loss 4.835\n",
      "2023-10-19 00:43:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 189 updates\n",
      "2023-10-19 00:43:09 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:43:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:43:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 10 @ 189 updates, score 4.847) (writing took 0.7893719389976468 seconds)\n",
      "2023-10-19 00:43:10 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2023-10-19 00:43:10 | INFO | train | epoch 010 | loss 4.843 | ntokens 58992 | nsentences 8 | wps 98157.5 | ups 1.66 | wpb 58992 | bsz 8 | num_updates 189 | lr 0.000378062 | gnorm 3.333 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 128\n",
      "2023-10-19 00:43:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:43:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 11\n",
      "2023-10-19 00:43:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 011:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:43:10 | INFO | fairseq.trainer | begin training epoch 11\n",
      "2023-10-19 00:43:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 011:  95%|▉| 19/20 [00:07<00:00,  4.78it/s, loss=4.841, ntokens=58992, nse2023-10-19 00:43:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:43:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:43:19 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 12\n",
      "\n",
      "epoch 011 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:43:21 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.855 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 209 | best_loss 4.835\n",
      "2023-10-19 00:43:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 209 updates\n",
      "2023-10-19 00:43:21 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:43:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:43:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 11 @ 209 updates, score 4.855) (writing took 0.7723479800042696 seconds)\n",
      "2023-10-19 00:43:21 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2023-10-19 00:43:21 | INFO | train | epoch 011 | loss 4.844 | ntokens 58992 | nsentences 8 | wps 102314 | ups 1.73 | wpb 58992 | bsz 8 | num_updates 209 | lr 0.000418058 | gnorm 4.03 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 139\n",
      "2023-10-19 00:43:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:43:22 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 12\n",
      "2023-10-19 00:43:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 012:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:43:22 | INFO | fairseq.trainer | begin training epoch 12\n",
      "2023-10-19 00:43:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 012:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.35it/s]2023-10-19 00:43:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:43:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:43:30 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 13\n",
      "\n",
      "epoch 012 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:43:32 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.733 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 229 | best_loss 4.835\n",
      "2023-10-19 00:43:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 229 updates\n",
      "2023-10-19 00:43:32 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:43:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:43:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 12 @ 229 updates, score 5.733) (writing took 0.7878077420173213 seconds)\n",
      "2023-10-19 00:43:33 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2023-10-19 00:43:33 | INFO | train | epoch 012 | loss 5 | ntokens 58992 | nsentences 8 | wps 101635 | ups 1.72 | wpb 58992 | bsz 8 | num_updates 229 | lr 0.000458054 | gnorm 13.906 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 151\n",
      "2023-10-19 00:43:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:43:34 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 13\n",
      "2023-10-19 00:43:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 013:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:43:34 | INFO | fairseq.trainer | begin training epoch 13\n",
      "2023-10-19 00:43:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 013:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.71it/s]2023-10-19 00:43:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:43:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:43:42 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 14\n",
      "\n",
      "epoch 013 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:43:44 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.959 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 249 | best_loss 4.835\n",
      "2023-10-19 00:43:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 249 updates\n",
      "2023-10-19 00:43:44 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:43:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:43:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 13 @ 249 updates, score 5.959) (writing took 0.772562656988157 seconds)\n",
      "2023-10-19 00:43:45 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2023-10-19 00:43:45 | INFO | train | epoch 013 | loss 7.399 | ntokens 58992 | nsentences 8 | wps 101789 | ups 1.73 | wpb 58992 | bsz 8 | num_updates 249 | lr 0.00049805 | gnorm 36.278 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 163\n",
      "2023-10-19 00:43:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:43:45 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 14\n",
      "2023-10-19 00:43:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 014:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:43:45 | INFO | fairseq.trainer | begin training epoch 14\n",
      "2023-10-19 00:43:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 014:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.25it/s]2023-10-19 00:43:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:43:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:43:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 15\n",
      "\n",
      "epoch 014 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:43:55 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.898 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 269 | best_loss 4.835\n",
      "2023-10-19 00:43:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 269 updates\n",
      "2023-10-19 00:43:55 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:43:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:43:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 14 @ 269 updates, score 4.898) (writing took 0.7841014489822555 seconds)\n",
      "2023-10-19 00:43:56 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2023-10-19 00:43:56 | INFO | train | epoch 014 | loss 5.302 | ntokens 58992 | nsentences 8 | wps 101667 | ups 1.72 | wpb 58992 | bsz 8 | num_updates 269 | lr 0.000538046 | gnorm 8.931 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 174\n",
      "2023-10-19 00:43:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:43:57 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 15\n",
      "2023-10-19 00:43:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 015:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:43:57 | INFO | fairseq.trainer | begin training epoch 15\n",
      "2023-10-19 00:43:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 015:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.44it/s]2023-10-19 00:44:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:44:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:44:05 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 16\n",
      "\n",
      "epoch 015 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:44:07 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.838 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 289 | best_loss 4.835\n",
      "2023-10-19 00:44:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 289 updates\n",
      "2023-10-19 00:44:07 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:44:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:44:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 15 @ 289 updates, score 4.838) (writing took 0.7733337969984859 seconds)\n",
      "2023-10-19 00:44:08 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2023-10-19 00:44:08 | INFO | train | epoch 015 | loss 4.859 | ntokens 58992 | nsentences 8 | wps 101368 | ups 1.72 | wpb 58992 | bsz 8 | num_updates 289 | lr 0.000578042 | gnorm 1.719 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 186\n",
      "2023-10-19 00:44:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:44:08 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 16\n",
      "2023-10-19 00:44:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 016:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:44:08 | INFO | fairseq.trainer | begin training epoch 16\n",
      "2023-10-19 00:44:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 016:  95%|▉| 19/20 [00:07<00:00,  4.64it/s, loss=5.48, ntokens=58992, nsen2023-10-19 00:44:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:44:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:44:17 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 17\n",
      "\n",
      "epoch 016 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:44:19 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 309 | best_loss 4.835\n",
      "2023-10-19 00:44:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 309 updates\n",
      "2023-10-19 00:44:19 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:44:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:44:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 16 @ 309 updates, score 4.835) (writing took 1.2882174730184488 seconds)\n",
      "2023-10-19 00:44:20 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2023-10-19 00:44:20 | INFO | train | epoch 016 | loss 4.836 | ntokens 58992 | nsentences 8 | wps 95930.4 | ups 1.63 | wpb 58992 | bsz 8 | num_updates 309 | lr 0.000618038 | gnorm 0.489 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 198\n",
      "2023-10-19 00:44:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:44:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 17\n",
      "2023-10-19 00:44:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 017:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:44:21 | INFO | fairseq.trainer | begin training epoch 17\n",
      "2023-10-19 00:44:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 017:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  3.85it/s]2023-10-19 00:44:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:44:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:44:29 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 18\n",
      "\n",
      "epoch 017 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:44:31 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 329 | best_loss 4.835\n",
      "2023-10-19 00:44:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 329 updates\n",
      "2023-10-19 00:44:31 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:44:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:44:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 17 @ 329 updates, score 4.835) (writing took 1.244478421023814 seconds)\n",
      "2023-10-19 00:44:32 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2023-10-19 00:44:32 | INFO | train | epoch 017 | loss 4.835 | ntokens 58992 | nsentences 8 | wps 97678.2 | ups 1.66 | wpb 58992 | bsz 8 | num_updates 329 | lr 0.000658034 | gnorm 0.172 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 210\n",
      "2023-10-19 00:44:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:44:33 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 18\n",
      "2023-10-19 00:44:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 018:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:44:33 | INFO | fairseq.trainer | begin training epoch 18\n",
      "2023-10-19 00:44:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 018:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.10it/s]2023-10-19 00:44:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:44:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:44:41 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 19\n",
      "\n",
      "epoch 018 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:44:43 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 349 | best_loss 4.834\n",
      "2023-10-19 00:44:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 349 updates\n",
      "2023-10-19 00:44:43 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:44:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:44:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 18 @ 349 updates, score 4.834) (writing took 1.2825144399830606 seconds)\n",
      "2023-10-19 00:44:44 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2023-10-19 00:44:44 | INFO | train | epoch 018 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 95958.9 | ups 1.63 | wpb 58992 | bsz 8 | num_updates 349 | lr 0.00069803 | gnorm 0.058 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 223\n",
      "2023-10-19 00:44:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:44:45 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 19\n",
      "2023-10-19 00:44:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 019:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:44:45 | INFO | fairseq.trainer | begin training epoch 19\n",
      "2023-10-19 00:44:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 019:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.17it/s]2023-10-19 00:44:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:44:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:44:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 20\n",
      "\n",
      "epoch 019 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:44:55 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 369 | best_loss 4.834\n",
      "2023-10-19 00:44:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 369 updates\n",
      "2023-10-19 00:44:55 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:44:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:44:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 19 @ 369 updates, score 4.835) (writing took 0.7884917950141244 seconds)\n",
      "2023-10-19 00:44:56 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2023-10-19 00:44:56 | INFO | train | epoch 019 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 100928 | ups 1.71 | wpb 58992 | bsz 8 | num_updates 369 | lr 0.000738026 | gnorm 0.022 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 234\n",
      "2023-10-19 00:44:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:44:57 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 20\n",
      "2023-10-19 00:44:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 020:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:44:57 | INFO | fairseq.trainer | begin training epoch 20\n",
      "2023-10-19 00:44:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 020:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.36it/s]2023-10-19 00:45:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:45:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:45:05 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 21\n",
      "\n",
      "epoch 020 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:45:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 389 | best_loss 4.834\n",
      "2023-10-19 00:45:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 389 updates\n",
      "2023-10-19 00:45:07 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:45:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:45:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 20 @ 389 updates, score 4.834) (writing took 1.278127705998486 seconds)\n",
      "2023-10-19 00:45:08 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2023-10-19 00:45:08 | INFO | train | epoch 020 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 96723.8 | ups 1.64 | wpb 58992 | bsz 8 | num_updates 389 | lr 0.000778022 | gnorm 0.01 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 246\n",
      "2023-10-19 00:45:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:45:09 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 21\n",
      "2023-10-19 00:45:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 021:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:45:09 | INFO | fairseq.trainer | begin training epoch 21\n",
      "2023-10-19 00:45:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 021:  95%|▉| 19/20 [00:07<00:00,  4.72it/s, loss=4.835, ntokens=58992, nse2023-10-19 00:45:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:45:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:45:18 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 22\n",
      "\n",
      "epoch 021 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.77s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:45:19 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 409 | best_loss 4.834\n",
      "2023-10-19 00:45:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 409 updates\n",
      "2023-10-19 00:45:19 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:45:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:45:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 21 @ 409 updates, score 4.834) (writing took 1.2650701949896757 seconds)\n",
      "2023-10-19 00:45:21 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
      "2023-10-19 00:45:21 | INFO | train | epoch 021 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 96564.1 | ups 1.64 | wpb 58992 | bsz 8 | num_updates 409 | lr 0.000818018 | gnorm 0.008 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 259\n",
      "2023-10-19 00:45:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:45:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 22\n",
      "2023-10-19 00:45:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 022:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:45:21 | INFO | fairseq.trainer | begin training epoch 22\n",
      "2023-10-19 00:45:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 022:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.36it/s]2023-10-19 00:45:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:45:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:45:30 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 23\n",
      "\n",
      "epoch 022 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:45:31 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 429 | best_loss 4.834\n",
      "2023-10-19 00:45:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 429 updates\n",
      "2023-10-19 00:45:31 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:45:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:45:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 22 @ 429 updates, score 4.834) (writing took 1.2804181179963052 seconds)\n",
      "2023-10-19 00:45:33 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
      "2023-10-19 00:45:33 | INFO | train | epoch 022 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 97285.2 | ups 1.65 | wpb 58992 | bsz 8 | num_updates 429 | lr 0.000858014 | gnorm 0.01 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 271\n",
      "2023-10-19 00:45:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:45:33 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 23\n",
      "2023-10-19 00:45:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 023:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:45:33 | INFO | fairseq.trainer | begin training epoch 23\n",
      "2023-10-19 00:45:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 023:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.15it/s]2023-10-19 00:45:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:45:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:45:42 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 24\n",
      "\n",
      "epoch 023 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.77s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:45:44 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 449 | best_loss 4.834\n",
      "2023-10-19 00:45:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 449 updates\n",
      "2023-10-19 00:45:44 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:45:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:45:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 23 @ 449 updates, score 4.834) (writing took 1.254932932992233 seconds)\n",
      "2023-10-19 00:45:45 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
      "2023-10-19 00:45:45 | INFO | train | epoch 023 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 97230.3 | ups 1.65 | wpb 58992 | bsz 8 | num_updates 449 | lr 0.00089801 | gnorm 0.015 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 283\n",
      "2023-10-19 00:45:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:45:46 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 24\n",
      "2023-10-19 00:45:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 024:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:45:46 | INFO | fairseq.trainer | begin training epoch 24\n",
      "2023-10-19 00:45:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 024:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.99it/s]2023-10-19 00:45:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:45:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:45:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 25\n",
      "\n",
      "epoch 024 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:45:56 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 469 | best_loss 4.834\n",
      "2023-10-19 00:45:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 469 updates\n",
      "2023-10-19 00:45:56 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:45:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:45:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 24 @ 469 updates, score 4.834) (writing took 1.279160726990085 seconds)\n",
      "2023-10-19 00:45:57 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
      "2023-10-19 00:45:57 | INFO | train | epoch 024 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 94811.8 | ups 1.61 | wpb 58992 | bsz 8 | num_updates 469 | lr 0.000938006 | gnorm 0.02 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 295\n",
      "2023-10-19 00:45:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:45:58 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 25\n",
      "2023-10-19 00:45:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 025:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:45:58 | INFO | fairseq.trainer | begin training epoch 25\n",
      "2023-10-19 00:45:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 025:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.64it/s]2023-10-19 00:46:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:46:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:46:06 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 26\n",
      "\n",
      "epoch 025 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:46:08 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 489 | best_loss 4.834\n",
      "2023-10-19 00:46:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 489 updates\n",
      "2023-10-19 00:46:08 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:46:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:46:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 25 @ 489 updates, score 4.834) (writing took 1.2633396500023082 seconds)\n",
      "2023-10-19 00:46:09 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
      "2023-10-19 00:46:09 | INFO | train | epoch 025 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 96682.2 | ups 1.64 | wpb 58992 | bsz 8 | num_updates 489 | lr 0.000978002 | gnorm 0.01 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 308\n",
      "2023-10-19 00:46:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:46:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 26\n",
      "2023-10-19 00:46:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 026:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:46:10 | INFO | fairseq.trainer | begin training epoch 26\n",
      "2023-10-19 00:46:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 026:  95%|▉| 19/20 [00:07<00:00,  4.90it/s, loss=4.834, ntokens=58992, nse2023-10-19 00:46:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:46:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:46:19 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 27\n",
      "\n",
      "epoch 026 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.77s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:46:20 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 509 | best_loss 4.834\n",
      "2023-10-19 00:46:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 509 updates\n",
      "2023-10-19 00:46:20 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:46:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:46:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 26 @ 509 updates, score 4.834) (writing took 1.2416105790180154 seconds)\n",
      "2023-10-19 00:46:22 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
      "2023-10-19 00:46:22 | INFO | train | epoch 026 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 97866.5 | ups 1.66 | wpb 58992 | bsz 8 | num_updates 509 | lr 0.001 | gnorm 0.024 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 320\n",
      "2023-10-19 00:46:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:46:22 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 27\n",
      "2023-10-19 00:46:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 027:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:46:22 | INFO | fairseq.trainer | begin training epoch 27\n",
      "2023-10-19 00:46:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 027:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.52it/s]2023-10-19 00:46:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:46:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:46:31 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 28\n",
      "\n",
      "epoch 027 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:46:32 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 529 | best_loss 4.834\n",
      "2023-10-19 00:46:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 529 updates\n",
      "2023-10-19 00:46:32 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:46:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:46:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 27 @ 529 updates, score 4.834) (writing took 1.2511955929803662 seconds)\n",
      "2023-10-19 00:46:34 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
      "2023-10-19 00:46:34 | INFO | train | epoch 027 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 97129.3 | ups 1.65 | wpb 58992 | bsz 8 | num_updates 529 | lr 0.001 | gnorm 0.046 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 332\n",
      "2023-10-19 00:46:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:46:34 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 28\n",
      "2023-10-19 00:46:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 028:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:46:34 | INFO | fairseq.trainer | begin training epoch 28\n",
      "2023-10-19 00:46:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 028:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.33it/s]2023-10-19 00:46:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:46:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:46:43 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 29\n",
      "\n",
      "epoch 028 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:46:45 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 549 | best_loss 4.834\n",
      "2023-10-19 00:46:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 549 updates\n",
      "2023-10-19 00:46:45 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:46:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:46:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 28 @ 549 updates, score 4.835) (writing took 0.7843568510143086 seconds)\n",
      "2023-10-19 00:46:45 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
      "2023-10-19 00:46:45 | INFO | train | epoch 028 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 100989 | ups 1.71 | wpb 58992 | bsz 8 | num_updates 549 | lr 0.001 | gnorm 0.031 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 344\n",
      "2023-10-19 00:46:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:46:46 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 29\n",
      "2023-10-19 00:46:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 029:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:46:46 | INFO | fairseq.trainer | begin training epoch 29\n",
      "2023-10-19 00:46:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 029:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.73it/s]2023-10-19 00:46:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:46:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:46:55 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 30\n",
      "\n",
      "epoch 029 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:46:56 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 569 | best_loss 4.834\n",
      "2023-10-19 00:46:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 569 updates\n",
      "2023-10-19 00:46:56 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:46:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:46:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 29 @ 569 updates, score 4.835) (writing took 0.79381617801846 seconds)\n",
      "2023-10-19 00:46:57 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
      "2023-10-19 00:46:57 | INFO | train | epoch 029 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 100744 | ups 1.71 | wpb 58992 | bsz 8 | num_updates 569 | lr 0.001 | gnorm 0.034 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 355\n",
      "2023-10-19 00:46:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:46:58 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 30\n",
      "2023-10-19 00:46:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 030:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:46:58 | INFO | fairseq.trainer | begin training epoch 30\n",
      "2023-10-19 00:46:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 030:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  3.66it/s]2023-10-19 00:47:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:47:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:47:06 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 31\n",
      "\n",
      "epoch 030 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:47:08 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 589 | best_loss 4.834\n",
      "2023-10-19 00:47:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 589 updates\n",
      "2023-10-19 00:47:08 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:47:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:47:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 30 @ 589 updates, score 4.835) (writing took 0.7798325160110835 seconds)\n",
      "2023-10-19 00:47:09 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
      "2023-10-19 00:47:09 | INFO | train | epoch 030 | loss 4.836 | ntokens 58992 | nsentences 8 | wps 100467 | ups 1.7 | wpb 58992 | bsz 8 | num_updates 589 | lr 0.001 | gnorm 0.71 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 367\n",
      "2023-10-19 00:47:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:47:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 31\n",
      "2023-10-19 00:47:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 031:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:47:10 | INFO | fairseq.trainer | begin training epoch 31\n",
      "2023-10-19 00:47:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 031:  95%|▉| 19/20 [00:07<00:00,  3.03it/s, loss=4.835, ntokens=58992, nse2023-10-19 00:47:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:47:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:47:18 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 32\n",
      "\n",
      "epoch 031 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:47:20 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 609 | best_loss 4.834\n",
      "2023-10-19 00:47:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 609 updates\n",
      "2023-10-19 00:47:20 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:47:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:47:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 31 @ 609 updates, score 4.835) (writing took 0.7698537529795431 seconds)\n",
      "2023-10-19 00:47:21 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
      "2023-10-19 00:47:21 | INFO | train | epoch 031 | loss 4.835 | ntokens 58992 | nsentences 8 | wps 100320 | ups 1.7 | wpb 58992 | bsz 8 | num_updates 609 | lr 0.001 | gnorm 0.351 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 379\n",
      "2023-10-19 00:47:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:47:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 32\n",
      "2023-10-19 00:47:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 032:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:47:21 | INFO | fairseq.trainer | begin training epoch 32\n",
      "2023-10-19 00:47:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 032:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.68it/s]2023-10-19 00:47:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:47:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:47:30 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 33\n",
      "\n",
      "epoch 032 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:47:32 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 629 | best_loss 4.834\n",
      "2023-10-19 00:47:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 629 updates\n",
      "2023-10-19 00:47:32 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:47:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:47:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 32 @ 629 updates, score 4.835) (writing took 0.7814240860170685 seconds)\n",
      "2023-10-19 00:47:32 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
      "2023-10-19 00:47:32 | INFO | train | epoch 032 | loss 4.835 | ntokens 58992 | nsentences 8 | wps 100160 | ups 1.7 | wpb 58992 | bsz 8 | num_updates 629 | lr 0.001 | gnorm 0.109 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 391\n",
      "2023-10-19 00:47:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:47:33 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 33\n",
      "2023-10-19 00:47:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 033:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:47:33 | INFO | fairseq.trainer | begin training epoch 33\n",
      "2023-10-19 00:47:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 033:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.90it/s]2023-10-19 00:47:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:47:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:47:42 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 34\n",
      "\n",
      "epoch 033 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:47:44 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 649 | best_loss 4.834\n",
      "2023-10-19 00:47:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 649 updates\n",
      "2023-10-19 00:47:44 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:47:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:47:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 33 @ 649 updates, score 4.834) (writing took 1.256561749993125 seconds)\n",
      "2023-10-19 00:47:45 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
      "2023-10-19 00:47:45 | INFO | train | epoch 033 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 93798.4 | ups 1.59 | wpb 58992 | bsz 8 | num_updates 649 | lr 0.001 | gnorm 0.026 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 403\n",
      "2023-10-19 00:47:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:47:46 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 34\n",
      "2023-10-19 00:47:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 034:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:47:46 | INFO | fairseq.trainer | begin training epoch 34\n",
      "2023-10-19 00:47:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 034:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.35it/s]2023-10-19 00:47:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:47:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:47:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 35\n",
      "\n",
      "epoch 034 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:47:56 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 669 | best_loss 4.834\n",
      "2023-10-19 00:47:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 669 updates\n",
      "2023-10-19 00:47:56 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:47:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:47:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 34 @ 669 updates, score 4.835) (writing took 0.7875528629811015 seconds)\n",
      "2023-10-19 00:47:57 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
      "2023-10-19 00:47:57 | INFO | train | epoch 034 | loss 4.835 | ntokens 58992 | nsentences 8 | wps 101740 | ups 1.72 | wpb 58992 | bsz 8 | num_updates 669 | lr 0.001 | gnorm 0.172 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 415\n",
      "2023-10-19 00:47:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:47:57 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 35\n",
      "2023-10-19 00:47:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 035:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:47:57 | INFO | fairseq.trainer | begin training epoch 35\n",
      "2023-10-19 00:47:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 035:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.49it/s]2023-10-19 00:48:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:48:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:48:06 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 36\n",
      "\n",
      "epoch 035 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:48:07 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 689 | best_loss 4.834\n",
      "2023-10-19 00:48:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 689 updates\n",
      "2023-10-19 00:48:07 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:48:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:48:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 35 @ 689 updates, score 4.835) (writing took 0.785937381995609 seconds)\n",
      "2023-10-19 00:48:08 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
      "2023-10-19 00:48:08 | INFO | train | epoch 035 | loss 4.839 | ntokens 58992 | nsentences 8 | wps 101096 | ups 1.71 | wpb 58992 | bsz 8 | num_updates 689 | lr 0.000999999 | gnorm 1.145 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 426\n",
      "2023-10-19 00:48:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:48:09 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 36\n",
      "2023-10-19 00:48:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 036:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:48:09 | INFO | fairseq.trainer | begin training epoch 36\n",
      "2023-10-19 00:48:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 036:  95%|▉| 19/20 [00:07<00:00,  5.11it/s, loss=4.836, ntokens=58992, nse2023-10-19 00:48:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:48:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:48:17 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 37\n",
      "\n",
      "epoch 036 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.77s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:48:19 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 709 | best_loss 4.834\n",
      "2023-10-19 00:48:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 709 updates\n",
      "2023-10-19 00:48:19 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:48:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:48:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 36 @ 709 updates, score 4.835) (writing took 0.780802491994109 seconds)\n",
      "2023-10-19 00:48:20 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
      "2023-10-19 00:48:20 | INFO | train | epoch 036 | loss 4.835 | ntokens 58992 | nsentences 8 | wps 100192 | ups 1.7 | wpb 58992 | bsz 8 | num_updates 709 | lr 0.000999999 | gnorm 0.381 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 438\n",
      "2023-10-19 00:48:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:48:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 37\n",
      "2023-10-19 00:48:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 037:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:48:21 | INFO | fairseq.trainer | begin training epoch 37\n",
      "2023-10-19 00:48:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 037:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.46it/s]2023-10-19 00:48:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:48:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:48:29 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 38\n",
      "\n",
      "epoch 037 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:48:31 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 729 | best_loss 4.834\n",
      "2023-10-19 00:48:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 729 updates\n",
      "2023-10-19 00:48:31 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:48:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:48:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 37 @ 729 updates, score 4.835) (writing took 0.7747315219894517 seconds)\n",
      "2023-10-19 00:48:32 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
      "2023-10-19 00:48:32 | INFO | train | epoch 037 | loss 4.835 | ntokens 58992 | nsentences 8 | wps 101807 | ups 1.73 | wpb 58992 | bsz 8 | num_updates 729 | lr 0.000999999 | gnorm 0.164 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 450\n",
      "2023-10-19 00:48:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:48:32 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 38\n",
      "2023-10-19 00:48:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 038:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:48:32 | INFO | fairseq.trainer | begin training epoch 38\n",
      "2023-10-19 00:48:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 038:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.10it/s]2023-10-19 00:48:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:48:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:48:41 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 39\n",
      "\n",
      "epoch 038 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:48:43 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 749 | best_loss 4.834\n",
      "2023-10-19 00:48:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 749 updates\n",
      "2023-10-19 00:48:43 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:48:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:48:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 38 @ 749 updates, score 4.834) (writing took 1.2554621270101052 seconds)\n",
      "2023-10-19 00:48:44 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
      "2023-10-19 00:48:44 | INFO | train | epoch 038 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 96194.8 | ups 1.63 | wpb 58992 | bsz 8 | num_updates 749 | lr 0.000999999 | gnorm 0.072 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 462\n",
      "2023-10-19 00:48:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:48:45 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 39\n",
      "2023-10-19 00:48:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 039:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:48:45 | INFO | fairseq.trainer | begin training epoch 39\n",
      "2023-10-19 00:48:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 039:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.51it/s]2023-10-19 00:48:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:48:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:48:53 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 40\n",
      "\n",
      "epoch 039 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:48:55 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 769 | best_loss 4.834\n",
      "2023-10-19 00:48:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 769 updates\n",
      "2023-10-19 00:48:55 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:48:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:48:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 39 @ 769 updates, score 4.834) (writing took 1.2730090670229401 seconds)\n",
      "2023-10-19 00:48:56 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
      "2023-10-19 00:48:56 | INFO | train | epoch 039 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 97824.6 | ups 1.66 | wpb 58992 | bsz 8 | num_updates 769 | lr 0.000999999 | gnorm 0.046 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 474\n",
      "2023-10-19 00:48:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:48:57 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 40\n",
      "2023-10-19 00:48:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 040:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:48:57 | INFO | fairseq.trainer | begin training epoch 40\n",
      "2023-10-19 00:48:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 040:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.33it/s]2023-10-19 00:49:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:49:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:49:05 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 41\n",
      "\n",
      "epoch 040 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:49:07 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 789 | best_loss 4.834\n",
      "2023-10-19 00:49:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 789 updates\n",
      "2023-10-19 00:49:07 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:49:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:49:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 40 @ 789 updates, score 4.835) (writing took 0.7795056060131174 seconds)\n",
      "2023-10-19 00:49:08 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
      "2023-10-19 00:49:08 | INFO | train | epoch 040 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 101076 | ups 1.71 | wpb 58992 | bsz 8 | num_updates 789 | lr 0.000999999 | gnorm 0.027 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 486\n",
      "2023-10-19 00:49:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:49:08 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 41\n",
      "2023-10-19 00:49:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 041:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:49:08 | INFO | fairseq.trainer | begin training epoch 41\n",
      "2023-10-19 00:49:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 041:  95%|▉| 19/20 [00:07<00:00,  4.21it/s, loss=4.835, ntokens=58992, nse2023-10-19 00:49:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:49:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:49:17 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 42\n",
      "\n",
      "epoch 041 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:49:19 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 809 | best_loss 4.834\n",
      "2023-10-19 00:49:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 809 updates\n",
      "2023-10-19 00:49:19 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:49:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:49:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 41 @ 809 updates, score 4.835) (writing took 0.7820187610050198 seconds)\n",
      "2023-10-19 00:49:19 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
      "2023-10-19 00:49:19 | INFO | train | epoch 041 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 100618 | ups 1.71 | wpb 58992 | bsz 8 | num_updates 809 | lr 0.000999999 | gnorm 0.075 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 497\n",
      "2023-10-19 00:49:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:49:20 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 42\n",
      "2023-10-19 00:49:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 042:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:49:20 | INFO | fairseq.trainer | begin training epoch 42\n",
      "2023-10-19 00:49:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 042:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.04it/s]2023-10-19 00:49:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:49:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:49:28 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 43\n",
      "\n",
      "epoch 042 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:49:30 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 829 | best_loss 4.834\n",
      "2023-10-19 00:49:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 829 updates\n",
      "2023-10-19 00:49:30 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:49:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:49:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 42 @ 829 updates, score 4.835) (writing took 0.7523434050090145 seconds)\n",
      "2023-10-19 00:49:31 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
      "2023-10-19 00:49:31 | INFO | train | epoch 042 | loss 4.838 | ntokens 58992 | nsentences 8 | wps 101039 | ups 1.71 | wpb 58992 | bsz 8 | num_updates 829 | lr 0.000999998 | gnorm 0.886 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 509\n",
      "2023-10-19 00:49:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:49:32 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 43\n",
      "2023-10-19 00:49:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 043:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:49:32 | INFO | fairseq.trainer | begin training epoch 43\n",
      "2023-10-19 00:49:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 043:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.36it/s]2023-10-19 00:49:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:49:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:49:40 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 44\n",
      "\n",
      "epoch 043 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:49:42 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 849 | best_loss 4.834\n",
      "2023-10-19 00:49:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 849 updates\n",
      "2023-10-19 00:49:42 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:49:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:49:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 43 @ 849 updates, score 4.835) (writing took 0.7701231719984207 seconds)\n",
      "2023-10-19 00:49:43 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
      "2023-10-19 00:49:43 | INFO | train | epoch 043 | loss 4.835 | ntokens 58992 | nsentences 8 | wps 100181 | ups 1.7 | wpb 58992 | bsz 8 | num_updates 849 | lr 0.000999998 | gnorm 0.35 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 521\n",
      "2023-10-19 00:49:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:49:43 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 44\n",
      "2023-10-19 00:49:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 044:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:49:43 | INFO | fairseq.trainer | begin training epoch 44\n",
      "2023-10-19 00:49:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 044:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.50it/s]2023-10-19 00:49:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:49:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:49:52 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 45\n",
      "\n",
      "epoch 044 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:49:54 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 869 | best_loss 4.834\n",
      "2023-10-19 00:49:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 869 updates\n",
      "2023-10-19 00:49:54 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:49:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:49:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 44 @ 869 updates, score 4.834) (writing took 1.2457899689907208 seconds)\n",
      "2023-10-19 00:49:55 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
      "2023-10-19 00:49:55 | INFO | train | epoch 044 | loss 4.835 | ntokens 58992 | nsentences 8 | wps 97609.2 | ups 1.65 | wpb 58992 | bsz 8 | num_updates 869 | lr 0.000999998 | gnorm 0.125 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 533\n",
      "2023-10-19 00:49:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:49:56 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 45\n",
      "2023-10-19 00:49:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 045:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:49:56 | INFO | fairseq.trainer | begin training epoch 45\n",
      "2023-10-19 00:49:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 045:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.53it/s]2023-10-19 00:50:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:50:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:50:04 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 46\n",
      "\n",
      "epoch 045 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:50:06 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 889 | best_loss 4.834\n",
      "2023-10-19 00:50:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 889 updates\n",
      "2023-10-19 00:50:06 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:50:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:50:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 45 @ 889 updates, score 4.834) (writing took 1.2767884620116092 seconds)\n",
      "2023-10-19 00:50:07 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
      "2023-10-19 00:50:07 | INFO | train | epoch 045 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 97619.5 | ups 1.65 | wpb 58992 | bsz 8 | num_updates 889 | lr 0.000999998 | gnorm 0.081 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 545\n",
      "2023-10-19 00:50:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:50:08 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 46\n",
      "2023-10-19 00:50:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 046:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:50:08 | INFO | fairseq.trainer | begin training epoch 46\n",
      "2023-10-19 00:50:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 046:  95%|▉| 19/20 [00:07<00:00,  5.49it/s, loss=4.835, ntokens=58992, nse2023-10-19 00:50:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:50:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:50:16 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 47\n",
      "\n",
      "epoch 046 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:50:18 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 4.834 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 909 | best_loss 4.834\n",
      "2023-10-19 00:50:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 909 updates\n",
      "2023-10-19 00:50:18 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt\n",
      "2023-10-19 00:50:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt (epoch 46 @ 909 updates, score 4.834) (writing took 1.277691055991454 seconds)\n",
      "2023-10-19 00:50:19 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
      "2023-10-19 00:50:19 | INFO | train | epoch 046 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 97666.1 | ups 1.66 | wpb 58992 | bsz 8 | num_updates 909 | lr 0.000999997 | gnorm 0.097 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 557\n",
      "2023-10-19 00:50:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:50:20 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 47\n",
      "2023-10-19 00:50:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 047:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:50:20 | INFO | fairseq.trainer | begin training epoch 47\n",
      "2023-10-19 00:50:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 047:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  3.84it/s]2023-10-19 00:50:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:50:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:50:28 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 48\n",
      "\n",
      "epoch 047 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:50:30 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 929 | best_loss 4.834\n",
      "2023-10-19 00:50:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 929 updates\n",
      "2023-10-19 00:50:30 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:50:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:50:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 47 @ 929 updates, score 4.835) (writing took 0.7736954500142019 seconds)\n",
      "2023-10-19 00:50:31 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
      "2023-10-19 00:50:31 | INFO | train | epoch 047 | loss 4.834 | ntokens 58992 | nsentences 8 | wps 101563 | ups 1.72 | wpb 58992 | bsz 8 | num_updates 929 | lr 0.000999997 | gnorm 0.046 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 569\n",
      "2023-10-19 00:50:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:50:31 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 48\n",
      "2023-10-19 00:50:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 048:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:50:31 | INFO | fairseq.trainer | begin training epoch 48\n",
      "2023-10-19 00:50:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 048:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  5.57it/s]2023-10-19 00:50:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:50:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:50:40 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 49\n",
      "\n",
      "epoch 048 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:50:41 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 4.84 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 949 | best_loss 4.834\n",
      "2023-10-19 00:50:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 949 updates\n",
      "2023-10-19 00:50:41 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:50:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:50:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 48 @ 949 updates, score 4.84) (writing took 0.7833565610053483 seconds)\n",
      "2023-10-19 00:50:42 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
      "2023-10-19 00:50:42 | INFO | train | epoch 048 | loss 4.84 | ntokens 58992 | nsentences 8 | wps 102012 | ups 1.73 | wpb 58992 | bsz 8 | num_updates 949 | lr 0.000999997 | gnorm 1.005 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 580\n",
      "2023-10-19 00:50:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:50:43 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 49\n",
      "2023-10-19 00:50:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 049:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:50:43 | INFO | fairseq.trainer | begin training epoch 49\n",
      "2023-10-19 00:50:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 049:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.59it/s]2023-10-19 00:50:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:50:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:50:51 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 50\n",
      "\n",
      "epoch 049 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:50:53 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 969 | best_loss 4.834\n",
      "2023-10-19 00:50:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 969 updates\n",
      "2023-10-19 00:50:53 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:50:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:50:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 49 @ 969 updates, score 4.835) (writing took 0.7820272370008752 seconds)\n",
      "2023-10-19 00:50:54 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
      "2023-10-19 00:50:54 | INFO | train | epoch 049 | loss 4.836 | ntokens 58992 | nsentences 8 | wps 101314 | ups 1.72 | wpb 58992 | bsz 8 | num_updates 969 | lr 0.000999997 | gnorm 0.507 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 592\n",
      "2023-10-19 00:50:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:50:55 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 50\n",
      "2023-10-19 00:50:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 20\n",
      "epoch 050:   0%|                                         | 0/20 [00:00<?, ?it/s]2023-10-19 00:50:55 | INFO | fairseq.trainer | begin training epoch 50\n",
      "2023-10-19 00:50:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 050:  95%|██████████████████████████████▍ | 19/20 [00:07<00:00,  4.98it/s]2023-10-19 00:51:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-10-19 00:51:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:51:03 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 51\n",
      "\n",
      "epoch 050 | valid on 'valid' subset:   0%|                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset: 100%|████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      "                                                                                \u001b[A2023-10-19 00:51:05 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 4.835 | ntokens 58992 | nsentences 8 | wps 0 | wpb 58992 | bsz 8 | num_updates 989 | best_loss 4.834\n",
      "2023-10-19 00:51:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 989 updates\n",
      "2023-10-19 00:51:05 | INFO | fairseq.trainer | Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:51:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt\n",
      "2023-10-19 00:51:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_last.pt (epoch 50 @ 989 updates, score 4.835) (writing took 0.7797251140000299 seconds)\n",
      "2023-10-19 00:51:05 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
      "2023-10-19 00:51:05 | INFO | train | epoch 050 | loss 4.835 | ntokens 58992 | nsentences 8 | wps 101944 | ups 1.73 | wpb 58992 | bsz 8 | num_updates 989 | lr 0.000999996 | gnorm 0.173 | loss_scale 0.0625 | train_wall 3 | gb_free 44.8 | wall 604\n",
      "2023-10-19 00:51:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-10-19 00:51:06 | INFO | fairseq_cli.train | done training in 600.3 seconds\n"
     ]
    }
   ],
   "source": [
    "!python3 -c 'import argparse; print(argparse.__file__)'\n",
    "!python3 /home/bayuan/Documents/fall23/fairseq/train.py \\\n",
    "    /home/bayuan/Documents/fall23/ecog2vec/manifest \\\n",
    "  --save-dir /home/bayuan/Documents/fall23/ecog2vec/model \\\n",
    "  --num-workers 6 --fp16 --max-update 400000 --save-interval 1 --no-epoch-checkpoints \\\n",
    "  --arch wav2vec --task audio_pretraining --min-lr 1e-06 --stop-min-lr 1e-09 --optimizer adam --lr 0.001 --lr-scheduler cosine \\\n",
    "  --conv-feature-layers \"[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]\" \\\n",
    "  --conv-aggregator-layers \"[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]\" \\\n",
    "  --skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion wav2vec --num-negatives 10 \\\n",
    "  --max-sample-size 1500000 --skip-invalid-size-inputs-valid-test --max-epoch 50  --batch-size 10 --max-tokens 150000000 --tensorboard-logdir /home/bayuan/Documents/fall23/ecog2vec/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model, and extract the $c$ vectors from each .nwb file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 20:54:16 | INFO | fairseq.models.wav2vec.wav2vec | Wav2VecModel(\n",
      "  (feature_extractor): ConvFeatureExtractionModel(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(8,), stride=(4,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (2-4): 3 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (5-6): 2 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_aggregator): ConvAggegator(\n",
      "    (conv_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): ReplicationPad1d((1, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(2,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ReplicationPad1d((2, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ReplicationPad1d((3, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(4,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ReplicationPad1d((4, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ReplicationPad1d((5, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(6,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): ReplicationPad1d((6, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): ReplicationPad1d((7, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(8,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): ReplicationPad1d((8, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (8): Sequential(\n",
      "        (0): ReplicationPad1d((9, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(10,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (9): Sequential(\n",
      "        (0): ReplicationPad1d((10, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (10): Sequential(\n",
      "        (0): ReplicationPad1d((11, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(12,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (11): Sequential(\n",
      "        (0): ReplicationPad1d((12, 0))\n",
      "        (1): Conv1d(512, 512, kernel_size=(13,), stride=(1,))\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (residual_proj): ModuleList(\n",
      "      (0-11): 12 x None\n",
      "    )\n",
      "  )\n",
      "  (wav2vec_predictions): Wav2VecPredictionsModel(\n",
      "    (project_to_steps): ConvTranspose2d(512, 512, kernel_size=(1, 12), stride=(1, 1))\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (dropout_feats): Dropout(p=0.0, inplace=False)\n",
      "  (dropout_agg): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import fairseq\n",
    "# from scipy.io import wavfile\n",
    "import soundfile as sf\n",
    "\n",
    "cp_path = '/home/bayuan/Documents/fall23/ecog2vec/model/checkpoint_best.pt'#'/path/to/wav2vec.pt'\n",
    "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
    "model = model[0]\n",
    "model.eval()\n",
    "\n",
    "wav_path = '/home/bayuan/Documents/fall23/ecog2vec/ecog/EFC400/EFC400_B4_1.wav'\n",
    "\n",
    "wav_input_16khz, sr = sf.read(wav_path)\n",
    "wav_input_16khz = wav_input_16khz.T\n",
    "wav_input_16khz = wav_input_16khz.reshape(1, 256, -1)\n",
    "\n",
    "wav_input_16khz = torch.from_numpy(wav_input_16khz).to(torch.float)\n",
    "# print(wav_input_16khz)\n",
    "\n",
    "# print(sr, wav_input_16khz.shape)\n",
    "z = model.feature_extractor(wav_input_16khz)\n",
    "c = model.feature_aggregator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8734, 0.8734, 0.8734,  ..., 0.8673, 0.8721, 0.8783],\n",
      "        [1.1981, 1.1981, 1.1981,  ..., 1.1979, 1.1879, 1.1902],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.6073, 0.6073, 0.6073,  ..., 0.6121, 0.6120, 0.6096],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1701, 0.1701, 0.1701,  ..., 0.1761, 0.1710, 0.1702]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 512, 1873])\n"
     ]
    }
   ],
   "source": [
    "print(c[0][:])\n",
    "print(c.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
