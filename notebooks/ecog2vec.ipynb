{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generating data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecog2vec.data_generator import NeuralDataGenerator\n",
    "\n",
    "# efc400 = NeuralDataGenerator('/NWB/EFC400', 'EFC400')\n",
    "# efc400.bad_electrodes = [1, 2, 33, 50, 54, 64, 128, 129, 193, 194, 256]\n",
    "# efc400.bad_electrodes = [x - 1 for x in efc400.bad_electrodes]\n",
    "# efc400.good_electrodes = [x for x in efc400.good_electrodes if x not in efc400.bad_electrodes]\n",
    "# efc400.electrode_name = 'R256GridElectrode electrodes'\n",
    "\n",
    "# efc401 = NeuralDataGenerator('/NWB/EFC401', 'EFC401')\n",
    "# efc401.bad_electrodes = [1,2,63,64,65,127,143,193,194,195,196,235,239,243,252,254,255,256]\n",
    "# efc401.bad_electrodes = [x - 1 for x in efc401.bad_electrodes]\n",
    "# efc401.good_electrodes = [x for x in efc401.good_electrodes if x not in efc401.bad_electrodes]\n",
    "# efc401.electrode_name = # Not needed, default value set to 401's electrode name (L256GridElectrode electrodes)\n",
    "\n",
    "# efc402 = NeuralDataGenerator('/NWB/EFC402', 'EFC402')\n",
    "# efc402.bad_electrodes = [129, 130, 131, 132, 133, 134, 135, 136, 137, 138, \n",
    "#                          139, 140, 141, 142, 143, 144, 145, 146, 147, 148, \n",
    "#                          149, 150, 151, 152, 153, 154, 155, 156, 157, 158, \n",
    "#                          159, 160, 161, 162, 163, 164, 165, 166, 167, 168, \n",
    "#                          169, 170, 171, 172, 173, 174, 175, 176, 177, 178, \n",
    "#                          179, 180, 181, 182, 183, 184, 185, 186, 187, 188, \n",
    "#                          189, 190, 191, 192, 193, 194, 195, 196, 197, 198, \n",
    "#                          199, 200, 201, 202, 203, 204, 205, 206, 207, 208, \n",
    "#                          209, 210, 211, 212, 213, 214, 215, 216, 217, 218, \n",
    "#                          219, 220, 221, 222, 223, 224, 225, 226, 227, 228, \n",
    "#                          229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
    "#                          239, 240, 241, 242, 243, 244, 245, 246, 247, 248, \n",
    "#                          249, 250, 251, 252, 253, 254, 255, 256]\n",
    "# efc402.bad_electrodes = [x - 1 for x in efc402.bad_electrodes]\n",
    "# efc402.good_electrodes = [x for x in efc402.good_electrodes if x not in efc402.bad_electrodes]\n",
    "# efc402.electrode_name = 'InferiorGrid electrodes'\n",
    "# efc402._bipolar_to_elec_map = efc402.bipolar_to_elec_map()\n",
    "# efc402._good_channels = efc402.good_channels()\n",
    "\n",
    "efc403 = NeuralDataGenerator('/NWB/EFC403', 'EFC403')\n",
    "efc403.bad_electrodes = [129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
    "                         139, 140, 141, 142, 143, 144, 145, 146, 147, 148,\n",
    "                         149, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
    "                         170, 171, 172, 173, 174, 175, 176, 177, 178, 179,\n",
    "                         180, 181]\n",
    "efc403.bad_electrodes = [x - 1 for x in efc403.bad_electrodes]\n",
    "efc403.good_electrodes = [x for x in efc403.good_electrodes if x not in efc403.bad_electrodes]\n",
    "efc403.electrode_name = 'Grid electrodes'\n",
    "# efc403._bipolar_to_elec_map = efc403.bipolar_to_elec_map()\n",
    "# efc403._good_channels = efc403.good_channels()\n",
    "\n",
    "print(len(efc403.good_electrodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Save clipped data for training,\n",
    "# save unclipped data to extract features from,\n",
    "# save entire recording as chunks for training,\n",
    "# save entire reocording to extract features from\n",
    "parent_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs'\n",
    "\n",
    "chopped_sentence_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/chopped_sentence'\n",
    "sentence_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/sentence'\n",
    "chopped_recording_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/chopped_recording'\n",
    "full_recording_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/full_recording'\n",
    "\n",
    "for directory in [parent_dir, chopped_sentence_dir, sentence_dir, chopped_recording_dir, full_recording_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"Directory created or already exists: {directory}\")\n",
    "\n",
    "chunk_length = 200000\n",
    "\n",
    "block_list = []\n",
    "\n",
    "# 'write_raw_data', but really it's filtered to the high\n",
    "# gamma range and the analytic amplitude is calculated\n",
    "efc403.write_raw_data(\n",
    "    # chopped_sentence_dir=chopped_sentence_dir, # don't need this for now\n",
    "    # sentence_dir=sentence_dir, # don't need this for now\n",
    "    full_recording_dir=full_recording_dir, # saves the entire recording, so features can be extracted\n",
    "    chopped_recording_dir=chopped_recording_dir, # splits recording into chunks for training\n",
    "    chunk_length=chunk_length,\n",
    "    BPR=False)  # chunk length... 200k should be fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train and extract `wav2vec` features\n",
    "\n",
    "Split into test/valid and train.\n",
    "\n",
    "Strides should downsample by ~30x; `wav2vec` takes 16kHz inputs and our raw data is at 3kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From facebookresearch/fairseq\n",
    "# Usage: python3 wav2vec_manifest.py W2V_TRAIN_DATA_PATH W2V_MANIFEST_PATH\n",
    "\n",
    "!python3 /home/bayuan/Documents/fall23/fairseq/examples/wav2vec/wav2vec_manifest.py \\\n",
    "    /home/bayuan/Documents/fall23/ecog2vec/elu_models/403/wav2vec_inputs_bpr_mocha/chopped_recording \\\n",
    "  --dest /home/bayuan/Documents/fall23/ecog2vec/elu_models/403/manifests_bpr/manifest_o4 \\\n",
    "  --ext wav \\\n",
    "  --valid-percent 0.1 \\\n",
    "  --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage from facebookresearch/fairseq\n",
    "\n",
    "!python3 -c 'import argparse; print(argparse.__file__)'\n",
    "!python3 /home/bayuan/Documents/fall23/fairseq/train.py    \\\n",
    "  /home/bayuan/Documents/fall23/ecog2vec/elu_models/400/manifest  \\\n",
    "  --save-dir /home/bayuan/Documents/fall23/ecog2vec/elu_tl_models/403/model_400_401_402  \\\n",
    "  --num-workers 6 --fp16 --max-update 400000 --save-interval 1 --infonce --no-epoch-checkpoints   \\\n",
    "  --arch wav2vec --task audio_pretraining --min-lr 1e-06 --stop-min-lr 1e-09 --optimizer adam \\\n",
    "  --lr 0.0001 --lr-scheduler cosine   \\\n",
    "  --conv-feature-layers \"[(512, 8, 5), (512, 4, 3), (512, 2, 2)]\"   \\\n",
    "  --conv-aggregator-layers \"[(512, 2, 1), (512, 3, 1), (512, 4, 1)]\"   \\\n",
    "  --skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 \\\n",
    "  --num-negatives 10   --max-sample-size 1500000 --skip-invalid-size-inputs-valid-test  --batch-size 10 \\\n",
    "  --max-tokens 1500000   --prediction-steps 12 \\\n",
    "  --criterion wav2vec --activation elu --patience 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract $c$ embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract entire recording features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Borrowed from facebookresearch/fairseq with minor modifications\n",
    "\n",
    "import torch\n",
    "import fairseq\n",
    "# from scipy.io import wavfile\n",
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from pynwb import NWBHDF5IO\n",
    "import pandas as pd\n",
    "\n",
    "file_list = []\n",
    "entire_recording_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_outputs/full_recording' \n",
    "saved_latent_sentences_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_outputs/latent_sentence'\n",
    "patient = 'EFC402' # CHANGE PER SUBJECT\n",
    "nwb_file_dir = f'/NWB/{patient}'\n",
    "nwb_file_prefix = f'{patient}_B'\n",
    "nwb_file_suffix = f'.wav.pt'\n",
    "\n",
    "cp_path = '/home/bayuan/Documents/fall23/ecog2vec/elu_tl_models/tl_401_402/checkpoint_best.pt' # CHANGE PER MODEL\n",
    "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
    "model = model[0]\n",
    "model.eval()\n",
    "\n",
    "for directory in [entire_recording_dir, saved_latent_sentences_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"Directory created or already exists: {directory}\")\n",
    "\n",
    "dir_path = f'/home/bayuan/Documents/fall23/ecog2vec/elu_models/402/wav2vec_inputs_bpr/full_recording' # CHANGE PER SUBJECT\n",
    "\n",
    "for file in os.listdir(dir_path):\n",
    "\n",
    "  wav_path = str(os.path.join(dir_path, file))\n",
    "\n",
    "  wav_input_16khz, sr = sf.read(wav_path)\n",
    "  \n",
    "  \n",
    "  wav_input_16khz = wav_input_16khz.T\n",
    "  wav_input_16khz = wav_input_16khz.reshape(1, 128, -1) # CHANGE TO NUMBER OF ELECTRODES\n",
    "\n",
    "  wav_input_16khz = torch.from_numpy(wav_input_16khz).to(torch.float)\n",
    "  # print(wav_input_16khz.shape)\n",
    "\n",
    "  # print(sr, wav_input_16khz.shape)\n",
    "  z = model.feature_extractor(wav_input_16khz)\n",
    "  c = model.feature_aggregator(z)\n",
    "  \n",
    "  torch.save(c, f\"/home/bayuan/Documents/fall23/ecog2vec/wav2vec_outputs/full_recording/{file}.pt\")\n",
    "\n",
    "# Split the recording's context vector into its \n",
    "# individual sentences.\n",
    "\n",
    "for file in os.listdir(entire_recording_dir):\n",
    "  c_filepath = os.path.join(entire_recording_dir, file)\n",
    "  \n",
    "  c = torch.load(c_filepath).detach().numpy()[0]\n",
    "  nwb_file = file.rstrip(nwb_file_suffix)\n",
    "  \n",
    "  nwb_path = os.path.join(nwb_file_dir, nwb_file)\n",
    "  io = NWBHDF5IO(nwb_path, load_namespaces=True, mode='r')\n",
    "  nwbfile = io.read()\n",
    "  \n",
    "  sr = 101.7\n",
    "  \n",
    "  starts = list(nwbfile.trials[:]['start_time'] * sr)\n",
    "  stops = list(nwbfile.trials[:]['stop_time'] * sr)\n",
    "  starts = [int(start) for start in starts]\n",
    "  stops = [int(stop) for stop in stops]\n",
    "  \n",
    "  i = 0\n",
    "  for start, stop in zip(starts, stops):\n",
    "    speaking_segment = c[:,start:stop]\n",
    "    torch.save(speaking_segment, f'{saved_latent_sentences_dir}/{nwb_file}_{i}.wav.pt')\n",
    "    i = i + 1\n",
    "    \n",
    "# file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We don't want periodicity here\n",
    "\n",
    "c = torch.load('/home/bayuan/Documents/fall23/ecog2vec/wav2vec_outputs/latent_sentence/EFC401_B41.nwb_0.wav.pt')\n",
    "cc = c# .detach().numpy()[0]\n",
    "\n",
    "print(cc.shape)\n",
    "# print(cc[6])\n",
    "\n",
    "fig, ax = plt.subplots(3,3)\n",
    "\n",
    "# print(len(cc[0]))\n",
    "ax[0,0].plot(cc[0])\n",
    "ax[0,1].plot(cc[1])\n",
    "ax[0,2].plot(cc[2])\n",
    "ax[1,0].plot(cc[3])\n",
    "ax[1,1].plot(cc[4])\n",
    "ax[1,2].plot(cc[5])\n",
    "ax[2,0].plot(cc[6])\n",
    "ax[2,1].plot(cc[7])\n",
    "ax[2,2].plot(cc[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write to tf_records for `ecog2txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Taken from `jgm/ecog2txt`. Minor modifications.\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from machine_learning.neural_networks import tf_helpers as tfh\n",
    "import pdb\n",
    "\n",
    "from pynwb import NWBHDF5IO\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "# Creating a list of dictionaries of:\n",
    "# \n",
    "# `ecog_sequence`: ECoG data, clipped to token(-sequence) length\n",
    "# `text_sequence`: the corresponding text token(-sequence)\n",
    "# `audio_sequence`: the corresponding audio (MFCC) token sequence (gonna set to)\n",
    "# `phoneme_sequence`: ditto for phonemes--with repeats\n",
    "#\n",
    "# Then saving them as tf_records\n",
    "\n",
    "\n",
    "# Only need to change the following 3 lines\n",
    "tfrecord_dir = '/home/bayuan/Documents/fall23/ecog2vec/elu_tl_models/tl_401_402/tfrecords_402_tl_on_401' # CHANGE TFRECORDS PATH HERE\n",
    "patient = 'EFC402'    # CHANGE PER SUBJECT      \n",
    "blocks = [4, 5, 6, 7, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 26, 27, 33, 34, 35, 44, 45, 46, 47, 48, 49, 58, 59, 60]\n",
    "\n",
    "\n",
    "# [4, 7, 10, 13, 19, 20, 21, 28, 35, 39, 52, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 70, 73, 74, 75, 76, 77, 83, 92, 93, 94, 95, 97, 98, 99, 100, 101, 108, 109, 110, 111, 112, 113, 114, 115]\n",
    "\n",
    "# [3, 4, 6, 7, 9, 10, 12, 13, 15, 17, \n",
    "#     18, 19, 20, 21, 22, 27, 28, 30, 33,\n",
    "#     35, 38, 39, 40, 42, 44, 46, 48, 50,\n",
    "#     52, 53, 54, 55, 56, 59, 60, 61, 62,\n",
    "#     63, 64, 65, 70, 73, 74, 75, 76, 77,\n",
    "#     83, 92, 93, 94, 95, 97, 98, 99, 100,\n",
    "#     101, 108, 109, 110, 111, 112, 113, \n",
    "#     114, 115]\n",
    "\n",
    "# [4, 5, 6, 7, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 26, 27, 33, 34, 35, 44, 45, 46, 47, 48, 49, 58, 59, 60]\n",
    "\n",
    "# [3, 23, 72]\n",
    "\n",
    "# [4, 41, 57, 61, 66, 69, 73, 77, 83, 87]\n",
    "\n",
    "def transcription_to_array(trial_t0, trial_tF, onset_times, offset_times, transcription, max_length, sampling_rate):\n",
    "    \n",
    "    # if the transcription is missing (e.g. for covert trials)\n",
    "    if transcription is None:\n",
    "        return np.full(max_length, 'pau', dtype='<U5')\n",
    "\n",
    "    # get just the parts of transcript relevant to this trial\n",
    "    trial_inds = (onset_times >= trial_t0) * (offset_times < trial_tF)\n",
    "    transcript = np.array(transcription.description.split(' '))[trial_inds]\n",
    "    onset_times = np.array(onset_times[trial_inds])\n",
    "    offset_times = np.array(offset_times[trial_inds])\n",
    "\n",
    "    # vectorized indexing\n",
    "    sample_times = trial_t0 + np.arange(max_length)/sampling_rate\n",
    "    indices = (\n",
    "        (sample_times[None, :] >= onset_times[:, None]) *\n",
    "        (sample_times[None, :] < offset_times[:, None])\n",
    "    )\n",
    "\n",
    "    # no more than one phoneme should be on at once...\n",
    "    try:\n",
    "        # print('exactly one phoneme:', np.all(np.sum(indices, 0) == 1))\n",
    "        assert np.all(np.sum(indices, 0) < 2)\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "\n",
    "    # ...but there can be locations with *zero* phonemes; assume 'pau' here\n",
    "    transcript = np.insert(transcript, 0, 'pau')\n",
    "    indices = np.sum(indices*(np.arange(1, len(transcript))[:, None]), 0)\n",
    "\n",
    "    return transcript[indices]\n",
    "\n",
    "def sentence_tokenize(token_list): # token_type = word_sequence\n",
    "    tokenized_sentence = [\n",
    "                (token.lower() + '_').encode('utf-8') for token in token_list\n",
    "            ]\n",
    "    return tokenized_sentence\n",
    "\n",
    "def write_to_Protobuf(path, example_dicts):\n",
    "    '''\n",
    "    Collect the relevant ECoG data and then write to disk as a (google)\n",
    "        protocol buffer.\n",
    "    '''\n",
    "    writer = tf.io.TFRecordWriter(\n",
    "        path)\n",
    "    for example_dict in example_dicts:\n",
    "        feature_example = tfh.make_feature_example(example_dict)\n",
    "        writer.write(feature_example.SerializeToString())\n",
    "            \n",
    "# sorting function for latent representation filenames\n",
    "def custom_sort_key(filename):\n",
    "    num_part = int(filename.split('nwb_')[1].split('.wav.pt')[0])\n",
    "    return num_part\n",
    "\n",
    "all_example_dict = []\n",
    "\n",
    "\n",
    "for block in blocks:\n",
    "    tfrecord_path = f'{tfrecord_dir}/{patient}_B{block}.tfrecord'\n",
    "\n",
    "    nwb_filepath = folder_path = f\"/NWB/{patient}/{patient}_B{block}.nwb\"\n",
    "    io = NWBHDF5IO(nwb_filepath, load_namespaces=True, mode='r')\n",
    "    nwbfile = io.read()\n",
    "    \n",
    "    ### GETTING LATENT REPRESENTATION PATHS ###\n",
    "    c_vectors_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_outputs/latent_sentence'\n",
    "    prefix = f'{patient}_B{block}.nwb_' \n",
    "\n",
    "    c_file_path_list = []\n",
    "\n",
    "    for filename in os.listdir(c_vectors_dir):\n",
    "        if filename.startswith(prefix):\n",
    "            c_file_path_list.append(os.path.join(c_vectors_dir, filename))\n",
    "            \n",
    "    c_file_path_list = sorted(c_file_path_list, key=custom_sort_key)\n",
    "    print(c_file_path_list)\n",
    "    ###########################################\n",
    "    phoneme_transcriptions = nwbfile.processing['behavior'].data_interfaces['BehavioralEpochs'].interval_series #['phoneme transcription'].timestamps[:]\n",
    "\n",
    "    token_type = 'word_sequence'\n",
    "\n",
    "    max_seconds_dict = {\n",
    "        'phoneme': 0.2,\n",
    "        'word': 1.0,\n",
    "        'word_sequence': 6.25,\n",
    "        'word_piece_sequence': 6.25,\n",
    "        'phoneme_sequence': 6.25,\n",
    "        'trial': 6.25\n",
    "    }\n",
    "\n",
    "    if 'phoneme transcription' in phoneme_transcriptions:\n",
    "        print(f'Phoneme transcription for block {block} exists.')\n",
    "        phoneme_transcript = phoneme_transcriptions['phoneme transcription']\n",
    "        phoneme_onset_times = phoneme_transcript.timestamps[\n",
    "            phoneme_transcript.data[()] == 1]\n",
    "        phoneme_offset_times = phoneme_transcript.timestamps[\n",
    "            phoneme_transcript.data[()] == -1]\n",
    "    else:\n",
    "        phoneme_transcript = None\n",
    "        phoneme_onset_times = None\n",
    "        phoneme_offset_times = None\n",
    "\n",
    "    example_dicts = []\n",
    "\n",
    "    makin_sr = 200 # screw it, actually just pass in c_sr everywhere\n",
    "    c_sr = 101.7 # 399# 18\n",
    "\n",
    "    for index, trial in enumerate(nwbfile.trials):\n",
    "        \n",
    "        # ECOG (C) SEQUENCE\n",
    "        c_filepath = c_file_path_list[index]\n",
    "        c = torch.load(c_filepath) #.detach().numpy()[0] # [1, nchannel, samples]\n",
    "        # c = c.reshape(512,-1) # [nchannel, samples]\n",
    "        c = c.T # [samples, nchannel]\n",
    "    \n",
    "        print(c.shape)\n",
    "        nsamples = c.shape[0]\n",
    "        \n",
    "        # TEXT SEQUENCE\n",
    "        speech_string = trial['transcription'].values[0]\n",
    "        text_sequence = sentence_tokenize(speech_string.split(' ')) # , 'text_sequence')\n",
    "        \n",
    "        # AUDIO SEQUENCE    \n",
    "        audio_sequence = []\n",
    "        \n",
    "        # PHONEME SEQUENCE\n",
    "        t0 = float(trial.iloc[0].start_time)\n",
    "        tF = float(trial.iloc[0].stop_time)\n",
    "    \n",
    "        i0 = np.rint(c_sr*t0).astype(int)\n",
    "        iF = np.rint(c_sr*tF).astype(int)\n",
    "        \n",
    "        M = iF - i0\n",
    "        \n",
    "        max_seconds = max_seconds_dict.get(token_type) # , 0.2) # i don't think this 0.2 default is necessary for the scope of this\n",
    "        max_samples = int(np.floor(c_sr*max_seconds))\n",
    "        max_length = min(M, max_samples)\n",
    "        \n",
    "        phoneme_array = transcription_to_array(\n",
    "                        t0, tF, phoneme_onset_times, phoneme_offset_times,\n",
    "                        phoneme_transcript, max_length, c_sr # makin_sr\n",
    "                    )\n",
    "        \n",
    "        phoneme_sequence = [ph.encode('utf-8') for ph in phoneme_array]\n",
    "        \n",
    "        if len(phoneme_sequence) != nsamples:\n",
    "            if len(phoneme_sequence) > nsamples:\n",
    "                phoneme_sequence = [phoneme_sequence[i] for i in range(nsamples)]\n",
    "            else:\n",
    "                for i in range(nsamples - len(phoneme_sequence)):\n",
    "                    phoneme_sequence.append(phoneme_sequence[-1])\n",
    "        \n",
    "        print('\\n------------------------')\n",
    "        print(f'For sentence {index}: ')\n",
    "        print(c[0:5,0:5])\n",
    "        print(f'Latent representation shape: {c.shape} (should be [samples, nchannel])')\n",
    "        print(text_sequence)\n",
    "        print(f'Audio sequence: {audio_sequence}')\n",
    "        print(f'Length of phoneme sequence: {len(phoneme_sequence)}')\n",
    "        print(phoneme_sequence)\n",
    "        print('------------------------\\n')\n",
    "        \n",
    "        example_dicts.append({'ecog_sequence': c, 'text_sequence': text_sequence, 'audio_sequence': [], 'phoneme_sequence': phoneme_sequence,})\n",
    "    #     break\n",
    "    # break\n",
    "    all_example_dict.extend(example_dicts)\n",
    "    print(len(example_dicts))\n",
    "    print(len(all_example_dict))\n",
    "    write_to_Protobuf(tfrecord_path, example_dicts)\n",
    "\n",
    "print(len(all_example_dict))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
