{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecog2vec.data_generator import NeuralDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "efc401 = NeuralDataGenerator('/NWB/EFC401', 'EFC401')\n",
    "efc401.bad_electrodes = [1,2,63,64,65,127,143,193,194,195,196,235,239,243,252,254,255,256]\n",
    "efc401.bad_electrodes = [x - 1 for x in efc401.bad_electrodes]\n",
    "efc401.good_electrodes = [x for x in efc401.good_electrodes if x not in efc401.bad_electrodes]\n",
    "\n",
    "# import random\n",
    "\n",
    "# efc400 = NeuralDataGenerator('/NWB/EFC400', 'EFC400')\n",
    "# efc400.bad_electrodes = [1, 2, 33, 50, 54, 64, 128, 129, 193, 194, 256]\n",
    "\n",
    "# available_numbers = [num for num in range(1, 257) if num not in efc400.bad_electrodes]\n",
    "# random_numbers = random.sample(available_numbers, 7)\n",
    "# efc400.bad_electrodes = efc400.bad_electrodes + random_numbers\n",
    "\n",
    "# efc400.bad_electrodes = [x - 1 for x in efc400.bad_electrodes]\n",
    "# efc400.good_electrodes = [x for x in efc400.good_electrodes if x not in efc400.bad_electrodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created or already exists: /home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs\n",
      "Directory created or already exists: /home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/chopped_sentence\n",
      "Directory created or already exists: /home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/sentence\n",
      "Directory created or already exists: /home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/chopped_recording\n",
      "Directory created or already exists: /home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/full_recording\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.6.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.3.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Save clipped data for training,\n",
    "# save unclipped data to extract features from,\n",
    "# save entire recording as chunks for training,\n",
    "# save entire reocording to extract features from\n",
    "parent_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs'\n",
    "\n",
    "chopped_sentence_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/chopped_sentence'\n",
    "sentence_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/sentence'\n",
    "chopped_recording_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/chopped_recording'\n",
    "full_recording_dir = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/full_recording'\n",
    "\n",
    "for directory in [parent_dir, chopped_sentence_dir, sentence_dir, chopped_recording_dir, full_recording_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"Directory created or already exists: {directory}\")\n",
    "\n",
    "chunk_length = 200000\n",
    "\n",
    "block_list = []\n",
    "\n",
    "# 'write_raw_data', but really it's filtered to the high\n",
    "# gamma range and the analytic amplitude is calculated\n",
    "efc401.write_raw_data(\n",
    "    # chopped_sentence_dir=chopped_sentence_dir,\n",
    "    # sentence_dir=sentence_dir,\n",
    "    full_recording_dir=full_recording_dir,\n",
    "    chopped_recording_dir=chopped_recording_dir,\n",
    "    chunk_length=chunk_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train and extract `wav2vec` features\n",
    "\n",
    "Split into test/valid and train.\n",
    "\n",
    "Strides should downsample by ~30x; `wav2vec` takes 16kHz inputs and our raw data is at 3kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from facebookresearch/fairseq with minor modifications\n",
    "\n",
    "!python3 /home/bayuan/Documents/fall23/fairseq/examples/wav2vec/wav2vec_manifest.py \\\n",
    "    /home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/chopped_recording \\\n",
    "  --dest /home/bayuan/Documents/fall23/ecog2vec/manifest \\\n",
    "  --ext wav \\\n",
    "  --valid-percent 0.1 \\\n",
    "  --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-19 10:43:06.186514: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-19 10:43:06.186554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-19 10:43:06.187772: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-19 10:43:06.192802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-19 10:43:06.718641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[2023-12-19 10:43:08,747][fairseq_cli.train][INFO] - Args: \n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "[2023-12-19 10:43:09,661][fairseq_cli.train][INFO] - Wav2Vec2Model(\n",
      "  (feature_extractor): ConvFeatureExtractionModel(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(238, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "        (3): GELU(approximate='none')\n",
      "      )\n",
      "      (1-4): 4 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): GELU(approximate='none')\n",
      "      )\n",
      "      (5-6): 2 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "  (dropout_input): Dropout(p=0.1, inplace=False)\n",
      "  (dropout_features): Dropout(p=0.1, inplace=False)\n",
      "  (quantizer): GumbelVectorQuantizer(\n",
      "    (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
      "  )\n",
      "  (project_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (encoder): TransformerEncoder(\n",
      "    (pos_conv): Sequential(\n",
      "      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "      (1): SamePad()\n",
      "      (2): GELU(approximate='none')\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerSentenceEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (final_proj): Linear(in_features=768, out_features=256, bias=True)\n",
      ")\n",
      "[2023-12-19 10:43:09,662][fairseq_cli.train][INFO] - task: AudioPretrainingTask\n",
      "[2023-12-19 10:43:09,662][fairseq_cli.train][INFO] - model: Wav2Vec2Model\n",
      "[2023-12-19 10:43:09,662][fairseq_cli.train][INFO] - criterion: Wav2vecCriterion\n",
      "[2023-12-19 10:43:09,663][fairseq_cli.train][INFO] - num. shared model params: 96,258,048 (num. trained: 96,258,048)\n",
      "[2023-12-19 10:43:09,663][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)\n",
      "[2023-12-19 10:43:09,664][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 10, skipped 0 samples\n",
      "[2023-12-19 10:43:09,787][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.1.0.bias\n",
      "[2023-12-19 10:43:09,787][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.2.0.bias\n",
      "[2023-12-19 10:43:09,787][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.3.0.bias\n",
      "[2023-12-19 10:43:09,787][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.4.0.bias\n",
      "[2023-12-19 10:43:09,787][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.5.0.bias\n",
      "[2023-12-19 10:43:09,787][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.6.0.bias\n",
      "[2023-12-19 10:43:09,787][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************\n",
      "[2023-12-19 10:43:09,788][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.528 GB ; name = NVIDIA RTX A6000                        \n",
      "[2023-12-19 10:43:09,788][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************\n",
      "[2023-12-19 10:43:09,788][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)\n",
      "[2023-12-19 10:43:09,788][fairseq_cli.train][INFO] - max tokens per device = 1400000 and max sentences per device = None\n",
      "[2023-12-19 10:43:09,788][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:43:09,788][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:43:09,788][fairseq.trainer][INFO] - loading train data for epoch 1\n",
      "[2023-12-19 10:43:09,789][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 67, skipped 0 samples\n",
      "[2023-12-19 10:43:09,790][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:43:09,790][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True\n",
      "[2023-12-19 10:43:09,790][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True\n",
      "[2023-12-19 10:43:09,790][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch\n",
      "[2023-12-19 10:43:09,827][fairseq.data.iterators][INFO] - First train data shape: torch.Size([200000, 238])\n",
      "[2023-12-19 10:43:09,827][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1\n",
      "[2023-12-19 10:43:10,516][fairseq_cli.train][INFO] - begin dry-run validation on \"valid\" subset\n",
      "[2023-12-19 10:43:10,516][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:43:10,517][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True\n",
      "[2023-12-19 10:43:10,517][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True\n",
      "[2023-12-19 10:43:10,517][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch\n",
      "[2023-12-19 10:43:10,544][fairseq.data.iterators][INFO] - First train data shape: torch.Size([200000, 238])\n",
      "[2023-12-19 10:43:10,544][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1\n",
      "[2023-12-19 10:43:14,884][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:43:14,887][fairseq.trainer][INFO] - begin training epoch 1\n",
      "[2023-12-19 10:43:14,888][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:43:27,521][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:43:27,522][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:43:28,113][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2\n",
      "[2023-12-19 10:43:30,459][valid][INFO] - {\"epoch\": 1, \"valid_loss\": \"6.722\", \"valid_ntokens\": \"1862\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.973\", \"valid_code_perplexity\": \"536.514\", \"valid_temp\": \"2\", \"valid_loss_0\": \"6.722\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01235\", \"valid_wps\": \"0\", \"valid_wpb\": \"1862\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"9\"}\n",
      "[2023-12-19 10:43:30,460][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 9 updates\n",
      "[2023-12-19 10:43:30,460][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:43:32,088][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:43:33,050][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 9 updates, score 6.722) (writing took 2.5908224880113266 seconds)\n",
      "[2023-12-19 10:43:33,051][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)\n",
      "[2023-12-19 10:43:33,051][train][INFO] - {\"epoch\": 1, \"train_loss\": \"6.721\", \"train_ntokens\": \"1989.56\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.974\", \"train_code_perplexity\": \"547.407\", \"train_temp\": \"2\", \"train_loss_0\": \"6.721\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.0119\", \"train_wps\": \"2137.8\", \"train_ups\": \"1.08\", \"train_wpb\": \"1989.6\", \"train_bsz\": \"7\", \"train_num_updates\": \"9\", \"train_lr\": \"1.40625e-07\", \"train_gnorm\": \"0.937\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"7\", \"train_gb_free\": \"41\", \"train_wall\": \"23\"}\n",
      "[2023-12-19 10:43:33,052][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:43:33,685][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2\n",
      "[2023-12-19 10:43:33,685][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:43:33,687][fairseq.trainer][INFO] - begin training epoch 2\n",
      "[2023-12-19 10:43:33,687][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:43:47,023][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:43:47,024][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:43:47,687][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3\n",
      "[2023-12-19 10:43:50,102][valid][INFO] - {\"epoch\": 2, \"valid_loss\": \"6.716\", \"valid_ntokens\": \"1890\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.974\", \"valid_code_perplexity\": \"538.028\", \"valid_temp\": \"2\", \"valid_loss_0\": \"6.716\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01164\", \"valid_wps\": \"0\", \"valid_wpb\": \"1890\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"18\", \"valid_best_loss\": \"6.716\"}\n",
      "[2023-12-19 10:43:50,103][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 18 updates\n",
      "[2023-12-19 10:43:50,104][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:43:52,176][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:43:53,642][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 18 updates, score 6.716) (writing took 3.539078013971448 seconds)\n",
      "[2023-12-19 10:43:53,642][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)\n",
      "[2023-12-19 10:43:53,643][train][INFO] - {\"epoch\": 2, \"train_loss\": \"6.721\", \"train_ntokens\": \"1966.22\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.974\", \"train_code_perplexity\": \"545.816\", \"train_temp\": \"2\", \"train_loss_0\": \"6.721\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01164\", \"train_wps\": \"859.4\", \"train_ups\": \"0.44\", \"train_wpb\": \"1966.2\", \"train_bsz\": \"7\", \"train_num_updates\": \"18\", \"train_lr\": \"2.8125e-07\", \"train_gnorm\": \"0.935\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"41\", \"train_wall\": \"44\"}\n",
      "[2023-12-19 10:43:53,644][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:43:54,313][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3\n",
      "[2023-12-19 10:43:54,314][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:43:54,316][fairseq.trainer][INFO] - begin training epoch 3\n",
      "[2023-12-19 10:43:54,316][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:44:01,223][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:44:01,224][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:44:01,875][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4\n",
      "[2023-12-19 10:44:04,286][valid][INFO] - {\"epoch\": 3, \"valid_loss\": \"6.707\", \"valid_ntokens\": \"2009\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.971\", \"valid_code_perplexity\": \"538.744\", \"valid_temp\": \"2\", \"valid_loss_0\": \"6.707\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01145\", \"valid_wps\": \"0\", \"valid_wpb\": \"2009\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"27\", \"valid_best_loss\": \"6.707\"}\n",
      "[2023-12-19 10:44:04,286][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 27 updates\n",
      "[2023-12-19 10:44:04,287][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:44:06,399][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:44:07,826][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 27 updates, score 6.707) (writing took 3.539682012051344 seconds)\n",
      "[2023-12-19 10:44:07,826][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)\n",
      "[2023-12-19 10:44:07,827][train][INFO] - {\"epoch\": 3, \"train_loss\": \"6.722\", \"train_ntokens\": \"1942.11\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.972\", \"train_code_perplexity\": \"545.815\", \"train_temp\": \"2\", \"train_loss_0\": \"6.722\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01081\", \"train_wps\": \"1232.3\", \"train_ups\": \"0.63\", \"train_wpb\": \"1942.1\", \"train_bsz\": \"7\", \"train_num_updates\": \"27\", \"train_lr\": \"4.21875e-07\", \"train_gnorm\": \"0.928\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"58\"}\n",
      "[2023-12-19 10:44:07,828][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:44:08,471][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4\n",
      "[2023-12-19 10:44:08,472][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:44:08,474][fairseq.trainer][INFO] - begin training epoch 4\n",
      "[2023-12-19 10:44:08,474][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:44:15,085][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:44:15,085][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:44:15,745][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5\n",
      "[2023-12-19 10:44:18,131][valid][INFO] - {\"epoch\": 4, \"valid_loss\": \"6.705\", \"valid_ntokens\": \"1967\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.961\", \"valid_code_perplexity\": \"537.464\", \"valid_temp\": \"2\", \"valid_loss_0\": \"6.705\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01068\", \"valid_wps\": \"0\", \"valid_wpb\": \"1967\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"36\", \"valid_best_loss\": \"6.705\"}\n",
      "[2023-12-19 10:44:18,132][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 36 updates\n",
      "[2023-12-19 10:44:18,132][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:44:20,287][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:44:21,701][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 36 updates, score 6.705) (writing took 3.569212550006341 seconds)\n",
      "[2023-12-19 10:44:21,701][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)\n",
      "[2023-12-19 10:44:21,701][train][INFO] - {\"epoch\": 4, \"train_loss\": \"6.712\", \"train_ntokens\": \"1960\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.967\", \"train_code_perplexity\": \"546.175\", \"train_temp\": \"2\", \"train_loss_0\": \"6.712\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01156\", \"train_wps\": \"1271.4\", \"train_ups\": \"0.65\", \"train_wpb\": \"1960\", \"train_bsz\": \"7\", \"train_num_updates\": \"36\", \"train_lr\": \"5.625e-07\", \"train_gnorm\": \"0.851\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"72\"}\n",
      "[2023-12-19 10:44:21,702][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:44:22,339][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5\n",
      "[2023-12-19 10:44:22,340][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:44:22,341][fairseq.trainer][INFO] - begin training epoch 5\n",
      "[2023-12-19 10:44:22,342][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:44:31,548][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:44:31,549][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:44:32,187][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6\n",
      "[2023-12-19 10:44:34,574][valid][INFO] - {\"epoch\": 5, \"valid_loss\": \"6.687\", \"valid_ntokens\": \"1960\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.948\", \"valid_code_perplexity\": \"527.202\", \"valid_temp\": \"2\", \"valid_loss_0\": \"6.687\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01173\", \"valid_wps\": \"0\", \"valid_wpb\": \"1960\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"45\", \"valid_best_loss\": \"6.687\"}\n",
      "[2023-12-19 10:44:34,575][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 45 updates\n",
      "[2023-12-19 10:44:34,575][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:44:36,590][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:44:38,035][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 45 updates, score 6.687) (writing took 3.459777866024524 seconds)\n",
      "[2023-12-19 10:44:38,035][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)\n",
      "[2023-12-19 10:44:38,035][train][INFO] - {\"epoch\": 5, \"train_loss\": \"6.707\", \"train_ntokens\": \"1953\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.957\", \"train_code_perplexity\": \"541.009\", \"train_temp\": \"2\", \"train_loss_0\": \"6.707\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01229\", \"train_wps\": \"1076.1\", \"train_ups\": \"0.55\", \"train_wpb\": \"1953\", \"train_bsz\": \"7\", \"train_num_updates\": \"45\", \"train_lr\": \"7.03125e-07\", \"train_gnorm\": \"0.747\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"88\"}\n",
      "[2023-12-19 10:44:38,036][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:44:38,671][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6\n",
      "[2023-12-19 10:44:38,671][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:44:38,673][fairseq.trainer][INFO] - begin training epoch 6\n",
      "[2023-12-19 10:44:38,673][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:44:45,235][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:44:45,235][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:44:45,877][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7\n",
      "[2023-12-19 10:44:48,264][valid][INFO] - {\"epoch\": 6, \"valid_loss\": \"6.683\", \"valid_ntokens\": \"1981\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.938\", \"valid_code_perplexity\": \"528.26\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.683\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01161\", \"valid_wps\": \"0\", \"valid_wpb\": \"1981\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"54\", \"valid_best_loss\": \"6.683\"}\n",
      "[2023-12-19 10:44:48,265][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 54 updates\n",
      "[2023-12-19 10:44:48,266][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:44:50,352][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:44:51,784][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 54 updates, score 6.683) (writing took 3.518621583993081 seconds)\n",
      "[2023-12-19 10:44:51,784][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)\n",
      "[2023-12-19 10:44:51,784][train][INFO] - {\"epoch\": 6, \"train_loss\": \"6.698\", \"train_ntokens\": \"1971.67\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.946\", \"train_code_perplexity\": \"540.096\", \"train_temp\": \"2\", \"train_loss_0\": \"6.698\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01167\", \"train_wps\": \"1290.7\", \"train_ups\": \"0.65\", \"train_wpb\": \"1971.7\", \"train_bsz\": \"7\", \"train_num_updates\": \"54\", \"train_lr\": \"8.4375e-07\", \"train_gnorm\": \"0.669\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"102\"}\n",
      "[2023-12-19 10:44:51,785][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:44:52,446][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7\n",
      "[2023-12-19 10:44:52,447][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:44:52,449][fairseq.trainer][INFO] - begin training epoch 7\n",
      "[2023-12-19 10:44:52,449][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:44:59,041][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:44:59,042][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:44:59,687][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8\n",
      "[2023-12-19 10:45:02,059][valid][INFO] - {\"epoch\": 7, \"valid_loss\": \"6.679\", \"valid_ntokens\": \"1820\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.928\", \"valid_code_perplexity\": \"523.136\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.679\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01319\", \"valid_wps\": \"0\", \"valid_wpb\": \"1820\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"63\", \"valid_best_loss\": \"6.679\"}\n",
      "[2023-12-19 10:45:02,060][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 63 updates\n",
      "[2023-12-19 10:45:02,060][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:45:04,145][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:45:05,564][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 63 updates, score 6.679) (writing took 3.503943244984839 seconds)\n",
      "[2023-12-19 10:45:05,564][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)\n",
      "[2023-12-19 10:45:05,564][train][INFO] - {\"epoch\": 7, \"train_loss\": \"6.692\", \"train_ntokens\": \"2043.22\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.935\", \"train_code_perplexity\": \"538.149\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.692\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01196\", \"train_wps\": \"1334.5\", \"train_ups\": \"0.65\", \"train_wpb\": \"2043.2\", \"train_bsz\": \"7\", \"train_num_updates\": \"63\", \"train_lr\": \"9.84375e-07\", \"train_gnorm\": \"0.595\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"116\"}\n",
      "[2023-12-19 10:45:05,565][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:45:06,213][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8\n",
      "[2023-12-19 10:45:06,213][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:45:06,215][fairseq.trainer][INFO] - begin training epoch 8\n",
      "[2023-12-19 10:45:06,216][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:45:12,790][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:45:12,791][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:45:13,437][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9\n",
      "[2023-12-19 10:45:15,812][valid][INFO] - {\"epoch\": 8, \"valid_loss\": \"6.678\", \"valid_ntokens\": \"2100\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.926\", \"valid_code_perplexity\": \"528.618\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.678\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01143\", \"valid_wps\": \"0\", \"valid_wpb\": \"2100\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"72\", \"valid_best_loss\": \"6.678\"}\n",
      "[2023-12-19 10:45:15,812][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 72 updates\n",
      "[2023-12-19 10:45:15,813][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:45:17,897][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:45:19,313][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 72 updates, score 6.678) (writing took 3.5010044209775515 seconds)\n",
      "[2023-12-19 10:45:19,314][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)\n",
      "[2023-12-19 10:45:19,314][train][INFO] - {\"epoch\": 8, \"train_loss\": \"6.689\", \"train_ntokens\": \"1945.22\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.928\", \"train_code_perplexity\": \"535.957\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.689\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01194\", \"train_wps\": \"1273.3\", \"train_ups\": \"0.65\", \"train_wpb\": \"1945.2\", \"train_bsz\": \"7\", \"train_num_updates\": \"72\", \"train_lr\": \"1.125e-06\", \"train_gnorm\": \"0.561\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"130\"}\n",
      "[2023-12-19 10:45:19,315][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:45:19,958][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9\n",
      "[2023-12-19 10:45:19,959][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:45:19,961][fairseq.trainer][INFO] - begin training epoch 9\n",
      "[2023-12-19 10:45:19,961][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:45:26,668][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:45:26,669][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:45:27,323][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10\n",
      "[2023-12-19 10:45:29,728][valid][INFO] - {\"epoch\": 9, \"valid_loss\": \"6.676\", \"valid_ntokens\": \"2009\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.931\", \"valid_code_perplexity\": \"522.995\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.676\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01195\", \"valid_wps\": \"0\", \"valid_wpb\": \"2009\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"81\", \"valid_best_loss\": \"6.676\"}\n",
      "[2023-12-19 10:45:29,729][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 81 updates\n",
      "[2023-12-19 10:45:29,729][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:45:31,882][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:45:33,361][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 9 @ 81 updates, score 6.676) (writing took 3.6324102979851887 seconds)\n",
      "[2023-12-19 10:45:33,361][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)\n",
      "[2023-12-19 10:45:33,362][train][INFO] - {\"epoch\": 9, \"train_loss\": \"6.684\", \"train_ntokens\": \"2001.22\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.926\", \"train_code_perplexity\": \"537.792\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.684\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.0111\", \"train_wps\": \"1282.1\", \"train_ups\": \"0.64\", \"train_wpb\": \"2001.2\", \"train_bsz\": \"7\", \"train_num_updates\": \"81\", \"train_lr\": \"1.26563e-06\", \"train_gnorm\": \"0.532\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"144\"}\n",
      "[2023-12-19 10:45:33,363][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:45:34,010][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10\n",
      "[2023-12-19 10:45:34,010][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:45:34,012][fairseq.trainer][INFO] - begin training epoch 10\n",
      "[2023-12-19 10:45:34,012][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:45:40,641][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:45:40,641][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:45:41,293][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11\n",
      "[2023-12-19 10:45:43,704][valid][INFO] - {\"epoch\": 10, \"valid_loss\": \"6.672\", \"valid_ntokens\": \"2072\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.928\", \"valid_code_perplexity\": \"528.164\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.672\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.00965\", \"valid_wps\": \"0\", \"valid_wpb\": \"2072\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"90\", \"valid_best_loss\": \"6.672\"}\n",
      "[2023-12-19 10:45:43,704][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 90 updates\n",
      "[2023-12-19 10:45:43,705][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:45:45,860][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:45:47,324][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 90 updates, score 6.672) (writing took 3.619872938026674 seconds)\n",
      "[2023-12-19 10:45:47,324][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)\n",
      "[2023-12-19 10:45:47,325][train][INFO] - {\"epoch\": 10, \"train_loss\": \"6.683\", \"train_ntokens\": \"1974.78\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.929\", \"train_code_perplexity\": \"536.689\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.683\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01086\", \"train_wps\": \"1272.9\", \"train_ups\": \"0.64\", \"train_wpb\": \"1974.8\", \"train_bsz\": \"7\", \"train_num_updates\": \"90\", \"train_lr\": \"1.40625e-06\", \"train_gnorm\": \"0.495\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"158\"}\n",
      "[2023-12-19 10:45:47,326][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:45:47,981][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11\n",
      "[2023-12-19 10:45:47,982][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:45:47,984][fairseq.trainer][INFO] - begin training epoch 11\n",
      "[2023-12-19 10:45:47,984][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:45:54,636][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:45:54,637][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:45:55,279][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12\n",
      "[2023-12-19 10:45:57,674][valid][INFO] - {\"epoch\": 11, \"valid_loss\": \"6.669\", \"valid_ntokens\": \"1897\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.931\", \"valid_code_perplexity\": \"523.204\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.669\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01054\", \"valid_wps\": \"0\", \"valid_wpb\": \"1897\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"99\", \"valid_best_loss\": \"6.669\"}\n",
      "[2023-12-19 10:45:57,675][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 99 updates\n",
      "[2023-12-19 10:45:57,675][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:45:59,651][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:46:01,134][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 99 updates, score 6.669) (writing took 3.458881372003816 seconds)\n",
      "[2023-12-19 10:46:01,134][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)\n",
      "[2023-12-19 10:46:01,134][train][INFO] - {\"epoch\": 11, \"train_loss\": \"6.677\", \"train_ntokens\": \"1961.56\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.932\", \"train_code_perplexity\": \"533.18\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.677\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.00991\", \"train_wps\": \"1278.4\", \"train_ups\": \"0.65\", \"train_wpb\": \"1961.6\", \"train_bsz\": \"7\", \"train_num_updates\": \"99\", \"train_lr\": \"1.54688e-06\", \"train_gnorm\": \"0.446\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"171\"}\n",
      "[2023-12-19 10:46:01,135][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:46:01,776][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12\n",
      "[2023-12-19 10:46:01,777][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:46:01,779][fairseq.trainer][INFO] - begin training epoch 12\n",
      "[2023-12-19 10:46:01,779][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:46:08,589][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:46:08,590][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:46:09,226][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13\n",
      "[2023-12-19 10:46:11,634][valid][INFO] - {\"epoch\": 12, \"valid_loss\": \"6.668\", \"valid_ntokens\": \"1925\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.939\", \"valid_code_perplexity\": \"525.263\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.668\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.00987\", \"valid_wps\": \"0\", \"valid_wpb\": \"1925\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"108\", \"valid_best_loss\": \"6.668\"}\n",
      "[2023-12-19 10:46:11,635][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 108 updates\n",
      "[2023-12-19 10:46:11,636][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:46:13,739][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:46:15,228][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 108 updates, score 6.668) (writing took 3.5933422750094905 seconds)\n",
      "[2023-12-19 10:46:15,229][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)\n",
      "[2023-12-19 10:46:15,229][train][INFO] - {\"epoch\": 12, \"train_loss\": \"6.679\", \"train_ntokens\": \"1974\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.936\", \"train_code_perplexity\": \"538.147\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.679\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01266\", \"train_wps\": \"1260.5\", \"train_ups\": \"0.64\", \"train_wpb\": \"1974\", \"train_bsz\": \"7\", \"train_num_updates\": \"108\", \"train_lr\": \"1.6875e-06\", \"train_gnorm\": \"0.452\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"41\", \"train_wall\": \"185\"}\n",
      "[2023-12-19 10:46:15,230][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:46:15,893][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13\n",
      "[2023-12-19 10:46:15,893][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:46:15,895][fairseq.trainer][INFO] - begin training epoch 13\n",
      "[2023-12-19 10:46:15,895][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:46:23,698][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:46:23,698][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:46:24,353][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14\n",
      "[2023-12-19 10:46:26,757][valid][INFO] - {\"epoch\": 13, \"valid_loss\": \"6.668\", \"valid_ntokens\": \"2044\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.944\", \"valid_code_perplexity\": \"533.591\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.668\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01517\", \"valid_wps\": \"0\", \"valid_wpb\": \"2044\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"117\", \"valid_best_loss\": \"6.668\"}\n",
      "[2023-12-19 10:46:26,758][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 117 updates\n",
      "[2023-12-19 10:46:26,759][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:46:28,844][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:46:30,325][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 117 updates, score 6.668) (writing took 3.566871297021862 seconds)\n",
      "[2023-12-19 10:46:30,325][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)\n",
      "[2023-12-19 10:46:30,326][train][INFO] - {\"epoch\": 13, \"train_loss\": \"6.677\", \"train_ntokens\": \"1972.44\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.939\", \"train_code_perplexity\": \"538.611\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.677\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01149\", \"train_wps\": \"1175.9\", \"train_ups\": \"0.6\", \"train_wpb\": \"1972.4\", \"train_bsz\": \"7\", \"train_num_updates\": \"117\", \"train_lr\": \"1.82813e-06\", \"train_gnorm\": \"0.432\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"3\", \"train_gb_free\": \"40.9\", \"train_wall\": \"201\"}\n",
      "[2023-12-19 10:46:30,327][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:46:30,964][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14\n",
      "[2023-12-19 10:46:30,965][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:46:30,967][fairseq.trainer][INFO] - begin training epoch 14\n",
      "[2023-12-19 10:46:30,967][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:46:42,061][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:46:42,062][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:46:42,723][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15\n",
      "[2023-12-19 10:46:45,118][valid][INFO] - {\"epoch\": 14, \"valid_loss\": \"6.666\", \"valid_ntokens\": \"2044\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.952\", \"valid_code_perplexity\": \"531.041\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.666\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.00978\", \"valid_wps\": \"0\", \"valid_wpb\": \"2044\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"126\", \"valid_best_loss\": \"6.666\"}\n",
      "[2023-12-19 10:46:45,119][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 126 updates\n",
      "[2023-12-19 10:46:45,120][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:46:47,255][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:46:48,749][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 126 updates, score 6.666) (writing took 3.630060642026365 seconds)\n",
      "[2023-12-19 10:46:48,749][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)\n",
      "[2023-12-19 10:46:48,750][train][INFO] - {\"epoch\": 14, \"train_loss\": \"6.673\", \"train_ntokens\": \"2006.67\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.948\", \"train_code_perplexity\": \"542.133\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.673\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01047\", \"train_wps\": \"980.3\", \"train_ups\": \"0.49\", \"train_wpb\": \"2006.7\", \"train_bsz\": \"7\", \"train_num_updates\": \"126\", \"train_lr\": \"1.96875e-06\", \"train_gnorm\": \"0.37\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"219\"}\n",
      "[2023-12-19 10:46:48,751][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:46:49,398][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15\n",
      "[2023-12-19 10:46:49,398][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:46:49,400][fairseq.trainer][INFO] - begin training epoch 15\n",
      "[2023-12-19 10:46:49,401][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:46:56,278][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:46:56,279][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:46:56,935][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16\n",
      "[2023-12-19 10:46:59,322][valid][INFO] - {\"epoch\": 15, \"valid_loss\": \"6.664\", \"valid_ntokens\": \"1967\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.959\", \"valid_code_perplexity\": \"528.332\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.664\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01423\", \"valid_wps\": \"0\", \"valid_wpb\": \"1967\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"135\", \"valid_best_loss\": \"6.664\"}\n",
      "[2023-12-19 10:46:59,323][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 135 updates\n",
      "[2023-12-19 10:46:59,323][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:47:01,442][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:47:02,860][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 135 updates, score 6.664) (writing took 3.5374639309593476 seconds)\n",
      "[2023-12-19 10:47:02,860][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)\n",
      "[2023-12-19 10:47:02,861][train][INFO] - {\"epoch\": 15, \"train_loss\": \"6.674\", \"train_ntokens\": \"1995.78\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.954\", \"train_code_perplexity\": \"542.749\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.674\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01119\", \"train_wps\": \"1272.9\", \"train_ups\": \"0.64\", \"train_wpb\": \"1995.8\", \"train_bsz\": \"7\", \"train_num_updates\": \"135\", \"train_lr\": \"2.10938e-06\", \"train_gnorm\": \"0.373\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"233\"}\n",
      "[2023-12-19 10:47:02,862][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:47:03,500][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16\n",
      "[2023-12-19 10:47:03,501][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:47:03,503][fairseq.trainer][INFO] - begin training epoch 16\n",
      "[2023-12-19 10:47:03,503][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:47:10,238][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:47:10,239][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:47:10,888][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17\n",
      "[2023-12-19 10:47:13,296][valid][INFO] - {\"epoch\": 16, \"valid_loss\": \"6.664\", \"valid_ntokens\": \"2114\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.961\", \"valid_code_perplexity\": \"538.004\", \"valid_temp\": \"1.999\", \"valid_loss_0\": \"6.664\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01325\", \"valid_wps\": \"0\", \"valid_wpb\": \"2114\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"144\", \"valid_best_loss\": \"6.664\"}\n",
      "[2023-12-19 10:47:13,297][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 144 updates\n",
      "[2023-12-19 10:47:13,297][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:47:15,412][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:47:16,863][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 16 @ 144 updates, score 6.664) (writing took 3.5662199549842626 seconds)\n",
      "[2023-12-19 10:47:16,863][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)\n",
      "[2023-12-19 10:47:16,863][train][INFO] - {\"epoch\": 16, \"train_loss\": \"6.673\", \"train_ntokens\": \"2010.56\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.957\", \"train_code_perplexity\": \"544.503\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.673\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01177\", \"train_wps\": \"1292.3\", \"train_ups\": \"0.64\", \"train_wpb\": \"2010.6\", \"train_bsz\": \"7\", \"train_num_updates\": \"144\", \"train_lr\": \"2.25e-06\", \"train_gnorm\": \"0.348\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"247\"}\n",
      "[2023-12-19 10:47:16,864][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:47:17,511][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17\n",
      "[2023-12-19 10:47:17,512][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:47:17,514][fairseq.trainer][INFO] - begin training epoch 17\n",
      "[2023-12-19 10:47:17,514][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:47:24,345][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:47:24,345][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:47:25,025][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18\n",
      "[2023-12-19 10:47:27,427][valid][INFO] - {\"epoch\": 17, \"valid_loss\": \"6.665\", \"valid_ntokens\": \"1911\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.963\", \"valid_code_perplexity\": \"541.473\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.665\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01204\", \"valid_wps\": \"0\", \"valid_wpb\": \"1911\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"153\", \"valid_best_loss\": \"6.664\"}\n",
      "[2023-12-19 10:47:27,428][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 153 updates\n",
      "[2023-12-19 10:47:27,429][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:47:29,537][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:47:29,590][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 17 @ 153 updates, score 6.665) (writing took 2.161822816997301 seconds)\n",
      "[2023-12-19 10:47:29,590][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)\n",
      "[2023-12-19 10:47:29,591][train][INFO] - {\"epoch\": 17, \"train_loss\": \"6.67\", \"train_ntokens\": \"2023.78\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.96\", \"train_code_perplexity\": \"546.724\", \"train_temp\": \"1.999\", \"train_loss_0\": \"6.67\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01257\", \"train_wps\": \"1431.1\", \"train_ups\": \"0.71\", \"train_wpb\": \"2023.8\", \"train_bsz\": \"7\", \"train_num_updates\": \"153\", \"train_lr\": \"2.39063e-06\", \"train_gnorm\": \"0.316\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"260\"}\n",
      "[2023-12-19 10:47:29,592][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:47:30,255][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18\n",
      "[2023-12-19 10:47:30,256][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:47:30,258][fairseq.trainer][INFO] - begin training epoch 18\n",
      "[2023-12-19 10:47:30,258][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:47:37,161][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:47:37,161][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:47:37,844][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19\n",
      "[2023-12-19 10:47:40,299][valid][INFO] - {\"epoch\": 18, \"valid_loss\": \"6.663\", \"valid_ntokens\": \"2121\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.964\", \"valid_code_perplexity\": \"541.031\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.663\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.0132\", \"valid_wps\": \"0\", \"valid_wpb\": \"2121\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"162\", \"valid_best_loss\": \"6.663\"}\n",
      "[2023-12-19 10:47:40,300][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 162 updates\n",
      "[2023-12-19 10:47:40,300][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:47:42,434][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:47:44,000][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 162 updates, score 6.663) (writing took 3.700729993986897 seconds)\n",
      "[2023-12-19 10:47:44,001][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)\n",
      "[2023-12-19 10:47:44,001][train][INFO] - {\"epoch\": 18, \"train_loss\": \"6.667\", \"train_ntokens\": \"1946.78\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.962\", \"train_code_perplexity\": \"542.321\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.667\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01307\", \"train_wps\": \"1215.9\", \"train_ups\": \"0.62\", \"train_wpb\": \"1946.8\", \"train_bsz\": \"7\", \"train_num_updates\": \"162\", \"train_lr\": \"2.53125e-06\", \"train_gnorm\": \"0.31\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.9\", \"train_wall\": \"274\"}\n",
      "[2023-12-19 10:47:44,002][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:47:44,674][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19\n",
      "[2023-12-19 10:47:44,675][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:47:44,677][fairseq.trainer][INFO] - begin training epoch 19\n",
      "[2023-12-19 10:47:44,677][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:47:51,713][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:47:51,713][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:47:52,365][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20\n",
      "[2023-12-19 10:47:54,773][valid][INFO] - {\"epoch\": 19, \"valid_loss\": \"6.664\", \"valid_ntokens\": \"1946\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.964\", \"valid_code_perplexity\": \"536.12\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.664\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01233\", \"valid_wps\": \"0\", \"valid_wpb\": \"1946\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"171\", \"valid_best_loss\": \"6.663\"}\n",
      "[2023-12-19 10:47:54,774][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 171 updates\n",
      "[2023-12-19 10:47:54,774][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:47:56,841][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:47:56,896][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 19 @ 171 updates, score 6.664) (writing took 2.122057114029303 seconds)\n",
      "[2023-12-19 10:47:56,896][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)\n",
      "[2023-12-19 10:47:56,896][train][INFO] - {\"epoch\": 19, \"train_loss\": \"6.67\", \"train_ntokens\": \"2012.11\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.963\", \"train_code_perplexity\": \"542.972\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.67\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01104\", \"train_wps\": \"1404.3\", \"train_ups\": \"0.7\", \"train_wpb\": \"2012.1\", \"train_bsz\": \"7\", \"train_num_updates\": \"171\", \"train_lr\": \"2.67188e-06\", \"train_gnorm\": \"0.3\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"287\"}\n",
      "[2023-12-19 10:47:56,897][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:47:57,568][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20\n",
      "[2023-12-19 10:47:57,568][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:47:57,570][fairseq.trainer][INFO] - begin training epoch 20\n",
      "[2023-12-19 10:47:57,570][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:48:04,596][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:48:04,596][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:48:05,264][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 21\n",
      "[2023-12-19 10:48:07,727][valid][INFO] - {\"epoch\": 20, \"valid_loss\": \"6.663\", \"valid_ntokens\": \"1988\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.965\", \"valid_code_perplexity\": \"534.897\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.663\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01157\", \"valid_wps\": \"0\", \"valid_wpb\": \"1988\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"180\", \"valid_best_loss\": \"6.663\"}\n",
      "[2023-12-19 10:48:07,727][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 180 updates\n",
      "[2023-12-19 10:48:07,728][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:48:09,895][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:48:11,393][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 20 @ 180 updates, score 6.663) (writing took 3.6652698240359314 seconds)\n",
      "[2023-12-19 10:48:11,393][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)\n",
      "[2023-12-19 10:48:11,393][train][INFO] - {\"epoch\": 20, \"train_loss\": \"6.668\", \"train_ntokens\": \"1996.56\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.963\", \"train_code_perplexity\": \"541.411\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.668\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01185\", \"train_wps\": \"1239.6\", \"train_ups\": \"0.62\", \"train_wpb\": \"1996.6\", \"train_bsz\": \"7\", \"train_num_updates\": \"180\", \"train_lr\": \"2.8125e-06\", \"train_gnorm\": \"0.295\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"41\", \"train_wall\": \"302\"}\n",
      "[2023-12-19 10:48:11,394][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:48:12,037][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 21\n",
      "[2023-12-19 10:48:12,038][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:48:12,040][fairseq.trainer][INFO] - begin training epoch 21\n",
      "[2023-12-19 10:48:12,040][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:48:18,870][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:48:18,870][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:48:19,532][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 22\n",
      "[2023-12-19 10:48:21,929][valid][INFO] - {\"epoch\": 21, \"valid_loss\": \"6.663\", \"valid_ntokens\": \"1911\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.966\", \"valid_code_perplexity\": \"535.917\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.663\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.00994\", \"valid_wps\": \"0\", \"valid_wpb\": \"1911\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"189\", \"valid_best_loss\": \"6.663\"}\n",
      "[2023-12-19 10:48:21,930][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 189 updates\n",
      "[2023-12-19 10:48:21,930][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:48:23,934][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:48:25,390][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 21 @ 189 updates, score 6.663) (writing took 3.4603174349758774 seconds)\n",
      "[2023-12-19 10:48:25,390][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)\n",
      "[2023-12-19 10:48:25,391][train][INFO] - {\"epoch\": 21, \"train_loss\": \"6.667\", \"train_ntokens\": \"1975.56\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.963\", \"train_code_perplexity\": \"545.945\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.667\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01181\", \"train_wps\": \"1270.2\", \"train_ups\": \"0.64\", \"train_wpb\": \"1975.6\", \"train_bsz\": \"7\", \"train_num_updates\": \"189\", \"train_lr\": \"2.95313e-06\", \"train_gnorm\": \"0.271\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.9\", \"train_wall\": \"316\"}\n",
      "[2023-12-19 10:48:25,392][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:48:26,040][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 22\n",
      "[2023-12-19 10:48:26,041][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:48:26,042][fairseq.trainer][INFO] - begin training epoch 22\n",
      "[2023-12-19 10:48:26,043][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:48:33,314][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:48:33,314][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:48:33,970][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 23\n",
      "[2023-12-19 10:48:36,372][valid][INFO] - {\"epoch\": 22, \"valid_loss\": \"6.661\", \"valid_ntokens\": \"2023\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.966\", \"valid_code_perplexity\": \"543.683\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.661\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.00989\", \"valid_wps\": \"0\", \"valid_wpb\": \"2023\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"198\", \"valid_best_loss\": \"6.661\"}\n",
      "[2023-12-19 10:48:36,373][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 198 updates\n",
      "[2023-12-19 10:48:36,373][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:48:38,446][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:48:39,942][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 22 @ 198 updates, score 6.661) (writing took 3.5687776380218565 seconds)\n",
      "[2023-12-19 10:48:39,942][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)\n",
      "[2023-12-19 10:48:39,942][train][INFO] - {\"epoch\": 22, \"train_loss\": \"6.668\", \"train_ntokens\": \"1987.22\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.965\", \"train_code_perplexity\": \"546.46\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.668\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01129\", \"train_wps\": \"1229.1\", \"train_ups\": \"0.62\", \"train_wpb\": \"1987.2\", \"train_bsz\": \"7\", \"train_num_updates\": \"198\", \"train_lr\": \"3.09375e-06\", \"train_gnorm\": \"0.268\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"330\"}\n",
      "[2023-12-19 10:48:39,943][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:48:40,604][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 23\n",
      "[2023-12-19 10:48:40,605][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:48:40,607][fairseq.trainer][INFO] - begin training epoch 23\n",
      "[2023-12-19 10:48:40,607][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:48:44,573][train_inner][INFO] - {\"epoch\": 23, \"update\": 22.222, \"loss\": \"6.686\", \"ntokens\": \"1981.35\", \"nsentences\": \"7\", \"prob_perplexity\": \"639.952\", \"code_perplexity\": \"541.891\", \"temp\": \"1.999\", \"loss_0\": \"6.686\", \"loss_1\": \"0\", \"loss_2\": \"0\", \"accuracy\": \"0.01161\", \"wps\": \"1236\", \"ups\": \"0.62\", \"wpb\": \"1981.3\", \"bsz\": \"7\", \"num_updates\": \"200\", \"lr\": \"3.125e-06\", \"gnorm\": \"0.517\", \"loss_scale\": \"128\", \"train_wall\": \"48\", \"gb_free\": \"40.8\", \"wall\": \"335\"}\n",
      "[2023-12-19 10:48:47,316][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:48:47,316][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:48:47,959][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 24\n",
      "[2023-12-19 10:48:50,383][valid][INFO] - {\"epoch\": 23, \"valid_loss\": \"6.662\", \"valid_ntokens\": \"1911\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.963\", \"valid_code_perplexity\": \"536.607\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.662\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01308\", \"valid_wps\": \"0\", \"valid_wpb\": \"1911\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"207\", \"valid_best_loss\": \"6.661\"}\n",
      "[2023-12-19 10:48:50,384][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 207 updates\n",
      "[2023-12-19 10:48:50,385][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:48:52,503][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:48:52,586][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 23 @ 207 updates, score 6.662) (writing took 2.202159169013612 seconds)\n",
      "[2023-12-19 10:48:52,587][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)\n",
      "[2023-12-19 10:48:52,587][train][INFO] - {\"epoch\": 23, \"train_loss\": \"6.666\", \"train_ntokens\": \"1988\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.965\", \"train_code_perplexity\": \"545.382\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.666\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01269\", \"train_wps\": \"1415\", \"train_ups\": \"0.71\", \"train_wpb\": \"1988\", \"train_bsz\": \"7\", \"train_num_updates\": \"207\", \"train_lr\": \"3.23437e-06\", \"train_gnorm\": \"0.247\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.9\", \"train_wall\": \"343\"}\n",
      "[2023-12-19 10:48:52,588][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:48:53,239][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 24\n",
      "[2023-12-19 10:48:53,240][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:48:53,242][fairseq.trainer][INFO] - begin training epoch 24\n",
      "[2023-12-19 10:48:53,242][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:48:59,975][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:48:59,976][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:49:00,640][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 25\n",
      "[2023-12-19 10:49:03,119][valid][INFO] - {\"epoch\": 24, \"valid_loss\": \"6.662\", \"valid_ntokens\": \"2009\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.965\", \"valid_code_perplexity\": \"535.597\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.662\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01045\", \"valid_wps\": \"0\", \"valid_wpb\": \"2009\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"216\", \"valid_best_loss\": \"6.661\"}\n",
      "[2023-12-19 10:49:03,119][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 216 updates\n",
      "[2023-12-19 10:49:03,120][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:49:05,257][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:49:05,333][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 24 @ 216 updates, score 6.662) (writing took 2.2135922799934633 seconds)\n",
      "[2023-12-19 10:49:05,333][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)\n",
      "[2023-12-19 10:49:05,334][train][INFO] - {\"epoch\": 24, \"train_loss\": \"6.667\", \"train_ntokens\": \"2012.11\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.964\", \"train_code_perplexity\": \"542.125\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.667\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.0106\", \"train_wps\": \"1420.7\", \"train_ups\": \"0.71\", \"train_wpb\": \"2012.1\", \"train_bsz\": \"7\", \"train_num_updates\": \"216\", \"train_lr\": \"3.375e-06\", \"train_gnorm\": \"0.25\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"356\"}\n",
      "[2023-12-19 10:49:05,335][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:49:05,981][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 25\n",
      "[2023-12-19 10:49:05,982][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:49:05,984][fairseq.trainer][INFO] - begin training epoch 25\n",
      "[2023-12-19 10:49:05,984][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:49:12,730][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:49:12,730][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:49:13,387][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 26\n",
      "[2023-12-19 10:49:15,829][valid][INFO] - {\"epoch\": 25, \"valid_loss\": \"6.661\", \"valid_ntokens\": \"2037\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.965\", \"valid_code_perplexity\": \"536.835\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.661\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.00933\", \"valid_wps\": \"0\", \"valid_wpb\": \"2037\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"225\", \"valid_best_loss\": \"6.661\"}\n",
      "[2023-12-19 10:49:15,830][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 225 updates\n",
      "[2023-12-19 10:49:15,830][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:49:17,983][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:49:19,506][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 25 @ 225 updates, score 6.661) (writing took 3.6766902859671973 seconds)\n",
      "[2023-12-19 10:49:19,507][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)\n",
      "[2023-12-19 10:49:19,507][train][INFO] - {\"epoch\": 25, \"train_loss\": \"6.665\", \"train_ntokens\": \"2002.78\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.964\", \"train_code_perplexity\": \"544.591\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.665\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01121\", \"train_wps\": \"1271.8\", \"train_ups\": \"0.64\", \"train_wpb\": \"2002.8\", \"train_bsz\": \"7\", \"train_num_updates\": \"225\", \"train_lr\": \"3.51563e-06\", \"train_gnorm\": \"0.23\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"370\"}\n",
      "[2023-12-19 10:49:19,508][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:49:20,162][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 26\n",
      "[2023-12-19 10:49:20,163][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:49:20,165][fairseq.trainer][INFO] - begin training epoch 26\n",
      "[2023-12-19 10:49:20,165][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:49:27,309][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:49:27,310][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:49:27,970][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 27\n",
      "[2023-12-19 10:49:30,362][valid][INFO] - {\"epoch\": 26, \"valid_loss\": \"6.661\", \"valid_ntokens\": \"2037\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.965\", \"valid_code_perplexity\": \"537.399\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.661\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01325\", \"valid_wps\": \"0\", \"valid_wpb\": \"2037\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"234\", \"valid_best_loss\": \"6.661\"}\n",
      "[2023-12-19 10:49:30,363][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 234 updates\n",
      "[2023-12-19 10:49:30,363][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:49:32,432][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:49:33,857][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 26 @ 234 updates, score 6.661) (writing took 3.4944482009741478 seconds)\n",
      "[2023-12-19 10:49:33,857][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)\n",
      "[2023-12-19 10:49:33,858][train][INFO] - {\"epoch\": 26, \"train_loss\": \"6.665\", \"train_ntokens\": \"2012.11\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.966\", \"train_code_perplexity\": \"548.501\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.665\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01231\", \"train_wps\": \"1261.9\", \"train_ups\": \"0.63\", \"train_wpb\": \"2012.1\", \"train_bsz\": \"7\", \"train_num_updates\": \"234\", \"train_lr\": \"3.65625e-06\", \"train_gnorm\": \"0.224\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"384\"}\n",
      "[2023-12-19 10:49:33,858][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:49:34,503][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 27\n",
      "[2023-12-19 10:49:34,503][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:49:34,505][fairseq.trainer][INFO] - begin training epoch 27\n",
      "[2023-12-19 10:49:34,505][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:49:41,660][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:49:41,660][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:49:42,318][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 28\n",
      "[2023-12-19 10:49:44,728][valid][INFO] - {\"epoch\": 27, \"valid_loss\": \"6.66\", \"valid_ntokens\": \"1981\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.967\", \"valid_code_perplexity\": \"534.278\", \"valid_temp\": \"1.998\", \"valid_loss_0\": \"6.66\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01363\", \"valid_wps\": \"0\", \"valid_wpb\": \"1981\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"243\", \"valid_best_loss\": \"6.66\"}\n",
      "[2023-12-19 10:49:44,729][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 243 updates\n",
      "[2023-12-19 10:49:44,729][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:49:46,861][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:49:48,375][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 27 @ 243 updates, score 6.66) (writing took 3.646028217044659 seconds)\n",
      "[2023-12-19 10:49:48,375][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)\n",
      "[2023-12-19 10:49:48,375][train][INFO] - {\"epoch\": 27, \"train_loss\": \"6.663\", \"train_ntokens\": \"1993.44\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.967\", \"train_code_perplexity\": \"547.933\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.663\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01198\", \"train_wps\": \"1235.8\", \"train_ups\": \"0.62\", \"train_wpb\": \"1993.4\", \"train_bsz\": \"7\", \"train_num_updates\": \"243\", \"train_lr\": \"3.79687e-06\", \"train_gnorm\": \"0.21\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"399\"}\n",
      "[2023-12-19 10:49:48,376][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:49:49,018][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 28\n",
      "[2023-12-19 10:49:49,019][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:49:49,021][fairseq.trainer][INFO] - begin training epoch 28\n",
      "[2023-12-19 10:49:49,021][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:49:56,442][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:49:56,442][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:49:57,106][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 29\n",
      "[2023-12-19 10:49:59,542][valid][INFO] - {\"epoch\": 28, \"valid_loss\": \"6.66\", \"valid_ntokens\": \"1981\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.968\", \"valid_code_perplexity\": \"540.369\", \"valid_temp\": \"1.997\", \"valid_loss_0\": \"6.66\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01262\", \"valid_wps\": \"0\", \"valid_wpb\": \"1981\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"252\", \"valid_best_loss\": \"6.66\"}\n",
      "[2023-12-19 10:49:59,543][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 252 updates\n",
      "[2023-12-19 10:49:59,543][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:50:01,655][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:50:03,174][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 28 @ 252 updates, score 6.66) (writing took 3.631268751982134 seconds)\n",
      "[2023-12-19 10:50:03,174][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)\n",
      "[2023-12-19 10:50:03,175][train][INFO] - {\"epoch\": 28, \"train_loss\": \"6.664\", \"train_ntokens\": \"2002\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.967\", \"train_code_perplexity\": \"547.044\", \"train_temp\": \"1.998\", \"train_loss_0\": \"6.664\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01043\", \"train_wps\": \"1217.5\", \"train_ups\": \"0.61\", \"train_wpb\": \"2002\", \"train_bsz\": \"7\", \"train_num_updates\": \"252\", \"train_lr\": \"3.9375e-06\", \"train_gnorm\": \"0.202\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"413\"}\n",
      "[2023-12-19 10:50:03,176][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:50:03,817][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 29\n",
      "[2023-12-19 10:50:03,818][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:50:03,820][fairseq.trainer][INFO] - begin training epoch 29\n",
      "[2023-12-19 10:50:03,820][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:50:10,564][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:50:10,565][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:50:11,230][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 30\n",
      "[2023-12-19 10:50:13,652][valid][INFO] - {\"epoch\": 29, \"valid_loss\": \"6.66\", \"valid_ntokens\": \"2051\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.969\", \"valid_code_perplexity\": \"541.541\", \"valid_temp\": \"1.997\", \"valid_loss_0\": \"6.66\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.0117\", \"valid_wps\": \"0\", \"valid_wpb\": \"2051\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"261\", \"valid_best_loss\": \"6.66\"}\n",
      "[2023-12-19 10:50:13,653][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 261 updates\n",
      "[2023-12-19 10:50:13,654][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:50:15,799][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:50:17,298][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 29 @ 261 updates, score 6.66) (writing took 3.6447434300207533 seconds)\n",
      "[2023-12-19 10:50:17,298][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)\n",
      "[2023-12-19 10:50:17,298][train][INFO] - {\"epoch\": 29, \"train_loss\": \"6.664\", \"train_ntokens\": \"1981\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.967\", \"train_code_perplexity\": \"546.791\", \"train_temp\": \"1.997\", \"train_loss_0\": \"6.664\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01155\", \"train_wps\": \"1262.4\", \"train_ups\": \"0.64\", \"train_wpb\": \"1981\", \"train_bsz\": \"7\", \"train_num_updates\": \"261\", \"train_lr\": \"4.07813e-06\", \"train_gnorm\": \"0.197\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.8\", \"train_wall\": \"428\"}\n",
      "[2023-12-19 10:50:17,299][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:50:17,942][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 30\n",
      "[2023-12-19 10:50:17,943][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:50:17,945][fairseq.trainer][INFO] - begin training epoch 30\n",
      "[2023-12-19 10:50:17,945][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:50:24,698][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:50:24,698][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:50:25,355][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 31\n",
      "[2023-12-19 10:50:27,779][valid][INFO] - {\"epoch\": 30, \"valid_loss\": \"6.66\", \"valid_ntokens\": \"1925\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.966\", \"valid_code_perplexity\": \"536.802\", \"valid_temp\": \"1.997\", \"valid_loss_0\": \"6.66\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01039\", \"valid_wps\": \"0\", \"valid_wpb\": \"1925\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"270\", \"valid_best_loss\": \"6.66\"}\n",
      "[2023-12-19 10:50:27,779][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 270 updates\n",
      "[2023-12-19 10:50:27,780][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:50:29,919][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:50:31,433][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 30 @ 270 updates, score 6.66) (writing took 3.654103215027135 seconds)\n",
      "[2023-12-19 10:50:31,434][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)\n",
      "[2023-12-19 10:50:31,434][train][INFO] - {\"epoch\": 30, \"train_loss\": \"6.664\", \"train_ntokens\": \"2005.11\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.968\", \"train_code_perplexity\": \"546.932\", \"train_temp\": \"1.997\", \"train_loss_0\": \"6.664\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.0133\", \"train_wps\": \"1276.7\", \"train_ups\": \"0.64\", \"train_wpb\": \"2005.1\", \"train_bsz\": \"7\", \"train_num_updates\": \"270\", \"train_lr\": \"4.21875e-06\", \"train_gnorm\": \"0.202\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"442\"}\n",
      "[2023-12-19 10:50:31,435][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:50:32,085][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 31\n",
      "[2023-12-19 10:50:32,086][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:50:32,088][fairseq.trainer][INFO] - begin training epoch 31\n",
      "[2023-12-19 10:50:32,088][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:50:38,805][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:50:38,805][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:50:39,466][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 32\n",
      "[2023-12-19 10:50:41,895][valid][INFO] - {\"epoch\": 31, \"valid_loss\": \"6.66\", \"valid_ntokens\": \"2009\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.968\", \"valid_code_perplexity\": \"542.458\", \"valid_temp\": \"1.997\", \"valid_loss_0\": \"6.66\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.00946\", \"valid_wps\": \"0\", \"valid_wpb\": \"2009\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"279\", \"valid_best_loss\": \"6.66\"}\n",
      "[2023-12-19 10:50:41,895][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 279 updates\n",
      "[2023-12-19 10:50:41,896][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:50:44,034][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:50:45,536][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 31 @ 279 updates, score 6.66) (writing took 3.640405712008942 seconds)\n",
      "[2023-12-19 10:50:45,536][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)\n",
      "[2023-12-19 10:50:45,536][train][INFO] - {\"epoch\": 31, \"train_loss\": \"6.664\", \"train_ntokens\": \"1975.56\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.966\", \"train_code_perplexity\": \"546.033\", \"train_temp\": \"1.997\", \"train_loss_0\": \"6.664\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01142\", \"train_wps\": \"1260.8\", \"train_ups\": \"0.64\", \"train_wpb\": \"1975.6\", \"train_bsz\": \"7\", \"train_num_updates\": \"279\", \"train_lr\": \"4.35938e-06\", \"train_gnorm\": \"0.185\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"456\"}\n",
      "[2023-12-19 10:50:45,537][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:50:46,186][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 32\n",
      "[2023-12-19 10:50:46,186][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:50:46,188][fairseq.trainer][INFO] - begin training epoch 32\n",
      "[2023-12-19 10:50:46,188][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:50:52,799][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:50:52,799][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:50:53,452][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 33\n",
      "[2023-12-19 10:50:55,860][valid][INFO] - {\"epoch\": 32, \"valid_loss\": \"6.659\", \"valid_ntokens\": \"2163\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.971\", \"valid_code_perplexity\": \"539.766\", \"valid_temp\": \"1.997\", \"valid_loss_0\": \"6.659\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01156\", \"valid_wps\": \"0\", \"valid_wpb\": \"2163\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"288\", \"valid_best_loss\": \"6.659\"}\n",
      "[2023-12-19 10:50:55,861][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 288 updates\n",
      "[2023-12-19 10:50:55,861][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:50:57,963][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_best.pt\n",
      "[2023-12-19 10:50:59,393][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 32 @ 288 updates, score 6.659) (writing took 3.5317232629749924 seconds)\n",
      "[2023-12-19 10:50:59,393][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)\n",
      "[2023-12-19 10:50:59,393][train][INFO] - {\"epoch\": 32, \"train_loss\": \"6.66\", \"train_ntokens\": \"2001.22\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.968\", \"train_code_perplexity\": \"544.725\", \"train_temp\": \"1.997\", \"train_loss_0\": \"6.66\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01249\", \"train_wps\": \"1299.8\", \"train_ups\": \"0.65\", \"train_wpb\": \"2001.2\", \"train_bsz\": \"7\", \"train_num_updates\": \"288\", \"train_lr\": \"4.5e-06\", \"train_gnorm\": \"0.167\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"40.7\", \"train_wall\": \"470\"}\n",
      "[2023-12-19 10:50:59,394][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:51:00,048][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 33\n",
      "[2023-12-19 10:51:00,049][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:51:00,051][fairseq.trainer][INFO] - begin training epoch 33\n",
      "[2023-12-19 10:51:00,051][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "[2023-12-19 10:51:06,955][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\n",
      "[2023-12-19 10:51:06,955][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:51:07,611][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 34\n",
      "[2023-12-19 10:51:10,023][valid][INFO] - {\"epoch\": 33, \"valid_loss\": \"6.66\", \"valid_ntokens\": \"1925\", \"valid_nsentences\": \"7\", \"valid_prob_perplexity\": \"639.968\", \"valid_code_perplexity\": \"535.559\", \"valid_temp\": \"1.997\", \"valid_loss_0\": \"6.66\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.01039\", \"valid_wps\": \"0\", \"valid_wpb\": \"1925\", \"valid_bsz\": \"7\", \"valid_num_updates\": \"297\", \"valid_best_loss\": \"6.659\"}\n",
      "[2023-12-19 10:51:10,024][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 297 updates\n",
      "[2023-12-19 10:51:10,024][fairseq.trainer][INFO] - Saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:51:12,142][fairseq.trainer][INFO] - Finished saving checkpoint to /home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-19/10-43-08/checkpoints/checkpoint_last.pt\n",
      "[2023-12-19 10:51:12,227][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 33 @ 297 updates, score 6.66) (writing took 2.2031498010037467 seconds)\n",
      "[2023-12-19 10:51:12,227][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)\n",
      "[2023-12-19 10:51:12,228][train][INFO] - {\"epoch\": 33, \"train_loss\": \"6.661\", \"train_ntokens\": \"1969.33\", \"train_nsentences\": \"7\", \"train_prob_perplexity\": \"639.969\", \"train_code_perplexity\": \"548.424\", \"train_temp\": \"1.997\", \"train_loss_0\": \"6.661\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.01241\", \"train_wps\": \"1381\", \"train_ups\": \"0.7\", \"train_wpb\": \"1969.3\", \"train_bsz\": \"7\", \"train_num_updates\": \"297\", \"train_lr\": \"4.64063e-06\", \"train_gnorm\": \"0.175\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"2\", \"train_gb_free\": \"41\", \"train_wall\": \"482\"}\n",
      "[2023-12-19 10:51:12,229][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True\n",
      "[2023-12-19 10:51:12,886][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 34\n",
      "[2023-12-19 10:51:12,886][fairseq.data.iterators][INFO] - grouped total_num_itrs = 9\n",
      "[2023-12-19 10:51:12,888][fairseq.trainer][INFO] - begin training epoch 34\n",
      "[2023-12-19 10:51:12,888][fairseq_cli.train][INFO] - Start iterating over samples\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bayuan/anaconda3/envs/ecog2txt/bin/fairseq-hydra-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq_cli/hydra_train.py\", line 87, in cli_main\n",
      "    hydra_main()\n",
      "  File \"/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hydra/main.py\", line 32, in decorated_main\n",
      "    _run_hydra(\n",
      "  File \"/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 346, in _run_hydra\n",
      "    run_and_report(\n",
      "  File \"/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 198, in run_and_report\n",
      "    return func()\n",
      "  File \"/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 347, in <lambda>\n",
      "    lambda: hydra.run(\n",
      "  File \"/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hydra/_internal/hydra.py\", line 107, in run\n",
      "    return run_job(\n",
      "  File \"/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/hydra/core/utils.py\", line 129, in run_job\n",
      "    ret.return_value = task_function(task_cfg)\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq_cli/hydra_train.py\", line 27, in hydra_main\n",
      "    _hydra_main(cfg)\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq_cli/hydra_train.py\", line 56, in _hydra_main\n",
      "    distributed_utils.call_main(cfg, pre_main, **kwargs)\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq/distributed/utils.py\", line 404, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq_cli/train.py\", line 205, in main\n",
      "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
      "  File \"/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq_cli/train.py\", line 327, in train\n",
      "    for i, samples in enumerate(progress):\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq/logging/progress_bar.py\", line 202, in __iter__\n",
      "    for i, obj in enumerate(self.iterable, start=self.n):\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq/data/iterators.py\", line 57, in __next__\n",
      "    x = next(self._itr)\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq/data/iterators.py\", line 615, in _chunk_iterator\n",
      "    for x in itr:\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq/data/iterators.py\", line 57, in __next__\n",
      "    x = next(self._itr)\n",
      "  File \"/home/bayuan/Documents/fall23/fairseq/fairseq/data/iterators.py\", line 746, in __next__\n",
      "    item = self._queue.get(True)\n",
      "  File \"/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/queue.py\", line 171, in get\n",
      "    self.not_empty.wait()\n",
      "  File \"/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/threading.py\", line 312, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!fairseq-hydra-train \\\n",
    "    task.data=/home/bayuan/Documents/fall23/ecog2vec/manifest \\\n",
    "    --config-dir /home/bayuan/Documents/fall23/fairseq/examples/wav2vec/config/pretraining \\\n",
    "    --config-name wav2vec2_base_librispeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract $c$ embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract entire recording, then chop up into sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 09:26:57.079053: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-24 09:26:57.079088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-24 09:26:57.080273: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-24 09:26:57.085988: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-24 09:26:57.633080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/bayuan/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Wav2Vec2Model' object has no attribute 'feature_aggregator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(wav_input_16khz.shape)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# print(sr, wav_input_16khz.shape)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfeature_extractor(wav_input_16khz)\n\u001b[0;32m---> 32\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_aggregator\u001b[49m(z)\n\u001b[1;32m     34\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(c, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/bayuan/Documents/fall23/ecog2vec/wav2vec_outputs/latent_sentence/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ecog2txt/lib/python3.9/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Wav2Vec2Model' object has no attribute 'feature_aggregator'"
     ]
    }
   ],
   "source": [
    "# Borrowed from facebookresearch/fairseq with minor modifications\n",
    "\n",
    "import torch\n",
    "import fairseq\n",
    "# from scipy.io import wavfile\n",
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "cp_path = '/home/bayuan/Documents/fall23/ecog2vec/notebooks/outputs/2023-12-20/15-32-57/checkpoints/checkpoint_best.pt'#'/path/to/wav2vec.pt'\n",
    "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
    "model = model[0]\n",
    "model.eval()\n",
    "\n",
    "dir_path = '/home/bayuan/Documents/fall23/ecog2vec/wav2vec_inputs/full_recording'\n",
    "\n",
    "for file in os.listdir(dir_path):\n",
    "\n",
    "    wav_path = str(os.path.join(dir_path, file))\n",
    "\n",
    "    wav_input_16khz, sr = sf.read(wav_path)\n",
    "    print(type(wav_input_16khz))\n",
    "    # \n",
    "    wav_input_16khz = wav_input_16khz.T\n",
    "    wav_input_16khz = wav_input_16khz.reshape(1, 238, -1)#wav_input_16khz[np.newaxis, :, :] # change to 256; 480\n",
    "\n",
    "    wav_input_16khz = torch.from_numpy(wav_input_16khz).to(torch.float)\n",
    "    # print(wav_input_16khz.shape)\n",
    "\n",
    "    # print(sr, wav_input_16khz.shape)\n",
    "    z = model.feature_extractor(wav_input_16khz)\n",
    "    c = model.feature_aggregator(z)\n",
    "    \n",
    "    torch.save(c, f\"/home/bayuan/Documents/fall23/ecog2vec/wav2vec_outputs/latent_sentence/{file}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog2txt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
