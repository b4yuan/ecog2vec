{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec\n",
    "\n",
    "This was an introductory exercise to verify `wav2vec`'s functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fairseq\n",
    "\n",
    "cp_path = '/home/bayuan/Documents/ecog2txt_infonce/wav2vec_large.pt'#'/path/to/wav2vec.pt'\n",
    "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
    "model = model[0]\n",
    "model.eval()\n",
    "\n",
    "wav_input_16khz = torch.randn(1,10000)\n",
    "z = model.feature_extractor(wav_input_16khz)\n",
    "c = model.feature_aggregator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training data manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "for i in range(30):\n",
    "    wav_input_16khz = np.random.normal(0, 1, 10000)\n",
    "    # print(wav_input_16khz.shape)\n",
    "    write(f'/home/brian/Documents/ECE496/ecog2txt_infonce/testwavs/wav_{i}.wav', 16000, wav_input_16khz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /home/bayuan/Documents/fall23/fairseq/examples/wav2vec/wav2vec_manifest.py \\\n",
    "    /home/bayuan/Documents/fall23/ecog2vec/testwavs \\\n",
    "  --dest /home/bayuan/Documents/fall23/ecog2vec/manifest \\\n",
    "  --ext wav \\\n",
    "  --valid-percent 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a wav2vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !module load use.own\n",
    "# !PYTHONPATH=/depot/jgmakin/data/conda_env/wav2vec/lib/python3.8/site-packages:/apps/cent7/xalt/site:/apps/cent7/xalt/libexec:/apps/spack/gilbreth/apps/intel-parallel-studio/cluster.2017.1-intel-17.0.1-2off4ih/advisor_2017.1.1.486553/pythonapi\n",
    "# !echo $PYTHONPATH\n",
    "# !module load conda-env/wav2vec-py3.8.5\n",
    "!python3 -c 'import argparse; print(argparse.__file__)'\n",
    "!python3 /home/bayuan/Documents/fall23/fairseq/train.py \\\n",
    "    /home/bayuan/Documents/fall23/ecog2vec/manifest \\\n",
    "  --save-dir /home/bayuan/Documents/fall23/ecog2vec/model \\\n",
    "  --num-workers 6 --fp16 --max-update 400000 --save-interval 1 --no-epoch-checkpoints \\\n",
    "  --arch wav2vec --task audio_pretraining --min-lr 1e-06 --stop-min-lr 1e-09 --optimizer adam --lr 0.001 --lr-scheduler cosine \\\n",
    "  --conv-feature-layers \"[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]\" \\\n",
    "  --conv-aggregator-layers \"[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]\" \\\n",
    "  --skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion wav2vec --num-negatives 10 \\\n",
    "  --max-sample-size 150000 --skip-invalid-size-inputs-valid-test --max-epoch 20 --max-tokens 1500000 --batch-size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract embeddings from the downstream task data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=/path/to/fairseq python examples/wav2vec/wav2vec_featurize.py --input /path/to/task/waves --output /path/to/output \\\n",
    "  --model /home/bayuan/Documents/ecog2txt_infonce/model/checkpoint_best.pt --split train valid test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
